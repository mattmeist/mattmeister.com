<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>MattMeister.com</title>
<link>https://www.mattmeister.com/soap_box.html</link>
<atom:link href="https://www.mattmeister.com/soap_box.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.4.553</generator>
<lastBuildDate>Thu, 05 Jun 2025 07:00:00 GMT</lastBuildDate>
<item>
  <title>Star ratings are a bad way to compare products (2)</title>
  <dc:creator>Matt Meister</dc:creator>
  <link>https://www.mattmeister.com/posts/star_ratings_kilos.html</link>
  <description><![CDATA[ 




<style>
  p {
    margin-top: 1.25em;
  }
  h4 {
    margin-top: 2em;
  }
</style>
<div style="text-align: left; margin-top: 3em;">
<p>This is the second in a series about star ratings/online reviews. I plan to mainly talk about my own research. But who knows.</p>
</div>
<p>The kilogram is a useful measure of mass. The kilogram is widely used because it is objectively defined. One kilogram is equal to the weight of one liter of liquid water—so a bathroom scale tells users how much liquid water they equate to. Beyond providing a good ice breaker for dinner parties, this also makes the kilogram easy to interpret and compare; the kilogram helps people to compare objects to important benchmarks (e.g., “Is this truck too heavy to cross this bridge?”), and to other objects (e.g., “Which truck should I take to cross this rickety bridge?”). It is a good thing that the kilogram is widely used—the kilogram is a good measure.</p>
<p>Star ratings are similar to the kilogram in one sense, but very different in another. Ratings are also widely used. Almost anywhere we shop online, we find some form of ratings. However, star ratings are absolutely not objectively defined. There is no equivalent to the liter of water that defines what each star in a rating represents, and not even guidance as to what ratings <em>should</em> measure. As a result, ratings are open to being swayed by context. In <a href="../posts/star_ratings_steves.html">my last post</a>, I wrote about one example of mental context—expectations; influenced by Airbnb Superhost certifications—and its effect on ratings. In this post, I’ll write about this in a more general sense, explaining why (I think) context influences our evaluations.<sup>1</sup></p>
<section id="evaluations-with-bad-scales" class="level4">
<h4 data-anchor-id="evaluations-with-bad-scales">Evaluations with bad scales</h4>
<p>What would the world look like without standardized measurements?</p>
<p>Imagine that we did not have <strong>any</strong> standardized measure of weight. Instead, we could only say that things were (i) “very light”, (ii) “light”, (iii) “neither light nor heavy”, (iv) “heavy”, or (v) “very heavy”.</p>
<p>In this world, how heavy would you say that André the Giant is?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.mattmeister.com/posts/star_ratings_kilos/andre.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%" alt="Photo of Andre the Giant"></p>
</figure>
</div>
<div style="text-align: left; margin-top: 4em;">
<p>Now, how heavy would you say this elephant is?<sup>2</sup></p>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.mattmeister.com/posts/star_ratings_kilos/elephant.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%" alt="Photo of an elephant"></p>
</figure>
</div>
<div style="text-align: left; margin-top: 4em;">
<p>Were they both “very heavy”?</p>
</div>
<p>Now, how about a this 2012 Mini Coupe?<sup>3</sup></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.mattmeister.com/posts/star_ratings_kilos/mini.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%" alt="Photo of a 2012 Mini Coupe"></p>
</figure>
</div>
<p>I am guessing that your ratings are a mess.</p>
<p>André the Giant? Clearly, a giant. It’s right there in the title. So he is “very heavy”.</p>
<p>The elephant? An elephant. Famously “very heavy”. But heavier than André the Giant for sure, so it probably doesn’t feel great to not be able to distinguish them.</p>
<p>The Mini Coupe is where this exercise really falls apart. It is a smaller version of a small vehicle. Calling it “very heavy” seems wrong, but it is also <strong>much</strong> heavier than André the Giant.</p>
<p>There is no perfect way to judge the heaviness of these three things, based purely on the scale and instructions I provided. If the scale was objective (like a kilogram), this would obviously be easy. It would also be relatively easier if I clearly explained <strong>how</strong> you should judge heaviness—for example, if I said “Judge the heaviness of these three things, in relation to eachother,” or “Judge the heaviness of these three things, in relation to the category they come from.”</p>
<p>Without clear instructions for how to judge the heaviness of these three things, we use whatever reference points come to mind. André the Giant was called such because he was extremely large, <em>compared to other humans</em>. This is a natural point of comparison for humans—other humans. Meanwhile, elephants are seen as large <em>compared to other animals</em>, because that is a natural comparison. Finally, the entire Mini car branding only makes sense in comparison to other cars.</p>
<p>Obviously however, this falls apart when we consider these judgments <strong>across</strong> things in different categories. These judgments only make sense when we know the comparisons they resulted from. But, even if we knew these comparisons, the scale (i.e., “very light” to “very heavy”) still lets us down; if all you know is that André the Giant is a very heavy human, and an elephant is a very heavy animal, you still have no idea how many André the Giants it would take to balance the weight of an elephant.</p>
</section>
<section id="i-thought-this-was-about-star-ratings" class="level4">
<h4 data-anchor-id="i-thought-this-was-about-star-ratings">I thought this was about star ratings?</h4>
<p>In <a href="../posts/star_ratings_steves.html">my last post</a>, I wrote about how Airbnb listings that are certified as Superhosts receive lower ratings than they would if they were not Superhosts. I said that this was because people don’t make the same comparisons when they evaluate Superhosts and regular hosts on Airbnb—they expect more from Superhosts. That was a specific example of a general problem.</p>
<p>When we evaluate anything online, we are not told <em>how</em> to make this evaluation. For example, here is the most recent review solicitation email I have received:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.mattmeister.com/posts/star_ratings_kilos/thriftbooks.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:70.0%" alt="Photo of an online review solicitation"></p>
</figure>
</div>
<p>That’s all the instruction I received. No benchmarks and no guidance for things to consider or compare my experience to (e.g., “Was the shipping time faster or shorter than Barnes and Noble?”). And this type of solicitation is the industry standard—you won’t find better most places.</p>
<p>This means we—as reviewers—have to decide for ourselves what a 1-, 2-, 3-, 4-, or 5-star experience is, for everything we review. There is nothing keeping us consistent to ourselves over time (if I am grumpy about work, I may write more negative reviews for socks), across experiences (I may be more harsh to Superhosts than regular Airbnbs), or across people (<a href="https://reinholtzresearch.com">Nick Reinholtz</a> is a ray of sunshine, while I am an under-bridge troll).</p>
<p>This problem is most serious in a case like Superhost status—when something that differs between alternatives also affects ratings. For an even clearer example, consider an experiment that Nick and I ran a few years ago. This experiment was previously in a <a href="https://papers.ssrn.com/abstract=4119082">working paper</a> alongside the Airbnb data, but is now in its own manuscript, under review at JEP: General.]</p>
<p>In this experiment,<sup>4</sup> we had participants complete ten rounds of a boring, effortful task. They were shown a screen of 36 ones and zeros in a 6×6 grid, and reported the number of zeros in that grid:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.mattmeister.com/posts/star_ratings_kilos/grid.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:50.0%" alt="Grid of 36 1s and 0s."></p>
</figure>
</div>
<p>Participants were paid a small amount for each grid they answered correctly. However, we did not tell people exactly what their pay would be before starting. Instead, we told them it would be <strong>one</strong> of the following pairs of possible payments:</p>
<ul>
<li>Either 5¢ (90%) or 25¢ (10%) per correct answer</li>
<li>Either 5¢ (90%) or 4¢ (10%) per correct answer</li>
</ul>
<p>Then, after participants completed their ten rounds, we actually randomly drew their payment rate, showed it to them, and asked them to rate the task from 1–5 stars.</p>
<p>The participants who were told they <em>could</em> be paid either 5¢ (90%) or 25¢ (10%) gave the task far lower ratings on average than the participants who were told they could be paid either 5¢ (90%) or 4¢ (10%).<sup>5</sup> This is despite the fact that they were paid more on average!<sup>6</sup> Obviously, however, they were not comparing their pay to this other group of people—they were not thinking of them—but to the reference points we gave them. For one group, 5¢ is awful pay (compared to 25¢). For the other, 5¢ is pretty good (compared to 4¢).</p>
<p>As with the last post, this is only an issue if people do not realize it when using ratings. Unfortunately, they don’t. In a pair of experiments in the paper, we frame these groups as different tasks, and ask people to select which of these two tasks they would like to complete.</p>
<p>When we only show participants the payment information, almost nobody selects the lower-paying task. This makes sense, as all of the information they can see suggests that task is better.</p>
<p>However, when we merely add the star ratings (similar to how Airbnb shows both Superhost status and star ratings), we find that many more people select the task which pays them lower. This makes sense to them! If the payment is different, the star ratings must contain some valuable information about the tasks, right??</p>
<p>Obviously, the ratings <em>do</em> contain some information. But that information is only relevant to people who are not comparing the two tasks to eachother. It is relevant to people who only experienced the one task—just the same as someone who only experiences one Airbnb at a time, or one restaurant in an evening, or any consumption experience.</p>
</section>
<section id="so-what" class="level4">
<h4 data-anchor-id="so-what">So what?</h4>
<p>I am hoping that at least one person who reads this thinks about André the Giant when they look at ratings online in the future. Partly because that seems like fun. But partly because star ratings really are not much less silly. Star ratings are a poor scale with no instruction, and a lot of people act like they are kilograms—at least within similar categories (e.g., I might compare ratings for McDonald’s to Wendy’s, but be less likely to compare McDonald’s to French Laundry).</p>
<p>For ratings, the takeaway here is similar to the last post. They cannot make fine distinctions between alternatives, and can be biased in more ways than we can anticipate. But at the same time, I am not sure these fine distinctions really matter in our lives. Most things are pretty good. Once we have a set of “good” alternatives, we’re probably just as happy flipping a coin.</p>
<p>Or, again, just stay at a Holiday Inn Express.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>In a later post, I will move onto physical contexts.↩︎</p></li>
<li id="fn2"><p>Photo by <a href="https://en.wikipedia.org/wiki/User:Muhammad_Mahdi_Karim" class="extiw" title="w:User:Muhammad Mahdi Karim">Muhammad Mahdi Karim</a> <a rel="nofollow" class="external text" href="https://facebook.com/micro2macro">Facebook</a><span typeof="mw:File"><span title="Wikimédia CH"></span></span>The making of this document was supported by <a href="https://meta.wikimedia.org/wiki/Wikimedia_CH" class="extiw" title="m:Wikimedia CH">Wikimedia CH</a>.↩︎</p></li>
<li id="fn3"><p>By IFCAR, Public Domain, https://commons.wikimedia.org/w/index.php?curid=17491394↩︎</p></li>
<li id="fn4"><p>The main paradigm is adapted from Abeler, J., Falk, A., Goette, L., &amp; Huffman, D. (2011). “Reference points and effort provision.” <em>American Economic Review</em>, 101(2), 470-492.↩︎</p></li>
<li id="fn5"><p>Means of 3.73 v. 4.44, F(1, 199) = 22.91, <em>p</em> &lt; .001, <em>d</em> = .68.↩︎</p></li>
<li id="fn6"><p>Means of $1.16 v. $.93↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>academic</category>
  <category>research</category>
  <category>star ratings</category>
  <guid>https://www.mattmeister.com/posts/star_ratings_kilos.html</guid>
  <pubDate>Thu, 05 Jun 2025 07:00:00 GMT</pubDate>
  <media:content url="https://www.mattmeister.com/posts/star_ratings_kilos/andre_elephant.png" medium="image" type="image/png" height="84" width="144"/>
</item>
<item>
  <title>Star ratings are a bad way to compare products (1)</title>
  <dc:creator>Matt Meister</dc:creator>
  <link>https://www.mattmeister.com/posts/star_ratings_steves.html</link>
  <description><![CDATA[ 




<style>
  p {
    margin-top: 1.25em;
  }
  h4 {
    margin-top: 2em;
  }
</style>
<div style="text-align: left; margin-top: 3em;  margin-bottom: 2em;">
<p>This is the first in a series about star ratings/online reviews. I plan to mainly talk about my own research. But who knows. Maybe we’ll get weird.</p>
</div>
<div style="text-align: center; margin-top: 2em;  margin-bottom: 2em;">
<p><em>Disclaimer: This post is in no way a paid advertisement for Rick Steves.</em></p>
</div>
<section id="the-promise" class="level4">
<h4 data-anchor-id="the-promise">The promise</h4>
<p>We make decisions by making comparisons.<sup>1</sup> Sometimes, we compare the cost of doing something to the benefit of doing that thing—asking questions like “Will I enjoy a vacation to Toledo enough to offset its cost?”. Other times, we compare options directly—questions like “Where should I stay overnight on my vacation in Toledo?”. This post is about the second type of comparison. I think that we under-appreciate how misinformed we are in these decisions.</p>
<p>Making these comparisons is often difficult. If I have never been to Toledo, nor the area around it, how am I supposed to decide which of Toledo’s hotels, Airbnbs, and Vrbos I will be happiest staying at? I can’t do this alone. I simply do not have the information.</p>
<p>So, how do we compare alternatives? We collect more information. Two decades ago, I would have gone to Chapters,<sup>2</sup> bought a copy of <a href="https://store.ricksteves.com/shop/p/madrid-guidebook">Rick Steves’ guidebook to Toledo and Madrid</a>, and followed Mr.&nbsp;Steves. If he preferred the <em>Hotel Carlos V</em> over the <em>Hotel Pintor El Greco</em>, then Carlos V it would be for me. Because how else could I compare?</p>
<p>The internet changed all of that. The internet democratized opinions, as it democratized everything.<sup>3</sup> With Tripadvisor, Yelp, and Google, we are no longer forced to seek out experts, a well-traveled friend, or simply guess. Instead, we can find a ton of information at miniscule effort and cost.</p>
<p>In theory, this is wonderful. The internet exposes us to opinions from people who are like us. Rick Steves is a professional traveler from <a href="https://en.wikipedia.org/wiki/Barstow,_California">Barstow, California</a>.<sup>4</sup> How could he possibly know what it is like to travel for me, a recreational traveler from El Cerrito, California? Certainly, the masses at Tripadvisor will know better <em>for me</em>. The internet also shows these opinions in clearly comparable formats—star ratings.</p>
<p>Therefore, if the <em>Hotel Carlos V</em> has an average star rating of 4.3, and the <em>Hotel Pintor El Greco</em> a 4.6, it <em>must</em> be true that the Pintor is the way to go! If it wasn’t better, why would it have a higher rating?</p>
</section>
<section id="the-problem" class="level4">
<h4 data-anchor-id="the-problem">The problem</h4>
<p>Unfortunately, reality is not so simple, in part because we don’t create ratings in the same way that we use them. Specifically, we don’t create a rating for a hotel by comparing our experience at that hotel to our experience at a different hotel—we can’t do this, because we only stay at one hotel at a time. And even if we do stay at multiple hotels over some period of time, we certainly do not stay at them all, or even a substantial fraction.</p>
<p>The result is that no two ratings mean the same thing; Even though they are on the same scale visually (1-5 stars), there is nothing to enforce ratings to be on the same scale mentally. In fact, they can be quite different, and misleading as a result. For example, we often evaluate things in comparison to our prior expectations for that thing. The higher our expectations, the more likely we are to be disappointed in a given experience,<sup>5</sup> which leads to lower ratings. This can become a big problem when two conditions are met:</p>
<ol type="1">
<li><p>Expectations differ systematically between alternatives</p></li>
<li><p>Future consumers—using ratings to make comparisons—under-appreciate the influence of expectations</p></li>
</ol>
<p>For an example of both conditions and their impact, I am going to discuss a recent paper <a href="https://reinholtzresearch.com">Nick Reinholtz</a> and I published in 2025 at Journal of Consumer Research (<a href="https://doi.org/10.1093/jcr/ucaf008">link</a>).<sup>6</sup></p>
</section>
<section id="airbnb-superhost-status" class="level4">
<h4 data-anchor-id="airbnb-superhost-status">Airbnb Superhost status</h4>
<p>Airbnb has to provide information to help their customers feel comfortable enough to make a purchase (like any platform, only more so). Airbnb collects guests’ ratings, but also presents <a href="https://www.airbnb.com/help/article/829">Superhost status</a>—their own, Airbnb-specific certification to distinguish “top-performing” listings. Lots of platforms who collect ratings also present certifications; eBay has <a href="https://www.ebay.com/sellercenter/protections/top-rated-program">“Top-Rated Sellers”</a>, Apple’s App Store has <a href="https://developer.apple.com/app-store/app-store-awards-2024/">Awards and “Apps We Love”</a>, Indigo Bookstores has <a href="https://www.indigo.ca/en-ca/books/heathers-picks/">“Heather’s Picks”</a>, and anywhere else you shop probably has something similar.<sup>7</sup></p>
<p>The idea behind these certifications is obvious—make customers expect that an awarded option will be good, leading them to buy. Unfortunately, expectations are not forgotten immediately after we decide to make a purchase. Instead, when we are asked to evaluate that purchase down the road, we compare our experiences to those same expectations.</p>
<p>In the case of Airbnb, this means that <strong>gaining</strong> Superhost status has a <strong>negative</strong> effect on ratings, and that <strong>losing</strong> Superhost status has a <strong>positive</strong> effect on ratings. Nick and I demonstrate this in three ways in the paper, two of which I mention here.</p>
<p>First, we compare ratings across different Airbnb listings over time. Specifically, we look at how ratings change over time for listings that change status, and compare that to how ratings change over the same period for listings that do not change status.</p>
<p>Because there are obvious differences between Superhost and non-Superhost listings other than their status, we cannot simply compare the average ratings between these groups. Superhosts are probably objectively better, so they will unsurprisingly have higher ratings. However, we can use this fact to our advantage by comparing the difference in ratings before one group changes status to the difference in ratings after.</p>
<p>The key assumption we make here is that nothing that influences ratings changes between listings in these groups alongside Superhost status. For example, we have to assume that listings do not also add or drop amenities substantially when they change Superhost status. This assumption should feel like a stretch, which necessitates the second analysis.</p>
<p>The figure below shows the average effect of gaining Superhost status on Airbnb ratings. Before quarter zero (when status is gained), this should be zero—after all, how can Superhost status have an effect before it is gained? If these estimates (the dark dots) are not zero, we have a problem with the assumption above. Clearly, we have a problem with this specification, as these estimates suggest listings that gain status have higher ratings before gaining status.</p>
<p>We’ll fix this later, but for now, notice that the effect is negative after the change (lighter blue points). Ratings are lower for those that gain status, despite being higher in the previous quarters.</p>
<div class="image-box">
<p>
<strong>Differences in ratings: Airbnb listings that gain superhost status (vs no change)</strong>
</p>
<p><img src="https://www.mattmeister.com/posts/star_ratings_steves/btw_gain.png" alt="Estimated differences between Airbnb ratings for listings that gain superhost status, vs no change."></p>
<p class="caption-note">
<em>Bars represent 95% confidence intervals. Dark blue points are estimated differences before changing status, light blue are after.</em>
</p>
</div>
<p>We see the same, reversed, for listings that lose status, compared to listings that are always Superhosts. Ratings are higher than we would expect for listings that lose status after the change, although the difference prior to the change in status is a problem.</p>
<div class="image-box">
<p>
<strong>Differences in ratings: Airbnb listings that lose superhost status (vs no change)</strong>
</p>
<p><img src="https://www.mattmeister.com/posts/star_ratings_steves/btw_lose.png" alt="Estimated differences between Airbnb ratings for listings that lose superhost status, vs no change."></p>
<p class="caption-note">
<em>Bars represent 95% confidence intervals. Dark blue points are estimated differences before changing status, light blue are after.</em>
</p>
</div>
<p>Clearly, something other than Superhost status changes between listings over time. We can’t know what that is exactly, but we don’t have to—we have another way to compare Airbnb ratings. Specifically, many properties are cross-listed on both Airbnb and Vrbo.com. Any changes in a property over time will not differ between platforms, except for things that the platform shows. Vrbo does not show a listing’s Airbnb Superhost status, so comparing Airbnb to Vrbo ratings for the same listing is a satisfyingly clean analysis.<sup>8</sup></p>
<p>The figure below shows the average effect of gaining Superhost status on ratings for listings that gain status on Airbnb and those same listings on Vrbo. This time, the dark blue dots are much closer to zero prior to the change in status, meaning that we can be fairly confident that the additional difference between the light blue dots is due to the change in status.</p>
<div class="image-box">
<p>
<strong>Differences in ratings: Airbnb listings that gain superhost status (vs same properties on Vrbo)</strong>
</p>
<p><img src="https://www.mattmeister.com/posts/star_ratings_steves/vrbo_gain.png" alt="Estimated differences between Airbnb ratings for listings that gain superhost status, vs themselves on Vrbo."></p>
<p class="caption-note">
<em>Bars represent 95% confidence intervals. Dark blue points are estimated differences before changing status, light blue are after.</em>
</p>
</div>
<p>We see an even stronger effect (in the opposite direction) for listings that lose status compared to themselves on Vrbo. Ratings are higher than we would expect on Airbnb after the loss in status, and the effects prior to the change in status are similar.</p>
<div class="image-box">
<p>
<strong>Differences in ratings: Airbnb listings that lose superhost status (vs same properties on Vrbo)</strong>
</p>
<p><img src="https://www.mattmeister.com/posts/star_ratings_steves/vrbo_lose.png" alt="Estimated differences between Airbnb ratings for listings that lose superhost status, vs themselves on Vrbo."></p>
<p class="caption-note">
<em>Bars represent 95% confidence intervals. Dark blue points are estimated differences before changing status, light blue are after.</em>
</p>
</div>
</section>
<section id="so-what" class="level4">
<h4 data-anchor-id="so-what">So what?</h4>
<p>The fact that something like Superhost status influences ratings is not, on its own, a reason to run back to Rick Steves. If we realize that Superhosts are judged on a harsher scale than non-Superhosts, we might interpret the ratings we see accordingly, <em>a la</em> Childish Gambino (“<a href="https://www.youtube.com/watch?v=L8gGHqPBuZM">A New York nine is an everywhere else six</a>”). Unfortunately, Mr.&nbsp;Gambino’s awareness of scales appears to be unique.</p>
<p>In a pair of follow-up experiments,<sup>9</sup> Nick and I showed participants pairs of Airbnb listings and asked them to choose which they would rather stay at. These pairs were always set up such that one listing had higher ratings, but the other was a Superhost. Below is an example of one of the five pairs of listings we used:</p>
<div class="image-box">
<p>
<strong>Example Airbnb choices shown in experiment</strong>
</p>
<img src="https://www.mattmeister.com/posts/star_ratings_steves/figure_6_smaller.png" alt="Images of Airbnb listings, one being superhost, the other having higher ratings.">
<p class="caption-note">
<em>Participants saw only one pair of images.</em>
</p>
</div>
<p>A majority of people (55%) chose to stay at the non-Superhost listing. Only 36% chose the Superhost, and 9% indicated no preference. Clearly, people under-appreciated the influence of the Superhost certification on ratings. Despite the fact that we are all influenced by expectations when rating things ourselves, we do not anticipate that other people are also influenced by expectations.</p>
<p>This idea might become more confusing the more you think about it. If we are influenced by Superhost status when creating ratings, why are we not equally influenced when using ratings to compare products? In my opinion, this is the most interesting aspect of Nick and my work. The next post will focus on this, but the gist is that it all comes down to <em>what</em> we can evaluate at a given time.</p>
<p>When Airbnb guests create ratings, Superhost status is displayed to them, and their expectations are probably top of mind. It’s easy to compare the experience to one’s expectations, or even to the idea of a “Super” host. Meanwhile, when we consider multiple Airbnb listings to book, different information feels comparable. Superhost status is either there or not—it’s chunky. Hard to deal with. Star ratings are a nice, continuous number. 4.8 is greater than 4.78, 4.69, but less than 4.83. This is what makes them promising in the first place! Unfortunately, there is a <strong>lot</strong> that creates a star rating that we do not know about and can’t adjust for.</p>
</section>
<section id="so-we-should-ignore-ratings" class="level4">
<h4 data-anchor-id="so-we-should-ignore-ratings">So we should ignore ratings?</h4>
<p>Probably not.</p>
<p>The messiness of ratings mean that we can’t compare a 4.8 to a 4.78 and feel confident. We probably can’t compare a lot of alternatives on their ratings in fact. But, we can compare a 4.8 to a 2.2. We may be bad at minute evaluations, but we can clearly spot disasters. Or pick at random between anything with a 4-star rating or better. Once you get above the level of Airbnbs infested with rodents, it’s really not a big deal.</p>
<p>We also have much better information available to us. Airbnb has a strong incentive for you to have a good time, so their Superhost certification is probably a good bet. As are a lot of platform certifications.</p>
<p>Experts have their place as well. They have the experience, and can compare alternatives when evaluating. Rick Steves has stayed at innumerable hotels—his recommendations come from the same comparison you are trying to make.</p>
<p>Alternatively, just stay at a Holiday Inn Express.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Not <em>always</em>, of course; When I have to use the restroom, I don’t compare the time cost to the time I would lose by changing my pants. But many of the decisions we care about result from comparisons.↩︎</p></li>
<li id="fn2"><p>In America, this translates to “Barnes and Noble”.↩︎</p></li>
<li id="fn3"><p>If only temporarily in some cases.↩︎</p></li>
<li id="fn4"><p>Which might be more arid and empty than the moon↩︎</p></li>
<li id="fn5"><p>A decent argument for cynicism↩︎</p></li>
<li id="fn6"><p>Before continuing, it is worth noting that at least two other papers provide compelling and well-specified evidence for the first condition and its impact. <em>“Can Lower(ed) Expert Opinions Lead to Better Consumer Ratings?: The Case of Michelin Stars”</em> by Xingyi Li and colleagues finds that losing Michelin stars has a positive effect on ratings for restaurants. <em>“The Good, the Bad and the Picky: Consumer Heterogeneity and the Reversal of Product Ratings”</em> by Tommaso Bondi, Michelangelo Rossi, and Ryan Stevens finds similarly that people are more critical of movies after they have been nominated for Oscars.↩︎</p></li>
<li id="fn7"><p>Sometimes these certifications are influenced by ratings (Superhost status is), sometimes they are not (Heather seems unswayed by the masses), and sometimes it is not clear (eBay’s Top Rated Sellers are a confusing example of this).↩︎</p></li>
<li id="fn8"><p>The specific assumption we make here is that nothing that influences ratings changes between platforms alongside Superhost status. Because Superhost status is entirely an Airbnb feature, this should be more defensible.↩︎</p></li>
<li id="fn9"><p>And another paper, detailed in the next blog post↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>academic</category>
  <category>research</category>
  <category>star ratings</category>
  <guid>https://www.mattmeister.com/posts/star_ratings_steves.html</guid>
  <pubDate>Tue, 03 Jun 2025 07:00:00 GMT</pubDate>
  <media:content url="https://www.mattmeister.com/posts/star_ratings_steves/steves_books.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
