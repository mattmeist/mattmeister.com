[
  {
    "objectID": "teaching/02 Text Analysis.html#pre-class-code-assignment-instructions",
    "href": "teaching/02 Text Analysis.html#pre-class-code-assignment-instructions",
    "title": "02 - Text Analysis",
    "section": "Pre-Class Code Assignment Instructions",
    "text": "Pre-Class Code Assignment Instructions\nIn this semester, I am going to ask you to do a fair bit of work before coming to class. This will make our class time shorter, more manageable, and hopefully less boring.\nI am also going to use this as an opportunity for you to directly earn grade points for your effort/labor, rather than “getting things right” on an exam.\nTherefore, I will ask you to work through the posted slides on Canvas before class. Throughout the slides, I will post Pre-Class Questions for you to work through in R. These will look like this:\n\nPre-Class Q1\n\nIn R, please write code that will read in the .csv from Canvas called sf_listings_2312.csv. Assign this the name bnb.\nYou will then write your answer in a .r script:\n\n\nClick to show code and output\n\n\n\n# Q1\n#bnb &lt;- read.csv(\"sf_listings_2312.csv\")\n\n\n\n\nImportant:\nTo earn full points, you need to organize your code correctly. Specifically, you need to:\n\nAnswer questions in order.\n\nIf you answer them out of order, just re-arrange the code after.\n\nPreface each answer with a comment (# Q1/# Q2/# Q3) that indicates exactly which question you are answering.\n\nPlease just write the letter Q and the number in this comment.\n\nMake sure your code runs on its own, on anyone’s computer.\n\nTo do this, I would always include rm(list = ls()) at the top of every .r script. This will clean everything from the environment, allowing you to see if this runs on my computer.\n\n\n\n\nHanding this in:\n\nYou must submit this to Canvas before 9:00am on the day of class. Even if class starts at 10:00am that day, these are always due at 9:00.\nYou must submit this code as a .txt file. This is because Canvas cannot present .R files to me in SpeedGrader. To save as .txt:\n\nClick File -&gt; New File -&gt; Text File\nCopy and paste your completed code to that new text file.\nSave the file as firstname_lastname_module.txt\n\nFor example, my file for Module 01 would be matt_meister_01.txt\nMy file for module 05 would be matt_meister_05.txt\n\n\n\n\n\nGrading:\n\nI will grade these for completion.\nYou will receive 1 point for every question you give an honest attempt to answer\nYour grade will be the number of questions you answer, divided by the total number of questions.\n\nThis is why it is important that you number each answer with # Q1.\nAny questions that are not numbered this way will be graded incomplete, because I can’t find them.\n\nYou will receive a 25% penalty for submitting these late.\nI will post my solutions after class."
  },
  {
    "objectID": "teaching/02 Text Analysis.html#text-analysis",
    "href": "teaching/02 Text Analysis.html#text-analysis",
    "title": "02 - Text Analysis",
    "section": "Text Analysis",
    "text": "Text Analysis\nLoad in these packages. If you do not have them, you will need to install them.\n\ne.g., install.packages(\"tidytext\")\n\n\nlibrary(tidytext)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(topicmodels)\nlibrary(tidyr)\nlibrary(Matrix)\n\nRead in the Airbnb listings from last class (as bnb) as well as the reviews (on Canvas):\n\n\nClick to show code and output\n\n\n\nrevs &lt;- read.csv('sf_reviews_2312.csv')\n\n\n\nWe have spent a lot of time with numbers. We have even dabbled a bit in turning text into numbers. For example, whenever we have made dummy codes/indicator variables (e.g., for gender), we are taking words and turning them into numbers.\nText is an extremely useful form of data, especially for us as market researchers. However, it is not always obvious how to take text and turn it into something that we can test–or use to test other things.\nIn this module, I will briefly introduce four kinds of text analysis:\n\nBag-of-words/sentiment\nTopic modeling\nKeywords\nClassification\n\nUnfortunately, we do not have the time to go into great detail on any one of these. Doing so could be an entire class. Therefore, I suggest–if you are interested–looking online at the many, many blogs/walkthroughs you can find about these.\n\nJoin Reviews and Listings\nWe are going to analyze the text of the reviews we have for our Airbnb listings. To do so, it would be nice to have the listing information joined with each of the Airbnb snapshots. We can do this with a join function, from dplyr. There are multiple kinds of joins:\n\n1. Inner Join (inner_join)\n\nDescription: Combines two datasets by returning only the rows with matching keys in both datasets.\nUse case: Use this when you need data that exists in both datasets.\nExample: inner_join(df1, df2, by = \"key_column\")\n\n\n\n2. Left Join (left_join)\n\nDescription: Returns all rows from the first dataset (df1) and the matching rows from the second dataset (df2). If no match is found, NA is returned for columns from df2.\nUse case: Use this when the focus is on keeping all rows from the left dataset.\nExample: left_join(df1, df2, by = \"key_column\")\n\n\n\n3. Right Join (right_join)\n\nDescription: Returns all rows from the second dataset (df2) and the matching rows from the first dataset (df1). If no match is found, NA is returned for columns from df1.\nUse case: Use this when the focus is on keeping all rows from the right dataset.\nExample: right_join(df1, df2, by = \"key_column\")\n\n\n\n4. Full Join (full_join)\n\nDescription: Returns all rows from both datasets. Rows with no match in either dataset will have NA in the missing columns.\nUse case: Use this when you want a complete merge of both datasets, keeping all rows regardless of matching.\nExample: full_join(df1, df2, by = \"key_column\")\n\n\n\n5. Semi Join (semi_join)\n\nDescription: Returns only the rows from the first dataset (df1) that have a match in the second dataset (df2). It does not add columns from df2.\nUse case: Use this to filter rows in the first dataset based on matching keys in the second dataset.\nExample: semi_join(df1, df2, by = \"key_column\")\n\n\n\n6. Anti Join (anti_join)\n\nDescription: Returns only the rows from the first dataset (df1) that do not have a match in the second dataset (df2).\nUse case: Use this to find rows in the first dataset that have no match in the second dataset.\nExample: anti_join(df1, df2, by = \"key_column\")\n\nWe will use inner_join(), because I want to only use reviews for which we have listing information.\nIn our case, the key column is the listing id. Unfortunately, this is called id in the bnb data frame, and listing_id in the revs data frame. I think the easiest thing is to create a new variable in bnb:\n\nPre-Class Q1\n\nHow can we join revs and bnb?\n\n\nClick to show code and output\n\n\n\nbnb$listing_id &lt;- bnb$id\nrevs &lt;- inner_join(revs, bnb, by = 'listing_id', suffix = c('_revs', '_listing'))\n\n\n\n\nsuffix = tells R what to put at the end of each column that is in both data frames. This tells us where something came from if it is duplicated."
  },
  {
    "objectID": "teaching/02 Text Analysis.html#cleaning-text",
    "href": "teaching/02 Text Analysis.html#cleaning-text",
    "title": "02 - Text Analysis",
    "section": "Cleaning Text",
    "text": "Cleaning Text\n\nWhy?\nText data is often messy, and has a lot of elements we can’t use, such as punctuation, numbers, extra spaces, and special characters. These can obscure underlying patterns, and/or make the data impossible to use. Cleaning the text helps:\n\nImprove the quality of analysis.\nEnsure uniformity in the text (e.g., all lowercase).\nPrepare the text for further processing, such as tokenization or sentiment analysis.\n\n\n\nHow?\n\nLowercasing: Ensures uniformity by converting all text to lowercase.\nRemoving punctuation: Strips out special characters that don’t contribute to the meaning of the text.\nRemoving numbers: Eliminates numeric characters if they are not relevant.\nRemoving stopwords: Excludes common words (e.g., “and,” “the”) that don’t add much value.\nTrimming whitespace: Removes extra spaces for clean formatting.\n\n\n\nExample\nThe text in revs is contained in the column comments. Below, we will complete each step mentioned above.\n\nPre-Class Q2\n\nLowercasing:\n\n\nClick to show code and output\n\n\n\nrevs$clean_text &lt;- tolower(revs$comments)\n\n\n\n\nPre-Class Q3\n\nRemove punctuation:\n\n\nClick to show code and output\n\n\n\nrevs$clean_text &lt;- str_remove_all(revs$clean_text, \"[[:punct:]]\")\n\n\n\n\nPre-Class Q4\n\nRemove numbers:\n\n\nClick to show code and output\n\n\n\nrevs$clean_text &lt;- str_remove_all(revs$clean_text, \"\\\\d*\")\n\n\n\n\nPre-Class Q5\n\nRemove stopwords:\n\n\nClick to show code and output\n\n\n\nrevs$clean_text &lt;- str_remove_all(revs$clean_text,\n                                  paste(stop_words$word, collapse = \"\\\\b|\\\\b\"))\n\n\n\n\nWhen we loaded tidytext, it also loaded the data frame stop_words in the background.\nThis contains a bunch of very common words, which don’t add much.\n\n\nWhat’s the \\\\b?\nThe \\\\b ensures that the pattern matches whole words only, rather than substrings inside larger words.\nIn our example:\nstr_remove_all(revs$clean_text, paste(stop_words$word, collapse = \"\\\\b|\\\\b\"))\nThe \\\\b ensures that stopwords like “a” or “the” are removed only when they are standalone words, not parts of other words.\n\nThe word “a” will be removed from “we found a rat in the fridge”, but the letter “a” will not\n\nResult: “we found rat in the fridge”\n\nThis prevents accidental removal of parts of words, ensuring cleaner and more accurate text processing.\n\n\nPre-Class Q6\n\nRemove whitespace:\n\n\nClick to show code and output\n\n\n\nrevs$clean_text &lt;- str_squish(revs$clean_text)\n\n\n\n\n\n\nPurpose of Each Step\n\nConvert to Lowercase (tolower()):\n\nText normalization to treat “Hello” and “hello” as the same.\n\nRemove Punctuation (str_remove_all(\"[[:punct:]]\")):\n\nStrips out symbols like !, ?, or . that don’t carry semantic meaning.\n\nRemove Numbers (str_remove_all(\"\\\\d*\")):\n\nGets rid of numeric values that may not be relevant to the analysis.\n\nRemove Stopwords (str_remove_all):\n\nUses a pre-defined list of stopwords from the tidytext package.\n\nTrim Extra Whitespace (str_squish):\n\nCleans up any remaining extra spaces for neatness.\n\n\n\n\nOutcome\nHere’s what the updated revs data looks like:\n\n\nClick to show code and output\n\n\n\nhead(revs[,c('comments', 'clean_text')])\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  comments\n1                                                                                The bad: Overall felt like a college dorm. The room was very small. the common areas dingy and the carpet and walls dirty. There’s a mildew smell on the air mixed with the many spices of everyone’s dinner. With no air conditioning or fan you have to sleep with the windows open, but the street noise is bad. The front door is an iron behemoth that shakes the whole place with a boom every time it closes. The good: convenient location and good communication\n2                                                                                                                                                                                                                                                                                          We really liked Lily's place! It was very clean and practical for our visit to San Francisco. The location is good, specially if you have a car. Heads-up, don’t park on the side walk. Although we parked exactly where Lily told us, we got a parking ticket.\n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                A good place to crash in a great location\n4 My partner and I were in SF for three days and feel like this hostel was well worth the price! The bathroom and showers were always clean and its location right outside of Chinatown was so convenient. The front desk was very accommodating and helpful with the check-in and check-out process, even offering to hold our bags for a few hours for the latter. The surrounding area generally felt safe, with my partner and I taking walks around the area until 9-10pm. Great experience overall! Highly recommend, especially given its location.\n5                                                                                                                                                                                                                                                                                                                                                                                                            Couldn’t recommend this place enough! Richard and Bina we both so lovely, helpful, and went above and beyond on communication. Very grateful!\n6                                                                                                                                                                                                                                                                                                                                                                                                                                          We had a great stay in Noe Valley! Hosts were so nice and eager to help with anything. Can’t beat the location!\n                                                                                                                                                                                                                                                     clean_text\n1                        bad college dorm common dingy carpet walls dirty mildew smell air mixed spices everyones dinner air conditioning fan sleep windows street noise bad front door iron behemoth shakes boom time closes convenient location communication\n2                                                                                                                                       lilys clean practical visit san francisco location specially car headsup dont park walk parked lily told parking ticket\n3                                                                                                                                                                                                                                                crash location\n4 partner sf days feel hostel worth price bathroom showers clean location chinatown convenient front desk accommodating helpful checkin checkout process offering hold bags hours surrounding safe partner taking walks pm experience highly recommend location\n5                                                                                                                                                                                           couldnt recommend richard bin lovely helpful communication grateful\n6                                                                                                                                                                                                                stay noe valley hosts nice eager beat location\n\n\n\n\nHere’s a 10-minute workshop outline for introducing Bag-of-Words (BoW) Analysis using tidyverse and tidytext, with a focus on sentiment analysis as the starting point. The goal is to give students hands-on experience with BoW and demonstrate its flexibility."
  },
  {
    "objectID": "teaching/02 Text Analysis.html#bag-of-wordssentiment-analysis",
    "href": "teaching/02 Text Analysis.html#bag-of-wordssentiment-analysis",
    "title": "02 - Text Analysis",
    "section": "Bag of Words/Sentiment Analysis",
    "text": "Bag of Words/Sentiment Analysis\n\nWhat is Bag-of-Words?\nBoW represents text as a collection of words, ignoring grammar and word order, focusing only on the presence (or frequency) of words. Effectively, this is identifying if a word exists in some text. For example, Did this reviewer say “rat”?\nAdvantages: - Simplicity and interpretability. - Versatile for various analyses. Disadvantages: - Ignores word order and context (e.g., sarcasm). - Can be improved with more sophisticated methods (e.g., word embeddings).\n\nUse Cases:\n\nSentiment analysis.\nWord frequency analysis.\nText classification and topic modeling.\n\n\n\n\nSentiment Analysis Example:\nWe will use the revs dataset to compute the average sentiment score of reviews. Sentiment analysis helps understand customer opinions. It is especially useful when we can’t quantify those opinions in other ways. With reviews, we usually have a star rating, but we actually don’t on Airbnb. So this will help fill in a gap.\nSentiment analysis is also a relatively simple application of BoW. It just assigns positive/negative labels to text.\n\nSteps\n\nTokenization: Break text into individual words.\nJoin Sentiments: Match words with sentiment scores.\nCompute Scores: Summarize sentiment for each review.\n\nStep 1: Tokenization\nWe tokenize with the function unnest_tokens() in R. The function takes a data frame, output, and input column. For us, the output is always going to be called “word”. To see how this works, let’s use a simple example. Remember, the text has been cleaned.\n\ntwo_reviews &lt;- data.frame(\n  id = c(1,2),\n  rating = c(2,5),\n  clean_text = c(\"disgusting this place was full of rats\", \"beautiful rats\")\n)\n\ntwo_reviews\n\n  id rating                             clean_text\n1  1      2 disgusting this place was full of rats\n2  2      5                         beautiful rats\n\ntwo_reviews_tokens &lt;- unnest_tokens(two_reviews, word, clean_text)\ntwo_reviews_tokens\n\n  id rating       word\n1  1      2 disgusting\n2  1      2       this\n3  1      2      place\n4  1      2        was\n5  1      2       full\n6  1      2         of\n7  1      2       rats\n8  2      5  beautiful\n9  2      5       rats\n\n\nStep 2: Join Sentiments\nTo join sentiments, we have to get a data frame of words and sentiments from somewhere. Luckily, similar to the stop.words, there are also sentiment data frames that come with tidytext. We will use the bing lexicon, for no real reason. Feel free to try others.\n\nbing_sentiments &lt;- get_sentiments(\"bing\")\nhead(bing_sentiments)\n\n# A tibble: 6 × 2\n  word       sentiment\n  &lt;chr&gt;      &lt;chr&gt;    \n1 2-faces    negative \n2 abnormal   negative \n3 abolish    negative \n4 abominable negative \n5 abominably negative \n6 abominate  negative \n\n\nbing_sentiments is a data frame of 6786 words, each with either a positive or negative tag. This is essentially our “bag” of words.\nUsing inner_join(), we can see how many positive and negative words are in each review. We can save this as two_reviews_bing.\n\ntwo_reviews_tokens |&gt;\n  inner_join(bing_sentiments, by = \"word\") \n\n  id rating       word sentiment\n1  1      2 disgusting  negative\n2  2      5  beautiful  positive\n\n\n\ntwo_reviews_tokens |&gt;\n  inner_join(bing_sentiments, by = \"word\") |&gt;\n  count(id, sentiment, sort = TRUE) \n\n  id sentiment n\n1  1  negative 1\n2  2  positive 1\n\n\n\ntwo_reviews_bing &lt;- two_reviews_tokens |&gt;\n  inner_join(bing_sentiments, by = \"word\") |&gt;\n  count(id, sentiment, sort = TRUE) \n\nStep 3: Summarize each review\n\ntwo_reviews_tokens |&gt;\n  inner_join(bing_sentiments, by = \"word\") |&gt;\n  count(id, sentiment, sort = TRUE) |&gt;\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |&gt;\n  mutate(sentiment_score = positive - negative)\n\n# A tibble: 2 × 4\n     id negative positive sentiment_score\n  &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;           &lt;int&gt;\n1     1        1        0              -1\n2     2        0        1               1\n\n\nAnd save that summary as two_reviews_sentiment.\n\ntwo_reviews_sentiment &lt;- two_reviews_tokens |&gt;\n  inner_join(bing_sentiments, by = \"word\") |&gt;\n  count(id, sentiment, sort = TRUE) |&gt;\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |&gt;\n  mutate(sentiment_score = positive - negative)\n\n\nPre-Class Q7\n\nNow, let’s do all of these steps for the revs data frame.\nPerform Tokenization for revs. Save the result as revs_tokens.\n\n\nClick to show code and output\n\n\n\nrevs_tokens &lt;- revs |&gt;\n  unnest_tokens(word, clean_text)\n\n\n\n\nPre-Class Q8\n\nGet sentiments and join them with revs_tokens. Save this as revs_tokens_bing.\n\n\nClick to show code and output\n\n\n\nbing_sentiments &lt;- get_sentiments(\"bing\")\n\nrevs_tokens_bing &lt;- revs_tokens |&gt;\n  inner_join(bing_sentiments, by = \"word\") |&gt;\n  count(id, sentiment, sort = TRUE) \n\nrevs_sentiment &lt;- revs_tokens_bing |&gt;\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |&gt;\n  mutate(sentiment_score = positive - negative)\n\n\n\n\nPre-Class Q9\n\nSummarize revs_tokens_bing. Save this as revs_sentiment.\n\n\nClick to show code and output\n\n\n\nrevs_sentiment &lt;- revs_tokens_bing |&gt;\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |&gt;\n  mutate(sentiment_score = positive - negative)\n\n\n\n\nPre-Class Q10\n\nJoin this back to revs, and save it as revs.\n\n\nClick to show code and output\n\n\n\nrevs &lt;- revs |&gt;\n  left_join(revs_sentiment, by = \"id\")\n\n\n\n\nPre-Class Q11\n\nVisualize the distribution of sentiment.\n\n\n\nMore Bags, More Words\nThis is a super flexible tool. You can look for anything. But, there are more common things to do. For example, Word Frequency Analysis identifies the most common words in reviews.\n\nPre-Class Q12\n\nFind the 10 most common words in revs.\n\n\nClick to show code and output\n\n\n\nword_frequencies &lt;- revs_tokens |&gt;\n  count(word, sort = TRUE)\n\nhead(word_frequencies, 10)\n\n          word    n\n1         stay 2445\n2           br 1382\n3     location 1255\n4        clean 1213\n5         host  931\n6         nice  801\n7  comfortable  783\n8    recommend  736\n9         easy  716\n10        home  657"
  },
  {
    "objectID": "teaching/02 Text Analysis.html#topic-modeling-with-lda",
    "href": "teaching/02 Text Analysis.html#topic-modeling-with-lda",
    "title": "02 - Text Analysis",
    "section": "Topic Modeling with LDA",
    "text": "Topic Modeling with LDA\nLatent Dirichlet Allocation (LDA) is a powerful machine learning method for discovering hidden topics in text data. It groups words that appear together frequently, which you can then label, and use to classify text later. LDA also requires a decent amount of computing power, so I am not going to ask coding questions in this section. I will ask you to discuss my results.\n\nWhat is Topic Modeling?\nTopic modeling is an “unsupervised” machine learning technique that identifies hidden themes or topics in a collection of documents. Unsupervised just means that there is no set outcome. There is no dependent variable. Topic modeling will just show what words appear together often.\nYou can use this to identify key themes, or to classify text into topics. For example: Airbnb reviews that talk about rats vs Airbnb reviews that talk about crime. The key difference between using topic modeling and bag-of-words for classification is that topic modeling should pick up context better than bag-of-words.\nAdvantages:\n\nUnsupervised: No need for labeled data.\nInterpretable: Provides human-readable results.\n\nDisadvantages:\n\nRequires careful preprocessing.\nMay struggle with small datasets or very sparse data.\n\n\n\nImplementation\nTo extract topics, we fit an LDA model to the “document-term matrix”. Specifically, we tell LDA how many topics we want with k =. I am going to ask for four.\nThe document-term matrix is similar to our data frame of tokens, but is summarized (somewhat) at the document level. For us, the document is each review, the term is each word. This document-term matrix tells us, for every word in all reviews, if the word is in that specific review.\n\n\nClick to show code and output\n\n\n\n# Convert to document-term matrix\nrevs_tokens &lt;- revs |&gt;\n  unnest_tokens(word, clean_text) |&gt;\n  count(id, word, sort = TRUE) |&gt; # Count word frequencies by review\n  ungroup()\ndtm &lt;- cast_dtm(revs_tokens, document = id, term = word, value = n)\n\nlda_model &lt;- LDA(dtm, k = 4, control = list(seed = 123))\n\n\n\nOnce this has run, we can identify the most important words for each topic. I am going to show 10.\n\n\nClick to show code and output\n\n\n\n# Extract top words for each topic\ntopics &lt;- tidy(lda_model, matrix = \"beta\")\n\ntop_terms &lt;- topics |&gt;\n  group_by(topic) |&gt;\n  slice_max(beta, n = 10) |&gt;\n  arrange(-beta) |&gt;\n  ungroup() |&gt;\n  mutate(term = factor(term))\n\n# View top words\nggplot(top_terms, aes(x = term, y = beta, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free_y\") +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Terms for Each Topic\",\n    x = \"Terms\",\n    y = \"Beta Value (Probability)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFrom these results, we would have to interpret the topics ourselves.\n\nPre-Class Q12\n\nHow would you label each of these four topics? Is there an obvious distinction between them?\n\nPre-Class Q13\n\nWe have missed something important in our pre-processing here. What do you notice about the words in each topic that we have missed? Alternatively, do you think we should fix this?"
  },
  {
    "objectID": "teaching/02 Text Analysis.html#keyword-analysis-with-tf-idf",
    "href": "teaching/02 Text Analysis.html#keyword-analysis-with-tf-idf",
    "title": "02 - Text Analysis",
    "section": "Keyword Analysis with TF-IDF",
    "text": "Keyword Analysis with TF-IDF\nTF-IDF (Term Frequency-Inverse Document Frequency) is a technique for identifying keywords that are unique to individual documents. This can be useful to distinguish keywords that tell us something meaningful from words that are merely used often.\n\nWhat is TF-IDF?\nTF-IDF measures the importance of a word in a document relative to all of our documents (which is called the corpus). - Formula:\n[(t, d) = (t, d) (t)]\n- Term Frequency (TF): Frequency of the word (t) in document (d). - Inverse Document Frequency (IDF): Logarithm of the ratio of total documents to the number of documents containing (t). - This highlights words that are frequent in one document but rare across others, identifying keywords unique to each document.\nAdvantages:\n\nBalances local and global importance of words.\nUseful for keyword extraction, document summarization, and search.\n\nDisadvantages:\n\nDoes not consider word context or semantics.\nSensitive to noisy or sparse data.\n\n\n\nImplementation\nKeyword analysis at the level of each review can be helpful, but it is potentially more useful to do this at the neighborhood level, with neighbourhood_cleansed.\nTo prepare the data for TF-IDF analysis, we have to tokenize and count word frequencies. We will do that within neighbourhood_cleansed.\n\n\nClick to show code and output\n\n\n\nrevs_tokens &lt;- revs |&gt;\n  unnest_tokens(word, clean_text) |&gt;\n  count(neighbourhood_cleansed, word, sort = TRUE)\n\n# Total word counts per document (for term frequency calculation)\nrevs_tokens &lt;- revs_tokens |&gt;\n  group_by(neighbourhood_cleansed) |&gt;\n  mutate(total_words = sum(n)) |&gt;\n  ungroup()\n\n\n\nThen, we can use functions from the tidytext package to calculate the TF-IDF scores for each word in each neighborhood. In this code, I am also going to only show five keywords per neighborhood.\n\n\nClick to show code and output\n\n\n\n# Compute TF-IDF\ntf_idf &lt;- revs_tokens |&gt;\n  bind_tf_idf(term = word, document = neighbourhood_cleansed, n = n)\n\ntf_idf &lt;- tf_idf |&gt;\n  arrange(desc(tf_idf)) |&gt;\n  select(neighbourhood_cleansed, word, tf_idf) |&gt;\n  group_by(neighbourhood_cleansed) |&gt;\n  slice_max(tf_idf, n = 5) |&gt;\n  ungroup()\n\n# Print top keywords\nhead(tf_idf, 20)\n\n# A tibble: 20 × 3\n   neighbourhood_cleansed word       tf_idf\n   &lt;chr&gt;                  &lt;chr&gt;       &lt;dbl&gt;\n 1 Bayview                tim       0.0102 \n 2 Bayview                jian      0.00880\n 3 Bayview                chris     0.00778\n 4 Bayview                dongmei   0.00635\n 5 Bayview                karl      0.00440\n 6 Bernal Heights         bernal    0.0201 \n 7 Bernal Heights         heights   0.00784\n 8 Bernal Heights         tyler     0.00507\n 9 Bernal Heights         ruben     0.00457\n10 Bernal Heights         cortland  0.00442\n11 Castro/Upper Market    castro    0.0210 \n12 Castro/Upper Market    dolores   0.00725\n13 Castro/Upper Market    todd      0.00554\n14 Castro/Upper Market    ashish    0.00487\n15 Castro/Upper Market    mission   0.00430\n16 Chinatown              staff     0.0137 \n17 Chinatown              motel     0.0127 \n18 Chinatown              mary      0.0101 \n19 Chinatown              gainengs  0.00920\n20 Chinatown              chinatown 0.00658\n\n\n\n\n\n\nWhat makes each neighborhood unique?\nKeyword extraction is most useful in distinguishing things. For example, we might want to know what keywords in each neighborhood are different from others. Here is one way we can do that:\n\nPre-Class Q14\n\nStep 1: Tokenize and count word frequencies by neighborhood\n\n\nClick to show code and output\n\n\n\nrevs_tokens &lt;- revs |&gt; \n  unnest_tokens(word, clean_text) |&gt; \n  count(neighbourhood_cleansed, word, sort = TRUE) |&gt; \n  group_by(neighbourhood_cleansed) |&gt; \n  mutate(total_words = sum(n)) |&gt; \n  ungroup()\n\n\n\n\nPre-Class Q15\n\nStep 2: Compute TF-IDF to identify terms\n\n\nClick to show code and output\n\n\n\ntf_idf &lt;- revs_tokens |&gt; \n  bind_tf_idf(term = word, document = neighbourhood_cleansed, n = n)\n\n\n\n\nPre-Class Q16\n\nStep 3: Calculate “proportional frequencies” for comparison across neighborhoods. This shows how large of a proportion of all reviews in this neighborhood each word is, compared to the same proportion overall.\n\n\nClick to show code and output\n\n\n\ntf_idf &lt;- tf_idf |&gt; \n  mutate(proportion = n / total_words) |&gt; \n  group_by(word) |&gt; \n  mutate(avg_proportion = mean(proportion), \n         z_score = (proportion - avg_proportion) / sd(proportion)) |&gt; \n  ungroup()\n\n\n\n\nPre-Class Q17\n\nStep 4: Identify the top distinctive keyword for each neighborhood.\n\n\nClick to show code and output\n\n\n\ndistinctive_keywords &lt;- tf_idf |&gt; \n  group_by(neighbourhood_cleansed) |&gt; \n  slice_max(z_score, n = 1) |&gt;  # Select most distinctive term\n  arrange(neighbourhood_cleansed, desc(z_score)) |&gt; \n  ungroup()\n\n\n\n\nPre-Class Q18\n\nStep 5: Interpret the results. What do you notice about these neighborhoods, based on their key words?\n\n\nClick to show code and output\n\n\n\nprint(distinctive_keywords)\n\n# A tibble: 36 × 10\n   neighbourhood_cleansed word            n total_words      tf   idf  tf_idf\n   &lt;chr&gt;                  &lt;chr&gt;       &lt;int&gt;       &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Bayview                pretty          8        1694 0.00472 0.251 0.00119\n 2 Bernal Heights         stores         10        3922 0.00255 0.492 0.00126\n 3 Castro/Upper Market    heart          17        5155 0.00330 0.750 0.00247\n 4 Chinatown              free            5         779 0.00642 0.405 0.00260\n 5 Crocker Amazon         entire          2         899 0.00222 0.639 0.00142\n 6 Diamond Heights        spacious        3          84 0.0357  0.150 0.00534\n 7 Downtown/Civic Center  muy            19        3590 0.00529 0.539 0.00285\n 8 Excelsior              smooth          3        2508 0.00120 0.944 0.00113\n 9 Financial District     price           8        1008 0.00794 0.405 0.00322\n10 Glen Park              appreciated     5         661 0.00756 0.216 0.00164\n# ℹ 26 more rows\n# ℹ 3 more variables: proportion &lt;dbl&gt;, avg_proportion &lt;dbl&gt;, z_score &lt;dbl&gt;\n\n\n\n\n\nPre-Class Q19\n\nThis is going to be hard to do. But, for two points, I want you to try to make a map that has each keyword plotted in its neighborhood."
  },
  {
    "objectID": "teaching/Conjoint 465.html#overview",
    "href": "teaching/Conjoint 465.html#overview",
    "title": "Conjoint Analysis",
    "section": "Overview",
    "text": "Overview\n\nConjoint analysis - intro & concepts\n\nProduct design concepts\n\nConjoint analysis - steps to perform\n\nIdentify a set of relevant product attributes\nDefine reasonable levels for those attributes\nCreate product profiles\nObtain consumer preferences for profiles\nAnalyze the data for each respondent\nSimulate market outcomes"
  },
  {
    "objectID": "teaching/Conjoint 465.html#conjoint-analysis-introduction-1",
    "href": "teaching/Conjoint 465.html#conjoint-analysis-introduction-1",
    "title": "Conjoint Analysis",
    "section": "Conjoint Analysis Introduction",
    "text": "Conjoint Analysis Introduction\nGood product design begins with understanding customers\n\n\nA market-oriented organization focuses on identifying and satisfying the needs and wants of its customers\nConjoint analysis is a powerful analytical tool that can aid managers in understanding and quantifying these needs and wants"
  },
  {
    "objectID": "teaching/Conjoint 465.html#product-design-concepts",
    "href": "teaching/Conjoint 465.html#product-design-concepts",
    "title": "Conjoint Analysis",
    "section": "Product design concepts",
    "text": "Product design concepts\n\n\nA product can be considered as a bundle of attributes\n\nAttributes provide value to consumers\n\nEach attribute can attain one or more levels\n\nLevels are specific values of attributes"
  },
  {
    "objectID": "teaching/Conjoint 465.html#product-design-concepts-example",
    "href": "teaching/Conjoint 465.html#product-design-concepts-example",
    "title": "Conjoint Analysis",
    "section": "Product design concepts: Example",
    "text": "Product design concepts: Example\nA computer can be described as:\n\n\n\n\nAttribute\nLevel 1\nLevel 2\nLevel 3\n\n\n\n\n1\nProcessor\n2.4 GHz\n3.2 GHz\n\n\n\n2\nRAM\n8 GB\n16 GB\n32 GB\n\n\n3\nStorage\n520 GB\n1040 GB\n2080 GB\n\n\n4\nPrice\n$400\n$800\n$1,200\n\n\n5\nColor\nGrey\nSilver\nBlack\n\n\n\nSample profiles:\n\n\n2.4 GHz processor, 8 GB RAM, 520 GB hard drive, $400 price, black\n2.4 GHz processor, 16 GB RAM, 520 GB hard drive, $800 price, grey\n3.2 GHz processor, 8 GB RAM, 1040 GB, hard drive, $800 price, silver\n3.2 GHz processor, 32 GB RAM, 1040 GB hard drive, $1,200 price, silver"
  },
  {
    "objectID": "teaching/Conjoint 465.html#how-do-we-know-what-product-people-will-like",
    "href": "teaching/Conjoint 465.html#how-do-we-know-what-product-people-will-like",
    "title": "Conjoint Analysis",
    "section": "How do we know what product people will like?",
    "text": "How do we know what product people will like?\nNeed a reliable method to measure preferences\n\n\nTwo approaches:\n\nStated preference -Directly ask customers how much they value attributes\nRevealed preference -Elicit customer preferences for whole products (profiles) -Infer value of attributes from responses"
  },
  {
    "objectID": "teaching/Conjoint 465.html#advantages-of-revealed-preference-approach",
    "href": "teaching/Conjoint 465.html#advantages-of-revealed-preference-approach",
    "title": "Conjoint Analysis",
    "section": "Advantages of revealed preference approach",
    "text": "Advantages of revealed preference approach\n\n\nEasier for respondents\nHard to answer questions like:\n\n“How much do you value a 4 hour battery life on your VR headset?”\n\nMore accurate\n\nCounters tendency to rate all attributes as (equally) important"
  },
  {
    "objectID": "teaching/Conjoint 465.html#conjoint-revealed-preference-via-experiment",
    "href": "teaching/Conjoint 465.html#conjoint-revealed-preference-via-experiment",
    "title": "Conjoint Analysis",
    "section": "Conjoint: Revealed preference via experiment",
    "text": "Conjoint: Revealed preference via experiment\n\n\nAttributes are considered jointly\n\nSubjects rate/rank/choose between product profiles\nSubjects forced to make trade-offs for attribute levels\nTrade-offs reveal true valuation\n\nLimitations\n\nAttributes must be known in advance\nHard to handle a large (&gt;10) number of attributes"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stages-in-conjoint-analysis",
    "href": "teaching/Conjoint 465.html#stages-in-conjoint-analysis",
    "title": "Conjoint Analysis",
    "section": "Stages in conjoint analysis",
    "text": "Stages in conjoint analysis\n\n\nIdentify a set of relevant product attributes\nDefine reasonable levels for those attributes\nCreate product profiles\nObtain consumer preferences for profiles\nAnalyze the data for each respondent\nSimulate market outcomes"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stages-1-2-attribute-level-selection-guidelines",
    "href": "teaching/Conjoint 465.html#stages-1-2-attribute-level-selection-guidelines",
    "title": "Conjoint Analysis",
    "section": "Stages 1 & 2: Attribute & level selection guidelines",
    "text": "Stages 1 & 2: Attribute & level selection guidelines\nAttributes in conjoint should be:\n\n\nClear and unambiguous\nActionable by the firm\nThe total number of attributes should be kept low\n\n5-6 is the average, most studies fall between 4 and 8\n\nLevels should span the realistic range of possible values\n\nE.g., include price levels close to min and max of market prices\nUse qualitative research and pretests to decide on attributes and levels"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stages-1-2-example-vr-headset",
    "href": "teaching/Conjoint 465.html#stages-1-2-example-vr-headset",
    "title": "Conjoint Analysis",
    "section": "Stages 1 & 2 example: VR Headset",
    "text": "Stages 1 & 2 example: VR Headset\n\nStandalone:\n\nYes vs no\n\nCellular network access:\n\nYes vs no\n\nPrice:\n\n$400 vs $800 vs $1,200\n\nBattery life:\n\n4 hours vs 8 hours vs 12 hours"
  },
  {
    "objectID": "teaching/Conjoint 465.html#how-many-possible-profiles-are-there",
    "href": "teaching/Conjoint 465.html#how-many-possible-profiles-are-there",
    "title": "Conjoint Analysis",
    "section": "How many possible profiles are there",
    "text": "How many possible profiles are there\n2 attributes with 3 levels and 2 with 2 levels?\n\n\\(3 \\times 3 \\times 2 \\times 2\\)\n\n\n\\(= 36\\)"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-3-create-product-profiles",
    "href": "teaching/Conjoint 465.html#stage-3-create-product-profiles",
    "title": "Conjoint Analysis",
    "section": "Stage 3: Create product profiles",
    "text": "Stage 3: Create product profiles\nData collection needs simplification (why?)\n\n\nShrink total set to only include possible/realistic products\n\nNo impossible combinations\n\nUnrealistic products can ruin our data"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-4-obtain-consumer-preferences-for-profiles",
    "href": "teaching/Conjoint 465.html#stage-4-obtain-consumer-preferences-for-profiles",
    "title": "Conjoint Analysis",
    "section": "Stage 4: Obtain consumer preferences for profiles",
    "text": "Stage 4: Obtain consumer preferences for profiles\n\n\nRating\n\nConsumers see a set of profiles, rate each\n\nPositives?\n\nPeople are used to rating things\nSimple\nStraightforward\n\nNegatives?\n\nNot a choice! (Not economically relevant necessarily)\nRatings are not necessarily comparable across products\n\n\n\nRanking\nChoice"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-4-obtain-consumer-preferences-for-profiles-1",
    "href": "teaching/Conjoint 465.html#stage-4-obtain-consumer-preferences-for-profiles-1",
    "title": "Conjoint Analysis",
    "section": "Stage 4: Obtain consumer preferences for profiles",
    "text": "Stage 4: Obtain consumer preferences for profiles\n\n\nRating\nRanking\n\nPeople see all choices and rank them\n\nPositives?\n\nSimple\nStraightforward\n\nNegatives?\n\nStill not a choice\nWhat if things aren’t evenly spaced?\n\n\n\nChoice"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-4-obtain-consumer-preferences-for-profiles-2",
    "href": "teaching/Conjoint 465.html#stage-4-obtain-consumer-preferences-for-profiles-2",
    "title": "Conjoint Analysis",
    "section": "Stage 4: Obtain consumer preferences for profiles",
    "text": "Stage 4: Obtain consumer preferences for profiles\n\n\nRating\nRanking\nChoice\n\nPeople see options (usually 2+ at a time) and choose\n\nPositives?\n\nReal!\nPotentially simple\n\nNegatives?\n\nNeed a lot of trials"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-5-data-analysis",
    "href": "teaching/Conjoint 465.html#stage-5-data-analysis",
    "title": "Conjoint Analysis",
    "section": "Stage 5: Data analysis",
    "text": "Stage 5: Data analysis\n\n\nTypically, conducted at the individual level\nRecovers a set of utility parameter estimates for each individual\nMode of analysis dependent on response format:\n\nRatings data — linear regression (OLS)"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-5-data-analysis-1",
    "href": "teaching/Conjoint 465.html#stage-5-data-analysis-1",
    "title": "Conjoint Analysis",
    "section": "Stage 5: Data analysis",
    "text": "Stage 5: Data analysis\nRead in these responses to a conjoint survey for tablet computers\n\nresponses_DF &lt;- read.csv(\"respondent_data.csv\") # survey reponses (\"Y\" variables)\nN &lt;- nrow(responses_DF) # number of subjects\nsummary(responses_DF)\n\n respondent_id      profile_1   profile_2       profile_3      profile_4   \n Min.   :  1.00   Min.   :1   Min.   :1.000   Min.   :1.00   Min.   :1.00  \n 1st Qu.: 39.50   1st Qu.:3   1st Qu.:4.000   1st Qu.:2.00   1st Qu.:1.00  \n Median : 73.00   Median :4   Median :5.000   Median :3.00   Median :2.00  \n Mean   : 74.17   Mean   :4   Mean   :4.675   Mean   :3.07   Mean   :2.14  \n 3rd Qu.:113.75   3rd Qu.:5   3rd Qu.:6.000   3rd Qu.:4.00   3rd Qu.:3.00  \n Max.   :145.00   Max.   :7   Max.   :7.000   Max.   :7.00   Max.   :7.00  \n   profile_5       profile_6      profile_7       profile_8       profile_9    \n Min.   :1.000   Min.   :1.00   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:3.000   1st Qu.:2.25   1st Qu.:1.000   1st Qu.:2.000   1st Qu.:1.000  \n Median :4.000   Median :4.00   Median :2.000   Median :3.000   Median :1.000  \n Mean   :3.544   Mean   :3.86   Mean   :2.193   Mean   :2.974   Mean   :1.789  \n 3rd Qu.:4.000   3rd Qu.:5.00   3rd Qu.:3.000   3rd Qu.:4.000   3rd Qu.:2.000  \n Max.   :7.000   Max.   :7.00   Max.   :7.000   Max.   :7.000   Max.   :6.000  \n   profile_10      profile_11      profile_12      profile_13   \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:7.000   1st Qu.:2.000   1st Qu.:4.000   1st Qu.:2.000  \n Median :7.000   Median :3.000   Median :5.000   Median :3.000  \n Mean   :6.588   Mean   :3.114   Mean   :4.772   Mean   :3.202  \n 3rd Qu.:7.000   3rd Qu.:4.000   3rd Qu.:6.000   3rd Qu.:4.000  \n Max.   :7.000   Max.   :6.000   Max.   :7.000   Max.   :7.000  \n   profile_14      profile_15      profile_16      profile_17      profile_18  \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.00  \n 1st Qu.:1.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.00  \n Median :2.000   Median :3.000   Median :4.000   Median :3.000   Median :3.00  \n Mean   :2.351   Mean   :3.158   Mean   :3.693   Mean   :3.386   Mean   :3.14  \n 3rd Qu.:3.000   3rd Qu.:4.000   3rd Qu.:5.000   3rd Qu.:4.000   3rd Qu.:4.00  \n Max.   :5.000   Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.00"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-5-data-analysis-2",
    "href": "teaching/Conjoint 465.html#stage-5-data-analysis-2",
    "title": "Conjoint Analysis",
    "section": "Stage 5: Data analysis",
    "text": "Stage 5: Data analysis\nFor tablet computers, a relevant set of product attributes might be:\n\nScreen size (inches)\nCellular network connectivity\nPrice\nBattery life (hrs)\nOperating system (OS)"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-5-data-analysis-3",
    "href": "teaching/Conjoint 465.html#stage-5-data-analysis-3",
    "title": "Conjoint Analysis",
    "section": "Stage 5: Data analysis",
    "text": "Stage 5: Data analysis\nFor tablet computers, a relevant set of product attributes might be:\n\n\n\n\n\n\n\n\n\n\n\nAttribute\n1\n2\n3\n4\n5\n\n\n\n\nLevel\nScreen (in)\nCell\nPrice ($)\nBattery (hr)\nOS\n\n\n1\n7\nN\n100\n4\nAndroid\n\n\n2\n10\nY\n300\n8\niOS\n\n\n3\n\n\n500\n12\nWindows"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-5-data-analysis-4",
    "href": "teaching/Conjoint 465.html#stage-5-data-analysis-4",
    "title": "Conjoint Analysis",
    "section": "Stage 5: Data analysis",
    "text": "Stage 5: Data analysis\nFor tablet computers, a relevant set of product combinations might be:\n\n\n\n\n\n\n\n\n\n\n\nProfile #\nScreen (in)\nCell\nPrice ($)\nBattery (hr)\nOS\n\n\n\n\n1\n7\nY\n300\n12\niOS\n\n\n2\n7\nY\n100\n8\niOS\n\n\n3\n10\nY\n500\n12\nWindows\n\n\n4\n7\nY\n300\n4\nWindows\n\n\n5\n7\nN\n300\n8\nAndroid\n\n\n6\n10\nN\n300\n12\niOS\n\n\n7\n7\nN\n500\n12\nWindows\n\n\n8\n10\nN\n300\n8\nWindows\n\n\n9\n7\nN\n500\n4\niOS\n\n\n10\n10\nY\n100\n12\nAndroid\n\n\n11\n10\nY\n300\n4\nAndroid\n\n\n12\n7\nN\n100\n12\nAndroid\n\n\n13\n10\nY\n500\n8\niOS\n\n\n14\n10\nN\n500\n4\nAndroid\n\n\n15\n10\nN\n100\n4\niOS\n\n\n16\n10\nN\n100\n8\nWindows\n\n\n17\n7\nY\n500\n8\nAndroid\n\n\n18\n7\nY\n100\n4\nWindows"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-5-data-analysis-5",
    "href": "teaching/Conjoint 465.html#stage-5-data-analysis-5",
    "title": "Conjoint Analysis",
    "section": "Stage 5: Data analysis",
    "text": "Stage 5: Data analysis\n\ndesign_DF &lt;- read.csv(\"survey_design.csv\") # survey design (\"X\" variables)\ndesign_DF\n\n   Screen Cell Price Battery      OS\n1       7    Y   300      12 Windows\n2       7    Y   100       8 Windows\n3      10    Y   500      12 Android\n4       7    Y   300       4 Android\n5       7    N   300       8     iOS\n6      10    N   300      12 Windows\n7       7    N   500      12 Android\n8      10    N   300       8 Android\n9       7    N   500       4 Windows\n10     10    Y   100      12     iOS\n11     10    Y   300       4     iOS\n12      7    N   100      12     iOS\n13     10    Y   500       8 Windows\n14     10    N   500       4     iOS\n15     10    N   100       4 Windows\n16     10    N   100       8 Android\n17      7    Y   500       8     iOS\n18      7    Y   100       4 Android"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-5-data-analysis-6",
    "href": "teaching/Conjoint 465.html#stage-5-data-analysis-6",
    "title": "Conjoint Analysis",
    "section": "Stage 5: Data analysis",
    "text": "Stage 5: Data analysis\nLet’s say we have a customer who responds to those profiles like this:\n\nresponse &lt;- c(3,4,4,2,4,4,3,4,1,7,4,6,2,3,4,6,4,5)\n\n\n\nest_DF &lt;- cbind(design_DF, response=response)\n\nlm1 &lt;- lm(response ~ factor(Screen) + factor(Cell) + factor(Price) + \n            factor(Battery) + factor(OS), \n          data=est_DF)\n\nsummary(lm1)\n\n\nCall:\nlm(formula = response ~ factor(Screen) + factor(Cell) + factor(Price) + \n    factor(Battery) + factor(OS), data = est_DF)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.58889 -0.18681 -0.01806  0.12778  0.57778 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         4.3472     0.3164  13.742 2.41e-07 ***\nfactor(Screen)10    0.6750     0.2093   3.225  0.01041 *  \nfactor(Cell)Y       0.0750     0.2093   0.358  0.72839    \nfactor(Price)300   -1.8333     0.2548  -7.195 5.11e-05 ***\nfactor(Price)500   -2.5000     0.2548  -9.812 4.19e-06 ***\nfactor(Battery)8    0.8333     0.2548   3.271  0.00967 ** \nfactor(Battery)12   1.3333     0.2548   5.233  0.00054 ***\nfactor(OS)iOS       0.6667     0.2548   2.617  0.02797 *  \nfactor(OS)Windows  -1.0000     0.2548  -3.925  0.00349 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4413 on 9 degrees of freedom\nMultiple R-squared:  0.9536,    Adjusted R-squared:  0.9124 \nF-statistic: 23.12 on 8 and 9 DF,  p-value: 3.972e-05"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-5-data-analysis-7",
    "href": "teaching/Conjoint 465.html#stage-5-data-analysis-7",
    "title": "Conjoint Analysis",
    "section": "Stage 5: Data analysis",
    "text": "Stage 5: Data analysis\nRatings Data\n\n\nIntercept captures utility of “baseline” profile\n\nBaseline profile corresponds to all omitted attribute levels\n\nPart-worths measure incremental utility relative to the baseline\n\nPart-worths of included attribute levels = regression coefficients\nPart-worths of omitted attribute levels = 0 by definition"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-5-data-analysis-8",
    "href": "teaching/Conjoint 465.html#stage-5-data-analysis-8",
    "title": "Conjoint Analysis",
    "section": "Stage 5: Data analysis",
    "text": "Stage 5: Data analysis\nRatings Data\n\n\nWhat is the baseline profile?\n\nThe baseline profile is the profile associated with the omitted factor levels from the dummy variable coding. In this case, the baseline profile is a 7” screen, no cellular connectivity, $100 price, 4 hour battery life, and Android OS.\n\nWhat is the expected utility (rating) for the baseline profile?\n\nThe utility/rating for this profile is given by the intercept estimate.\n4.3472\n\nInterpret the coefficients from the regression\n\nDummy variable coefficients represent in the incremental utility (rating), relative to the baseline profile\ne.g., the coefficient on factor(Screen)10 represents the incremental utility of changing from a 7” screen to a 10” screen"
  },
  {
    "objectID": "teaching/Conjoint 465.html#step-5-data-analysis",
    "href": "teaching/Conjoint 465.html#step-5-data-analysis",
    "title": "Conjoint Analysis",
    "section": "Step 5: Data analysis",
    "text": "Step 5: Data analysis\nGenerate similar estimates for each individual represented in our supplemental data, responses_DF\n\nHow?\n\n\n\nCreate an empty list-of-lists to hold the regression results for each individual\n\nlm_res &lt;- vector(mode = \"list\", length=nrow(responses_DF))\n\nLoop over individuals. For each individual i:\n\nCreate a dataframe that combines design_DF with the responses for individual i.\n\nEstimate a linear model with individual i’s responses as the dependent variable, and the columns of design_DF as (factor) regressors\nStore the linear model results to the i’th element of lm_res.\n\nTo access or assign (top level) lists in a list-of-lists, we must use double bracket indexing, as in: lm_res[[i]]"
  },
  {
    "objectID": "teaching/Conjoint 465.html#step-5-data-analysis-1",
    "href": "teaching/Conjoint 465.html#step-5-data-analysis-1",
    "title": "Conjoint Analysis",
    "section": "Step 5: Data analysis",
    "text": "Step 5: Data analysis\n\n## estimate regression models, 1 per individual\n# initialize an empty \"list of lists\" to hold lm() regression results\nlm_res &lt;- vector(mode=\"list\", length=nrow(responses_DF))\n# loop over subjects\nfor (i in 1:nrow(responses_DF)) {\n  # get survey reponses for subject i (dropping respondent_id)\n  response = as.numeric(responses_DF[i,2:ncol(responses_DF)])\n  # create \"estimation\" dataframe, including i's reponses (Y) and the design variables (X)\n  est_DF = cbind(design_DF, response=response)\n  # run regression for subject i, store as lm_res[[i]]\n  # note use of double bracket indexing [[ ]] syntax to access top-level lists\n  lm_res[[i]] = lm(response ~ factor(Screen) + factor(Cell) + factor(Price) + \n                     factor(Battery) + factor(OS),\n                   data=est_DF)\n  }"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-6-simulate-market-outcomes",
    "href": "teaching/Conjoint 465.html#stage-6-simulate-market-outcomes",
    "title": "Conjoint Analysis",
    "section": "Stage 6: Simulate market outcomes",
    "text": "Stage 6: Simulate market outcomes\n\n\nConsider a hypothetical market\n\nStart with closest approximation to current competitive landscape\nAdd 1st new product under consideration for introduction\n\nCompute expected utility for each of the products, for each subject in the sample\nPredict market shares (or units sold) for all products:\n\nAssume subjects choose product with the highest utility\nProduct share = fraction of subjects who choose that product\n\nRepeat steps above for remaining new product designs\n\nIntroduce product concept with highest expected profit/market share"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-6",
    "href": "teaching/Conjoint 465.html#stage-6",
    "title": "Conjoint Analysis",
    "section": "Stage 6",
    "text": "Stage 6\n\n\nConjoint allows us to evaluate “what if” scenarios\n\nHow would a hypothetical “new” product would fare in competition with existing products?\n\nIn the context of our example\n\nHow a new product by Toshiba will compete against the existing iPad\nAssume the existing “iPad” product corresponds to:\n\nScreen = 10 (inches)\nCell = “Y” (has cell connectivity)\nPrice = 500 ($)\nBattery = 8 (hrs)\nOS = “iOS”"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-6-1",
    "href": "teaching/Conjoint 465.html#stage-6-1",
    "title": "Conjoint Analysis",
    "section": "Stage 6",
    "text": "Stage 6\nAssume (initially) that Toshiba is considering one potential product design, Toshiba_A\n\n\nScreen = 7 (inches)\nCell = “N” (no cell connectivity)\nPrice = 300 ($)\nBattery = 12 (hrs)\nOS = “Android”\nThe cost to produce a tablet computer is:\n\n\\((75 + 5\\times(Screen==10) + 20\\times(Cell==Y) + 5\\times(Battery==8) + 15\\times(Battery==12))\\)"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-6-2",
    "href": "teaching/Conjoint 465.html#stage-6-2",
    "title": "Conjoint Analysis",
    "section": "Stage 6",
    "text": "Stage 6\nDefine Toshiba_A as the first product and the iPad as the second in a dataframe:\n\nprods1 &lt;- data.frame(Screen = c(7,10),\n                     Cell = c(\"N\",\"Y\"),\n                     Price = c(300,500),\n                     Battery = c(12,8),\n                     OS = c(\"Android\",\"iOS\"))\n\nrownames(prods1) = c(\"Toshiba_A\",\"iPad\")\nprods1\n\n          Screen Cell Price Battery      OS\nToshiba_A      7    N   300      12 Android\niPad          10    Y   500       8     iOS"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-6-3",
    "href": "teaching/Conjoint 465.html#stage-6-3",
    "title": "Conjoint Analysis",
    "section": "Stage 6",
    "text": "Stage 6\nUse lm1 to predict a choice between the iPad and Toshiba_A\nHints:\n\nCompute the utility (rating) that each consumer would derive from each alternative. You can use predict() for this task.\nPredict choice as the product with the highest utility (rating). The which.max() function can be useful here\n\n\n\nratings = predict(lm1, newdata=prods1)\nchoice = which.max(ratings)\nP = prods1[1,\"Price\"]\nMC = (75 + 5*(prods1[1,\"Screen\"]==10) + 20*(prods1[1,\"Cell\"]==\"Y\") + 5*(prods1[1,\"Battery\"]==8) + 15*(prods1[1,\"Battery\"]==12))\nprofit = (choice==1)*(P-MC)"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-6-4",
    "href": "teaching/Conjoint 465.html#stage-6-4",
    "title": "Conjoint Analysis",
    "section": "Stage 6",
    "text": "Stage 6\nDiscussion\n\n\nWhich product did the model predict we would choose?\n\niPad\n\nWhat is the cost of Toshiba_A?\n\nToshiba_A costs \\(75 + 15 = \\$90\\)"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-6-5",
    "href": "teaching/Conjoint 465.html#stage-6-5",
    "title": "Conjoint Analysis",
    "section": "Stage 6",
    "text": "Stage 6\nPredict choices for supplemental survey respones\n\n\nCreate a function called tablet_pft1 that takes as inputs:\n\nlm_res containing regression results for all subjects\nprods, a dataframe containing product definitions, as in the previous section\n\ntablet_pft1 should return:\n\nprofit - the profit for Toshiba’s product (product 1)\ncost - the cost of Toshiba’s product (product 1)\nchoices - predicted product choices for each subject in responses_DF (a list)"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-6-6",
    "href": "teaching/Conjoint 465.html#stage-6-6",
    "title": "Conjoint Analysis",
    "section": "Stage 6",
    "text": "Stage 6\nPredict choices for supplemental survey respones\n\n## function: tablet_pft1\n# inputs:\n# lm_res = list of lm() regression results\n# prods_DF = dataframe of product definitions\n# outputs:\n# choices = predicted choice of each subject (list)\n# cost = cost of product 1\n# profit = profit of product 1\ntablet_pft1 &lt;- function(lm_res, prods_DF) {\n  # initialize\n  N = length(lm_res) # number subjects\n  choices = rep(0,N)\n  # loop over subjects: predict ratings/utilities, determine expected choices\n  for (i in 1:N) {\n    ratings = predict(lm_res[[i]], newdata=prods_DF)\n    choices[i] = which.max(ratings)\n    }\n  # calculate demand, cost, profits for product 1 (1st row in prods_DF)\n  Q = sum(choices==1)\n  P = prods_DF[1,\"Price\"]\n  MC = (75 + 5*(prods_DF[1,\"Screen\"]==10) + 20*(prods_DF[1,\"Cell\"]==\"Y\") + \n          5*(prods_DF[1,\"Battery\"]==8) + 15*(prods_DF[1,\"Battery\"]==12))\n  pft = Q*(P-MC)\n  # return values\n  return(list(choices=choices, cost=MC, profit=pft))\n  }"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-6-7",
    "href": "teaching/Conjoint 465.html#stage-6-7",
    "title": "Conjoint Analysis",
    "section": "Stage 6",
    "text": "Stage 6\nUse tablet_pft1 to report:\n\nprofits for Toshiba_A\ndemand (expected units sold) for Toshiba_A\nthe cost of Toshiba_A\nHints\n\nFor each subject, compute the utility (rating) that each consumer would derive from each alternative\nFor each subject, predict choice as the product with the highest utility (rating)\n\n\n\n# call tablet_pft1 using prods1\nprods1.results &lt;- tablet_pft1(lm_res, prods1)\nprods1.results$profit # profits\n\n[1] 7350\n\nsum(prods1.results$choices==1) # units sold\n\n[1] 35\n\nprods1.results$cost # marginal cost\n\n[1] 90"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-6-8",
    "href": "teaching/Conjoint 465.html#stage-6-8",
    "title": "Conjoint Analysis",
    "section": "Stage 6",
    "text": "Stage 6\nToshiba_B alternative product\n\n\nAssume Toshiba is considering between releasing the Toshiba_A product and Toshiba_B\n\nScreen = 10 (inches)\nCell = “N” (no cell connectivity)\nPrice = 300 ($)\nBattery = 12 (hrs)\nOS = “Android”\n\nRepeat the analysis above, assuming the iPad competes against Toshiba_B only"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-6-9",
    "href": "teaching/Conjoint 465.html#stage-6-9",
    "title": "Conjoint Analysis",
    "section": "Stage 6",
    "text": "Stage 6\nToshiba_B alternative product\n\nprods2 &lt;- data.frame(Screen=c(10,10),\n                    Cell=c(\"N\",\"Y\"),\n                    Price=c(300,500),\n                    Battery=c(12,8),\n                    OS=c(\"Android\",\"iOS\"))\nrownames(prods1) &lt;- c(\"Toshiba_B\",\"iPad\")"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-6-10",
    "href": "teaching/Conjoint 465.html#stage-6-10",
    "title": "Conjoint Analysis",
    "section": "Stage 6",
    "text": "Stage 6\nToshiba_B alternative product\n\nprods2.results &lt;- tablet_pft1(lm_res, prods2)\nprods2.results$profit # profits\n\n[1] 9635\n\nsum(prods2.results$choices==1) # units sold\n\n[1] 47\n\nprods2.results$cost # marginal cost\n\n[1] 95"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-6-11",
    "href": "teaching/Conjoint 465.html#stage-6-11",
    "title": "Conjoint Analysis",
    "section": "Stage 6",
    "text": "Stage 6\nToshiba_B alternative product\n\n\nWhich product should Toshiba introduce?\n\nThe profit from Toshiba_B is higher ($9635 vs $7350), so Toshiba should go with product B"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-6-12",
    "href": "teaching/Conjoint 465.html#stage-6-12",
    "title": "Conjoint Analysis",
    "section": "Stage 6",
    "text": "Stage 6\nSearch over all Android OS alternative products\n\n\nThe trick here is first to enumerate all possible Android-based tablet designs. The expand.grid() function is useful for this purpose\nAfter enumerating all possible Android-based tablet designs, loop over the designs and compute profits as before. Store the profit values in a list.\nFind the element of the profit list with the highest profit. Use the index value for this profile to print the associated product attribute levels."
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-6-13",
    "href": "teaching/Conjoint 465.html#stage-6-13",
    "title": "Conjoint Analysis",
    "section": "Stage 6",
    "text": "Stage 6\n\n# create dataframe will all combinations of screen/cell/price/battery\nallprods_DF &lt;- expand.grid(unique(design_DF$Screen),\n                           unique(design_DF$Cell),\n                           unique(design_DF$Price),\n                           unique(design_DF$Battery))\ncolnames(allprods_DF) &lt;- c(\"Screen\",\"Cell\",\"Price\",\"Battery\")"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-6-14",
    "href": "teaching/Conjoint 465.html#stage-6-14",
    "title": "Conjoint Analysis",
    "section": "Stage 6",
    "text": "Stage 6\n\nNp &lt;- nrow(allprods_DF) # number of products to test\n# calculate profits for each candidate product, assuming iPad competition\npft &lt;- rep(0,Np)\nfor (i in 1:Np) {\n  prods = data.frame(Screen=c(allprods_DF[i,\"Screen\"],10),\n                     Cell=c(as.character(allprods_DF[i,\"Cell\"]),\"Y\"),\n                     Price=c(allprods_DF[i,\"Price\"],500),\n                     Battery=c(allprods_DF[i,\"Battery\"],8),\n                     OS=c(\"Android\",\"iOS\"))\n  prods.results &lt;- tablet_pft1(lm_res, prods)\n  pft[i] &lt;- prods.results$profit\n}"
  },
  {
    "objectID": "teaching/Conjoint 465.html#stage-6-15",
    "href": "teaching/Conjoint 465.html#stage-6-15",
    "title": "Conjoint Analysis",
    "section": "Stage 6",
    "text": "Stage 6\n\n# profit\nmax(pft)\n\n[1] 15015\n\n# change from Toshiba_B\nmax(pft)-prods2.results$profit\n\n[1] 5380\n\n100*(max(pft)-prods2.results$profit)/prods2.results$profit\n\n[1] 55.83809\n\n# profit-maximizing product\nallprods_DF[which.max(pft),]\n\n   Screen Cell Price Battery\n10     10    Y   500      12"
  },
  {
    "objectID": "teaching/Conjoint 465.html#guidelines",
    "href": "teaching/Conjoint 465.html#guidelines",
    "title": "Conjoint Analysis",
    "section": "Guidelines",
    "text": "Guidelines\n\nDo not use too many attributes (&lt;= 6)\n\nMore attributes increases the burden on consumers in at least two ways\n\nLonger questionnaires\nMakes each question harder to answer, because products become harder to evaluate\n\n\nFocus on the attributes for which managerial decisions need to be made\nDo not use subjective attributes or ambiguous level descriptions\nAvoid infeasible combinations"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#segmentation-in-marketing",
    "href": "teaching/Segmentation Slides.html#segmentation-in-marketing",
    "title": "Segmentation",
    "section": "Segmentation in Marketing",
    "text": "Segmentation in Marketing\n\nObjectives\nSteps\n\nData selection\nDistance measures\nClustering algorithms\nSelecting the number of clusters\nProfiling of clusters"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#segmentation-in-marketing-1",
    "href": "teaching/Segmentation Slides.html#segmentation-in-marketing-1",
    "title": "Segmentation",
    "section": "Segmentation in Marketing",
    "text": "Segmentation in Marketing\n\n\nMarket segmentation is one of the most fundamental marketing concepts:\n\nGrouping people (with the willingness and ability to buy) according to their similarity on dimensions related to product(s) under consideration\n\nBetter segments chosen -&gt; better success\n\nCustomize products and distribution strategies for target segments\n\nHigher customer satisfaction, retention\n\nCustomize promotions for target segments\n\nHigher customer acquisition, retention, up-selling\n\nCustomize prices for target segments\n\nExtract as much $ from targeted customers as possible"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#segmentation-marketers-dilemma",
    "href": "teaching/Segmentation Slides.html#segmentation-marketers-dilemma",
    "title": "Segmentation",
    "section": "Segmentation: Marketer’s dilemma",
    "text": "Segmentation: Marketer’s dilemma\nWe’re making a trade-off as marketers between:\n\n\nMarket segmentation (Heterogeneous market)\nMarket aggregation (Homogeneous market)"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#segmentation-marketers-dilemma-1",
    "href": "teaching/Segmentation Slides.html#segmentation-marketers-dilemma-1",
    "title": "Segmentation",
    "section": "Segmentation: Marketer’s dilemma",
    "text": "Segmentation: Marketer’s dilemma\n\nMarket segmentation (Heterogeneous market)\n\nHigher revenue\nIndividually-customized products\n\nHigh production/admin costs\nAppeals to many\nPriced near consumer WTP\n\n\nMarket aggregation (Homogeneous market)\n\nLower costs\nSingle product\n\nLow production/admin costs\nDoes not appeal to some\nPriced too low for some\n\n\nForming a few market segments strikes balance between extremes"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#segmentation",
    "href": "teaching/Segmentation Slides.html#segmentation",
    "title": "Segmentation",
    "section": "Segmentation",
    "text": "Segmentation\nCriteria\n\n\nSubstantial\n\nMeasurable market size\nLarge enough to warrant serving\n\nActionable\n\nSegment characteristics can be translated into targeted marketing policies\n\ne.g., age/income differences suggest different promotional vehicles\n\nTargeted policies must be consistent with firm abilities\n\nDifferential\n\nDifferences between segments should be clearly defined\nSegment-specific marketing policies can be implemented without overlap"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#segmentation-1",
    "href": "teaching/Segmentation Slides.html#segmentation-1",
    "title": "Segmentation",
    "section": "Segmentation",
    "text": "Segmentation\nApproaches\n\n\nManual\n\nManagerial experience, industry norms used to determine segments\nBad\n\nAutomatic\n\nForm segments using data-driven techniques\n\nHybrid\n\nCombination of manual & automatic (often best)\n\nCluster analysis is the most commonly used data-driven technique for segmenting customer data"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#cluster-analysis-1",
    "href": "teaching/Segmentation Slides.html#cluster-analysis-1",
    "title": "Segmentation",
    "section": "Cluster analysis",
    "text": "Cluster analysis\n\n\nWhat?\n\nGrouping of objects (e.g. customers) by similarity of attributes (variables)\nObjects in a group similar, distinct from objects in other groups\nNatural relationship to market segmentation\n\nHow?\n\nCalculates pairwise similarity measures\n\nSome measure of “distance” between objects (customers)\nMany possible distance metrics (Euclidian, Gower, etc.)\n\nSearches for “best” groupings using a clustering method (algorithm)\n\nMany possible clustering methods (k-means, hierarchical, etc.)"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#cluster-analysis-2",
    "href": "teaching/Segmentation Slides.html#cluster-analysis-2",
    "title": "Segmentation",
    "section": "Cluster analysis",
    "text": "Cluster analysis\nSteps\n\n\nSelect variables to use for clustering\nDefine distance measure between individuals (Euclidean, Gower, etc.)\nSelect clustering procedure (k-means, hierarchical, etc.)\nSelect number of clusters\nInterpret and profile the clusters"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-1-select-variables",
    "href": "teaching/Segmentation Slides.html#step-1-select-variables",
    "title": "Segmentation",
    "section": "Step 1: Select variables",
    "text": "Step 1: Select variables\nData sources\n\n\nPast behavior & derived metrics (firm’s CRM/transactions database)\n\nPast expenditure levels\nRecency & frequency of purchase\nLifetime value (CLV)\n\nPreference measures (CRM, survey/experimental research)\n\nConduct individual demand analyses, recover individual-specific parameters\nUse individual-specific parameters as input to cluster analysis\n\nDemographic variables (CRM database, Census records)\n\nDirectly observed — often limited, but sometimes directly observe gender, age, etc.\nImputed from geography — e.g., use knowledge of home ZIP to match Census demographics for region"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-1-select-variables-1",
    "href": "teaching/Segmentation Slides.html#step-1-select-variables-1",
    "title": "Segmentation",
    "section": "Step 1: Select variables",
    "text": "Step 1: Select variables\n\nDF &lt;- read.csv('apparel_customer_data.csv')\n\n\n\nsummary(DF)\n\n      iid         spend_online      spend_retail          age       \n Min.   :   14   Min.   :   0.00   Min.   :   0.00   Min.   :18.00  \n 1st Qu.: 2946   1st Qu.:   0.00   1st Qu.:   0.00   1st Qu.:33.00  \n Median : 5430   Median :  14.97   Median :  27.71   Median :41.00  \n Mean   : 5463   Mean   :  72.44   Mean   :  78.00   Mean   :40.91  \n 3rd Qu.: 8110   3rd Qu.:  70.72   3rd Qu.:  78.00   3rd Qu.:49.00  \n Max.   :10589   Max.   :1985.75   Max.   :2421.91   Max.   :88.00  \n     white           college            male           hh_inc       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :  2.499  \n 1st Qu.:0.7297   1st Qu.:0.3835   1st Qu.:0.000   1st Qu.: 59.356  \n Median :0.8550   Median :0.5580   Median :0.000   Median : 87.364  \n Mean   :0.7993   Mean   :0.5437   Mean   :0.091   Mean   : 96.254  \n 3rd Qu.:0.9422   3rd Qu.:0.7136   3rd Qu.:0.000   3rd Qu.:122.602  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.000   Max.   :250.001"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-1-select-variables-2",
    "href": "teaching/Segmentation Slides.html#step-1-select-variables-2",
    "title": "Segmentation",
    "section": "Step 1: Select variables",
    "text": "Step 1: Select variables\nData pre-processing\n\n\nClustering algorithms typically perform better on normally distributed data"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-1-select-variables-3",
    "href": "teaching/Segmentation Slides.html#step-1-select-variables-3",
    "title": "Segmentation",
    "section": "Step 1: Select variables",
    "text": "Step 1: Select variables\nData pre-processing\n\n\nClustering algorithms typically perform better on normally distributed data\nOften, we “normalize” data, using two techniques:\n\nLog-transform of highly skewed variables — to reduce skew & outlier influence\nStandardization of variables — de-mean and rescale variance to 1\n\nBest practice is to generate histograms of potential cluster variables\n\nInfer from distribution plots which variables to log-transform"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-1-select-variables-4",
    "href": "teaching/Segmentation Slides.html#step-1-select-variables-4",
    "title": "Segmentation",
    "section": "Step 1: Select variables",
    "text": "Step 1: Select variables\nData pre-processing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDF$log_spend_online &lt;- log(1+DF$spend_online)"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-1-select-variables-5",
    "href": "teaching/Segmentation Slides.html#step-1-select-variables-5",
    "title": "Segmentation",
    "section": "Step 1: Select variables",
    "text": "Step 1: Select variables\nData pre-processing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDF$log_spend_retail &lt;- log(1+DF$spend_retail)"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-2-define-distance-between-individuals",
    "href": "teaching/Segmentation Slides.html#step-2-define-distance-between-individuals",
    "title": "Segmentation",
    "section": "Step 2: Define distance between individuals",
    "text": "Step 2: Define distance between individuals\n\n\nWe measure similarity between two customers by calculating the “distance” between them\n\nDistance can be measured in different ways, depending on data type\n\nTwo most general-purpose distance metrics\n\nEuclidean — if all data is continuous, and not multi-modal\nGower — can be used with mixed (discrete data/dummy variables and continuous) data; also can work better with multi-modal data"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-2-define-distance-between-individuals-1",
    "href": "teaching/Segmentation Slides.html#step-2-define-distance-between-individuals-1",
    "title": "Segmentation",
    "section": "Step 2: Define distance between individuals",
    "text": "Step 2: Define distance between individuals\n\n\nEuclidean distance (continuous data)\n\nUsually applied to standardized data to give the same weight to all the variables\nEssentially the standard deviation between customers \\(i\\) and \\(j\\) (over \\(K\\) variables):\n\nCalculate difference between \\(i\\) and \\(j\\) on each individual variable\nSquare all of those (to make them positive)\nAdd those up\nTake square root of that"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-2-define-distance-between-individuals-2",
    "href": "teaching/Segmentation Slides.html#step-2-define-distance-between-individuals-2",
    "title": "Segmentation",
    "section": "Step 2: Define distance between individuals",
    "text": "Step 2: Define distance between individuals\nIs 199 more similar to 1163 or 9594?\n\nWhat is the Euclidean distance between customers 199 and 1163 on spend_online, hh_inc?\nStandardize first\n\nDF$spend_online_standardized &lt;- scale(DF$spend_online)[,1]\nDF$hh_inc_standardized &lt;- scale(DF$hh_inc)[,1]\n\n\n\n\nspend_online_dif &lt;- ( DF[ DF$iid == 199, ]$spend_online_standardized - DF[ DF$iid == 1163, ]$spend_online_standardized) ^ 2\nhh_inc_dif &lt;- ( DF[ DF$iid == 199, ]$hh_inc_standardized - DF[ DF$iid == 1163, ]$hh_inc_standardized) ^ 2\n\ned_199_1163 &lt;- sqrt( spend_online_dif + hh_inc_dif)\ned_199_1163\n\n[1] 0.3264905"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-2-define-distance-between-individuals-3",
    "href": "teaching/Segmentation Slides.html#step-2-define-distance-between-individuals-3",
    "title": "Segmentation",
    "section": "Step 2: Define distance between individuals",
    "text": "Step 2: Define distance between individuals\nWhat is the Euclidean distance between customers 199 and 9594 on spend_online, hh_inc?\n\n\nspend_online_dif &lt;- ( DF[ DF$iid == 199, ]$spend_online_standardized - DF[ DF$iid == 9594, ]$spend_online_standardized) ^ 2\nhh_inc_dif &lt;- ( DF[ DF$iid == 199, ]$hh_inc_standardized - DF[ DF$iid == 9594, ]$hh_inc_standardized) ^ 2\n\ned_199_9594 &lt;- sqrt( spend_online_dif + hh_inc_dif)\ned_199_9594\n\n[1] 1.273762"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-2-define-distance-between-individuals-4",
    "href": "teaching/Segmentation Slides.html#step-2-define-distance-between-individuals-4",
    "title": "Segmentation",
    "section": "Step 2: Define distance between individuals",
    "text": "Step 2: Define distance between individuals\n\n\nGower distance (mixed continuous & dummy variables)\n\nAlso sometimes useful for continuous data with more than 1 mode\nStandardization is effectively “built-in” to the distance formula"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-2-define-distance-measure-between-individuals",
    "href": "teaching/Segmentation Slides.html#step-2-define-distance-measure-between-individuals",
    "title": "Segmentation",
    "section": "Step 2: Define distance measure between individuals",
    "text": "Step 2: Define distance measure between individuals\nR implementation of Euclidean & Gower distance\ndaisy() function in cluster package\n\n\nEuclidean:\n\ndaisy(DF, metric = \"euclidean\", warnType = FALSE, stand = TRUE)\n\nDF: dataframe with all continuous variables\nwarnType=FALSE to silence warnings\nstand=TRUE to standardize variables\n\n\nGower distance:\n\ndaisy(DF, metric = \"gower\", warnType=FALSE)\n\nDF: dataframe with continuous/binary variables\nwarnType=FALSE to silence warnings"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-2-define-distance-measure-between-individuals-1",
    "href": "teaching/Segmentation Slides.html#step-2-define-distance-measure-between-individuals-1",
    "title": "Segmentation",
    "section": "Step 2: Define distance measure between individuals",
    "text": "Step 2: Define distance measure between individuals\nR implementation of Euclidean & Gower distance\ndaisy() function in cluster package\n\nDF &lt;- read.csv('apparel_customer_data.csv')\n\nlibrary(cluster)\n\nDF_euclidean &lt;- daisy(DF, metric = \"euclidean\", warnType = FALSE, stand = TRUE)"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-2-define-distance-measure-between-individuals-2",
    "href": "teaching/Segmentation Slides.html#step-2-define-distance-measure-between-individuals-2",
    "title": "Segmentation",
    "section": "Step 2: Define distance measure between individuals",
    "text": "Step 2: Define distance measure between individuals\nR implementation of Euclidean & Gower distance\ndaisy() function in cluster package\n\nDF_gower &lt;- daisy(DF, metric = \"gower\", warnType = FALSE)"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-3-select-clustering-procedure",
    "href": "teaching/Segmentation Slides.html#step-3-select-clustering-procedure",
    "title": "Segmentation",
    "section": "Step 3: Select clustering procedure",
    "text": "Step 3: Select clustering procedure\nTwo main branches of clustering algorithms\n\n\nHierarchical\n\nIteratively build up clusters from nearest (distance-wise) pairs\nCluster “tree” cut at depth = # of desired clusters\n\nNonhierarchical\n\nNumber of segments is specified by the analyst\nK-means maximizes ratio of between-cluster variance to within-cluster variance\nModel based approaches use classifiers such as logistic regression\n\nMost often, with “right” distance measure, get similar results\n\nWe focus on k-means"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-3-select-clustering-procedure-1",
    "href": "teaching/Segmentation Slides.html#step-3-select-clustering-procedure-1",
    "title": "Segmentation",
    "section": "Step 3: Select clustering procedure",
    "text": "Step 3: Select clustering procedure\nK-means clustering algorithm\n\n\nEach cluster is associated with a centroid (center point)\nEach point is assigned to the cluster with the closest centroid\nNumber of clusters, \\(K\\), must be specified\nThe basic algorithm is very simple:\n\nRandomly select \\(K\\) points as the initial centroids\nAssign all other points to the cluster with the nearest centroid\nRe-compute centroid as the average in that cluster\nReassign each point to the cluster with the nearest centroid\nRe-compute centroid as the average in that cluster\nRepeat until points don’t change"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-3-select-clustering-procedure-2",
    "href": "teaching/Segmentation Slides.html#step-3-select-clustering-procedure-2",
    "title": "Segmentation",
    "section": "Step 3: Select clustering procedure",
    "text": "Step 3: Select clustering procedure\nK-means clustering algorithm"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-3-select-clustering-procedure-3",
    "href": "teaching/Segmentation Slides.html#step-3-select-clustering-procedure-3",
    "title": "Segmentation",
    "section": "Step 3: Select clustering procedure",
    "text": "Step 3: Select clustering procedure\nK-means clustering algorithm\n\nSelect 3 points as the start:\n\n\ncentroids &lt;- sample( 1:nrow(df), 3)"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-3-select-clustering-procedure-4",
    "href": "teaching/Segmentation Slides.html#step-3-select-clustering-procedure-4",
    "title": "Segmentation",
    "section": "Step 3: Select clustering procedure",
    "text": "Step 3: Select clustering procedure\nK-means clustering algorithm\n\nAssign points to closest cluster:\n\n\ndf$distance_1 &lt;- sqrt( (df$x - df[centroids[1],]$x) ^ 2 + (df$y - df[centroids[1],]$y) ^ 2)\ndf$distance_2 &lt;- sqrt( (df$x - df[centroids[2],]$x) ^ 2 + (df$y - df[centroids[2],]$y) ^ 2)\ndf$distance_3 &lt;- sqrt( (df$x - df[centroids[3],]$x) ^ 2 + (df$y - df[centroids[3],]$y) ^ 2)\n\ndf$cluster &lt;- ifelse(df$distance_1 &lt; df$distance_2 & df$distance_1 &lt; df$distance_3, 1,\n                     ifelse( df$distance_2 &lt; df$distance_1 & df$distance_2 &lt; df$distance_3, 2,\n                             ifelse( df$distance_3 &lt; df$distance_1 & df$distance_3 &lt; df$distance_2, 3, NA_real_)))"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-3-select-clustering-procedure-5",
    "href": "teaching/Segmentation Slides.html#step-3-select-clustering-procedure-5",
    "title": "Segmentation",
    "section": "Step 3: Select clustering procedure",
    "text": "Step 3: Select clustering procedure\nK-means clustering algorithm\n\nCalculate new centroids:\n\n\ncentroids &lt;- data.frame(\n  cluster = as.factor(c(1, 2, 3)),\n  x = c(mean(df[ df$cluster == 1, ]$x), mean(df[ df$cluster == 2, ]$x), mean(df[ df$cluster == 3, ]$x)),\n  y = c(mean(df[ df$cluster == 1, ]$y), mean(df[ df$cluster == 2, ]$y), mean(df[ df$cluster == 3, ]$y))\n)"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-3-select-clustering-procedure-6",
    "href": "teaching/Segmentation Slides.html#step-3-select-clustering-procedure-6",
    "title": "Segmentation",
    "section": "Step 3: Select clustering procedure",
    "text": "Step 3: Select clustering procedure\nK-means clustering algorithm\n\nRe-assign points:\n\n\ndf$distance_1 &lt;- sqrt( (df$x - centroids[1,]$x) ^ 2 + (df$y - centroids[1,]$y) ^ 2)\ndf$distance_2 &lt;- sqrt( (df$x - centroids[2,]$x) ^ 2 + (df$y - centroids[2,]$y) ^ 2)\ndf$distance_3 &lt;- sqrt( (df$x - centroids[3,]$x) ^ 2 + (df$y - centroids[3,]$y) ^ 2)\n\ndf$cluster_2 &lt;- ifelse(df$distance_1 &lt; df$distance_2 & df$distance_1 &lt; df$distance_3, 1,\n                     ifelse( df$distance_2 &lt; df$distance_1 & df$distance_2 &lt; df$distance_3, 2,\n                             ifelse( df$distance_3 &lt; df$distance_1 & df$distance_3 &lt; df$distance_2, 3, NA_real_)))\n\nsum(df$cluster != df$cluster_2)\n\n[1] 8"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-3",
    "href": "teaching/Segmentation Slides.html#step-3",
    "title": "Segmentation",
    "section": "Step 3:",
    "text": "Step 3:\n\nCalculate new centroids:\n\n\ncentroids &lt;- data.frame(\n  cluster = as.factor(c(1, 2, 3)),\n  x = c(mean(df[ df$cluster_2 == 1, ]$x), mean(df[ df$cluster_2 == 2, ]$x), mean(df[ df$cluster_2 == 3, ]$x)),\n  y = c(mean(df[ df$cluster_2 == 1, ]$y), mean(df[ df$cluster_2 == 2, ]$y), mean(df[ df$cluster_2 == 3, ]$y))\n)"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-3-select-clustering-procedure-7",
    "href": "teaching/Segmentation Slides.html#step-3-select-clustering-procedure-7",
    "title": "Segmentation",
    "section": "Step 3: Select clustering procedure",
    "text": "Step 3: Select clustering procedure\nK-means clustering algorithm\n\nRe-assign points:\n\n\ndf$distance_1 &lt;- sqrt( (df$x - centroids[1,]$x) ^ 2 + (df$y - centroids[1,]$y) ^ 2)\ndf$distance_2 &lt;- sqrt( (df$x - centroids[2,]$x) ^ 2 + (df$y - centroids[2,]$y) ^ 2)\ndf$distance_3 &lt;- sqrt( (df$x - centroids[3,]$x) ^ 2 + (df$y - centroids[3,]$y) ^ 2)\n\ndf$cluster_3 &lt;- ifelse(df$distance_1 &lt; df$distance_2 & df$distance_1 &lt; df$distance_3, 1,\n                     ifelse( df$distance_2 &lt; df$distance_1 & df$distance_2 &lt; df$distance_3, 2,\n                             ifelse( df$distance_3 &lt; df$distance_1 & df$distance_3 &lt; df$distance_2, 3, NA_real_)))\n\nsum(df$cluster_2 != df$cluster_3)\n\n[1] 10"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-3-1",
    "href": "teaching/Segmentation Slides.html#step-3-1",
    "title": "Segmentation",
    "section": "Step 3:",
    "text": "Step 3:\n\nCalculate new centroids:\n\n\ncentroids &lt;- data.frame(\n  cluster = as.factor(c(1, 2, 3)),\n  x = c(mean(df[ df$cluster_3 == 1, ]$x), mean(df[ df$cluster_3 == 2, ]$x), mean(df[ df$cluster_3 == 3, ]$x)),\n  y = c(mean(df[ df$cluster_3 == 1, ]$y), mean(df[ df$cluster_3 == 2, ]$y), mean(df[ df$cluster_3 == 3, ]$y))\n)"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-3-2",
    "href": "teaching/Segmentation Slides.html#step-3-2",
    "title": "Segmentation",
    "section": "Step 3:",
    "text": "Step 3:\nK-means clustering algorithm\n\nRe-assign points:\n\n\ndf$distance_1 &lt;- sqrt( (df$x - centroids[1,]$x) ^ 2 + (df$y - centroids[1,]$y) ^ 2)\ndf$distance_2 &lt;- sqrt( (df$x - centroids[2,]$x) ^ 2 + (df$y - centroids[2,]$y) ^ 2)\ndf$distance_3 &lt;- sqrt( (df$x - centroids[3,]$x) ^ 2 + (df$y - centroids[3,]$y) ^ 2)\n\ndf$cluster_4 &lt;- ifelse(df$distance_1 &lt; df$distance_2 & df$distance_1 &lt; df$distance_3, 1,\n                     ifelse( df$distance_2 &lt; df$distance_1 & df$distance_2 &lt; df$distance_3, 2,\n                             ifelse( df$distance_3 &lt; df$distance_1 & df$distance_3 &lt; df$distance_2, 3, NA_real_)))\n\nsum(df$cluster_4 != df$cluster_3)\n\n[1] 5"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#k-means-in-r",
    "href": "teaching/Segmentation Slides.html#k-means-in-r",
    "title": "Segmentation",
    "section": "K-means in R",
    "text": "K-means in R\nOnce we have the distance (similarity) matrix from daisy():\n\n\nWe call kmeans() to perform the cluster analysis\n\nApplies to both Euclidean/Gower distance matrices\nUse nstart option to multi-start algorithm from multiple random points\n\nRecommended range: 10 to 25 (or more), depending on data regularity\n\n\nSyntax:\n\n4 segments, using a Gower distance matrix\n\nDF$clu_gower_4 &lt;- kmeans(DF_gower, centers = 4, nstart = 10)\n\n4 segments, using a Euclidean distance matrix\n\nDF$clu_euclid_4 &lt;- kmeans(DF_euclidean, centers = 4, nstart = 10)"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#k-means-in-r-1",
    "href": "teaching/Segmentation Slides.html#k-means-in-r-1",
    "title": "Segmentation",
    "section": "K-means in R",
    "text": "K-means in R\nIn total:\n\n# Calculate euclidean distance\nDF_euclidean &lt;- daisy(DF, metric = \"euclidean\", warnType = FALSE, stand = TRUE)\n\n# Create segments\nsegments &lt;- kmeans(DF_euclidean, centers = 4, nstart = 10)\n\n# Assign\nDF$clu_euclid_4 &lt;- segments$cluster\n\nYou can also do this in one step, but I wouldn’t recommend it\n\nCalculating the distances takes time\n\n\nDF$clu_euclid_4 &lt;- kmeans(\n  daisy(DF, metric = \"euclidean\", warnType = FALSE, stand = TRUE), \n  centers = 4, nstart = 10)$cluster"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-4-select-number-of-clusters",
    "href": "teaching/Segmentation Slides.html#step-4-select-number-of-clusters",
    "title": "Segmentation",
    "section": "Step 4: Select number of clusters",
    "text": "Step 4: Select number of clusters\n\n\nTo perform k-means, we must specify the number of clusters\nHow to determine “how many” clusters?\n\nStatistical guidance — elbow plots\nSegmentation criteria — substantial, actionable, differentiable\n\nElbow plots\n\nGraph the total within-cluster variation (the sum of squared distances from points to their cluster centroids) vs. # of clusters\nWithin-cluster variation decreases as \\(k\\) gets larger, because when the number of clusters increases, within-cluster distances get smaller\nThe idea of the elbow method is to choose \\(k\\) at which the within-cluster variation decreases abruptly"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-4-select-number-of-clusters-1",
    "href": "teaching/Segmentation Slides.html#step-4-select-number-of-clusters-1",
    "title": "Segmentation",
    "section": "Step 4: Select number of clusters",
    "text": "Step 4: Select number of clusters\nElbow plot example from df\nWhat was the within-cluster variation from the example I had?\n\n\nThe kmeans() output has a $withinss result, similar to how it has a $cluster result\n\nround( sum(kmeans(df_euclidean, centers = 3, nstart = 10)$withinss), 3)\n\n[1] 6006.087"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-4-select-number-of-clusters-2",
    "href": "teaching/Segmentation Slides.html#step-4-select-number-of-clusters-2",
    "title": "Segmentation",
    "section": "Step 4: Select number of clusters",
    "text": "Step 4: Select number of clusters\nElbow plot example from df\n\n# Euclidean distance\ndf_euclidean &lt;- daisy(df, metric = \"euclidean\", warnType = FALSE, stand = TRUE)\n\n# max number of clusters to test\nmax_clusters &lt;- 10 \n\n# list to hold within-cluster sum-of-squares\nwss &lt;- rep(0, max_clusters) \n\n# loop over cluster\nfor (i in 1:max_clusters) { \n  segments &lt;- kmeans(df_euclidean, centers = i, nstart=10)\n  wss[i] &lt;- sum(segments$withinss) # within-cluster sum-of-squares, summed over clusters\n}\n\nas.data.frame(wss)\n\n          wss\n1  402.138498\n2  149.588725\n3   26.886629\n4   21.013860\n5   16.578999\n6   14.285304\n7   12.939867\n8   11.182604\n9   10.316321\n10   9.181797"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-4-select-number-of-clusters-3",
    "href": "teaching/Segmentation Slides.html#step-4-select-number-of-clusters-3",
    "title": "Segmentation",
    "section": "Step 4: Select number of clusters",
    "text": "Step 4: Select number of clusters\nElbow plot example from df\n\nelbow_data &lt;- data.frame(k = 1:max_clusters, WCSS = wss)\n\nggplot(elbow_data, aes(x = k, y = WCSS)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"blue\") +\n  labs(title = \"k-means Elbow Plot\",\n       x = \"Number of Clusters\",\n       y = \"Within groups sum of squares\") +\n  theme_minimal()\n\n\n\nHere it’s not that clear from the elbow plot\n\nBecause it’s too perfect\nWe would probably choose based on abilities"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-4-select-number-of-clusters-4",
    "href": "teaching/Segmentation Slides.html#step-4-select-number-of-clusters-4",
    "title": "Segmentation",
    "section": "Step 4: Select number of clusters",
    "text": "Step 4: Select number of clusters\nElbow plot example from DF\n\n\n# Euclidean distance\nDF_euclidean &lt;- daisy(DF, metric = \"euclidean\", warnType = FALSE, stand = TRUE)\n\n# max number of clusters to test\nmax_clusters &lt;- 10 \n\n# list to hold within-cluster sum-of-squares\nwss &lt;- rep(0, max_clusters) \n\n# loop over cluster\nfor (i in 1:max_clusters) { \n  segments &lt;- kmeans(DF_euclidean, centers = i, nstart=10)\n  wss[i] &lt;- sum(segments$withinss) # within-cluster sum-of-squares, summed over clusters\n}\n\nas.data.frame(wss)\n\n         wss\n1  5441463.4\n2  3182491.3\n3  1862694.3\n4  1550044.8\n5  1310505.9\n6  1232772.6\n7  1194428.1\n8   786449.4\n9   747327.0\n10  720655.1"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-4-select-number-of-clusters-5",
    "href": "teaching/Segmentation Slides.html#step-4-select-number-of-clusters-5",
    "title": "Segmentation",
    "section": "Step 4: Select number of clusters",
    "text": "Step 4: Select number of clusters\nElbow plot example from DF\n\nelbow_data &lt;- data.frame(k = 1:max_clusters, WCSS = wss)\n\nggplot(elbow_data, aes(x = k, y = WCSS)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"blue\") +\n  labs(title = \"k-means Elbow Plot\",\n       x = \"Number of Clusters\",\n       y = \"Within groups sum of squares\") +\n  theme_minimal()\n\n\n\n3 or 4 is best"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-5-interpret-and-profile-the-clusters",
    "href": "teaching/Segmentation Slides.html#step-5-interpret-and-profile-the-clusters",
    "title": "Segmentation",
    "section": "Step 5: Interpret and profile the clusters",
    "text": "Step 5: Interpret and profile the clusters\n\n\nMain objects of interest\n\nSegment sizes — % of sample\nCluster centroids = mean values of variables within the cluster\n\nAssessment of segmentation criteria\n\nSubstantial — Unless of high value, potentially prune small segments\nActionable — Can we translate differences in clusters to targeted policies?\nDifferentiable — Sufficiently different to make/enforce different policies?"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-5-interpret-and-profile-the-clusters-1",
    "href": "teaching/Segmentation Slides.html#step-5-interpret-and-profile-the-clusters-1",
    "title": "Segmentation",
    "section": "Step 5: Interpret and profile the clusters",
    "text": "Step 5: Interpret and profile the clusters\nA simple way (among many) to compute sizes:\n\nsegments &lt;- kmeans(DF_euclidean, centers = 4, nstart=10)\n\nDF$cluster &lt;- segments$cluster\n\nlibrary(dplyr)\nDF |&gt;\n  group_by(cluster) |&gt;\n  summarise(size = n(),\n            proportion = round(n()/nrow(DF), 3))\n\n# A tibble: 4 × 3\n  cluster  size proportion\n    &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1       1   163      0.163\n2       2   386      0.386\n3       3    18      0.018\n4       4   433      0.433"
  },
  {
    "objectID": "teaching/Segmentation Slides.html#step-5-interpret-and-profile-the-clusters-2",
    "href": "teaching/Segmentation Slides.html#step-5-interpret-and-profile-the-clusters-2",
    "title": "Segmentation",
    "section": "Step 5: Interpret and profile the clusters",
    "text": "Step 5: Interpret and profile the clusters\nA simple way (among many) to compute means:\n\nDF |&gt;\n  group_by(cluster) |&gt;\n  summarise(across(c(spend_online, spend_retail, age, white, college, male, hh_inc), mean)) |&gt;\n  round(3)\n\n# A tibble: 4 × 8\n  cluster spend_online spend_retail   age white college  male hh_inc\n    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1       1        137.         136.   42.8 0.711   0.524 0.552  102. \n2       2         31.1         50.0  43.3 0.878   0.695 0      126. \n3       3        777.         841.   44.2 0.838   0.548 0.056  117. \n4       4         55.7         49.2  37.9 0.761   0.416 0       67.0"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#overview",
    "href": "teaching/Pricing Demand Slides.html#overview",
    "title": "Pricing and Demand",
    "section": "Overview",
    "text": "Overview\n\n\nMarketing-mix modeling & demand curves\nDemand models\n\nLinear model\nMultiplicative/log-linear demand model\n\nPrice elasticity\nEstimation and prediction with the log-linear demand model\nProfitability analysis\nExtension to multiple products"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#marketing-mix-modeling-mmm",
    "href": "teaching/Pricing Demand Slides.html#marketing-mix-modeling-mmm",
    "title": "Pricing and Demand",
    "section": "Marketing-mix modeling (MMM)",
    "text": "Marketing-mix modeling (MMM)\n\n\nModern approach to manage:\n\nPrices\nPromotion planning\nAdvertising measurement\nChannels of distribution\nCompetitive threats\n\nObjectives and approach\n\nUse data & statistical models to estimate how the marketing mix affects demand\nUnderstand effect of competitor’s actions on own-demand\nFine-tune marketing to achieve an objective, in particular increase profits and the value of a product"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#demand-curves-functions",
    "href": "teaching/Pricing Demand Slides.html#demand-curves-functions",
    "title": "Pricing and Demand",
    "section": "Demand curves (functions)",
    "text": "Demand curves (functions)\n\n\nA demand curve or demand function relates the quantity of a good sold to various elements of the marketing mix and other variables influencing consumer behavior\n\\(Q = \\beta_0 + \\beta_1Price + \\beta_2Promotion + \\beta_3Season + etc\\)\nBoth own and the competitor’s marketing actions will typically affect demand"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#marketing-mix-modeling-and-demand-estimation",
    "href": "teaching/Pricing Demand Slides.html#marketing-mix-modeling-and-demand-estimation",
    "title": "Pricing and Demand",
    "section": "Marketing-mix modeling and demand estimation",
    "text": "Marketing-mix modeling and demand estimation\n\n\nGoal\n\nInfer the relationship between unit sales and prices, promotions, and other variables\n\nTo use a statistical tool such as regression analysis we need a mathematical formulation of demand\n\nThis formulation should be flexible and have a good chance of fitting the demand relationship present in common data sets\n\nIn this lecture we focus on prices"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#the-linear-demand-model",
    "href": "teaching/Pricing Demand Slides.html#the-linear-demand-model",
    "title": "Pricing and Demand",
    "section": "The linear demand model",
    "text": "The linear demand model\nFormula: \\(Q = \\beta_0 + \\beta_1P\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(Q\\) is unit sales\n\\(P\\) is price\n\\(\\beta_0\\) is\n\n\\(Q\\) when \\(P = 0\\)\n\n\\(\\beta_1\\) is\n\nChange in \\(Q\\) when \\(P\\) increases by $1\n\nWhat is the formula in this example?\n\n\\(Q = 40 - 5 \\times P\\)"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#the-linear-demand-model-1",
    "href": "teaching/Pricing Demand Slides.html#the-linear-demand-model-1",
    "title": "Pricing and Demand",
    "section": "The linear demand model",
    "text": "The linear demand model\nFormula: \\(Q = \\beta_0 + \\beta_1P\\)\n\n\nEffect of a given price change is the same for all price points\n\nIs this a desirable property?"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#the-multiplicative-demand-model",
    "href": "teaching/Pricing Demand Slides.html#the-multiplicative-demand-model",
    "title": "Pricing and Demand",
    "section": "The multiplicative demand model",
    "text": "The multiplicative demand model\nFormula: \\(Q = A\\times P^{-\\eta}\\)\n\n\nEffect of a given price change is larger at low than at high price points\n\nConsistent with niche and mainstream segments of consumers"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#logarithms-and-the-exponential-function",
    "href": "teaching/Pricing Demand Slides.html#logarithms-and-the-exponential-function",
    "title": "Pricing and Demand",
    "section": "Logarithms and the exponential function",
    "text": "Logarithms and the exponential function\n\nThe log() (logarithm) and exp() (exponential function) will make it easier to manipulate and work with the multiplicative demand model\n\nParticularly useful: the logarithm transforms multiplication into addition\n\nNotation\n\nlog(x) is always the natural logarithm\nBoth \\(e^x\\) and exp(x_ mean the same thing—the exponential function applied to \\(x\\)\nMultiplicative demand model = log-linear demand model"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#logarithms-and-the-exponential-function-1",
    "href": "teaching/Pricing Demand Slides.html#logarithms-and-the-exponential-function-1",
    "title": "Pricing and Demand",
    "section": "Logarithms and the exponential function",
    "text": "Logarithms and the exponential function\n\nWe can use logarithms to transform the multiplicative demand model:\n\n\\(Q = A\\times P^{-\\eta}\\)\n\\(log(Q) = log(A) + log(P^{-\\eta})\\)\n\\(log(Q) = log(A) -\\eta\\times log(P)\\)\n\\(log(Q) = \\beta_0 +\\beta_1\\times log(P)\\)"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#in-r",
    "href": "teaching/Pricing Demand Slides.html#in-r",
    "title": "Pricing and Demand",
    "section": "In R",
    "text": "In R\n\n\\(log(Q) = \\beta_0 +\\beta_1\\times log(P)\\)\nlm( log(Q) ~ log(P))\n\n\n\nWe will therefore also refer to the multiplicative demand model as the log-linear demand model—both represent the same demand relationship\nWhy?\n\nThis model has the form of a linear regression\nWe can use lm() to estimate the model, using logged DV/IVs\n\\(\\beta_0\\) is the log-transformed intercept for log sales\n\\(\\beta_1\\) is the log-transformed effect of price on log sales"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#effect-of-parameters",
    "href": "teaching/Pricing Demand Slides.html#effect-of-parameters",
    "title": "Pricing and Demand",
    "section": "Effect of parameters",
    "text": "Effect of parameters\nParameter \\(A\\) is the intercept\n\nAn increase \\(A\\) increases the demand level at any given price\n\n\n\\(A\\) is higher in the red example."
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#effect-of-parameters-1",
    "href": "teaching/Pricing Demand Slides.html#effect-of-parameters-1",
    "title": "Pricing and Demand",
    "section": "Effect of parameters",
    "text": "Effect of parameters\nParameter \\(\\eta\\) is like the slope\n\nAn increase \\(\\eta\\) makes the demand curve steeper—more responsive to price"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#price-elasticity-1",
    "href": "teaching/Pricing Demand Slides.html#price-elasticity-1",
    "title": "Pricing and Demand",
    "section": "Price Elasticity",
    "text": "Price Elasticity\n\n\nThe (own) price elasticity of demand is defined as the percentage change in the quantity demanded relative to a given percentage change in price:\n\n\\(price\\space elasticity = \\frac{\\%\\triangle Q}{\\%\\triangle P} = \\frac{\\frac{Q_1 - Q_0}{Q_0}}{\\frac{P_1-P_0}{P_0}}\\)\nUseful because it does not depend on the level of unit sales or prices\nTypically negative (why?)\n\nInterpretation\n\nExample: Price elasticity is -2.8\nA 1% increase in price is associated with a 2.8% decrease in unit sales\n\nNote: The price elasticity can be calculated for any demand curve, not just the multiplicative demand model"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#categorization-of-price-elasticities",
    "href": "teaching/Pricing Demand Slides.html#categorization-of-price-elasticities",
    "title": "Pricing and Demand",
    "section": "Categorization of price elasticities",
    "text": "Categorization of price elasticities\n\nInelastic demand: \\(-1 &lt; price\\space elasticity &lt; 0\\)\nElastic demand: \\(price\\space elasticity &lt; -1\\)\n\n\n\nAfter a price cut:\n\nRevenue increases for elastic demand\n\nBecause Q increases a LOT\n\nRevenue decreases for inelastic demand\n\nBecause Q does not increase enough"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#empirical-generalizations-about-own-price-elasticity",
    "href": "teaching/Pricing Demand Slides.html#empirical-generalizations-about-own-price-elasticity",
    "title": "Pricing and Demand",
    "section": "Empirical generalizations about (own) price elasticity",
    "text": "Empirical generalizations about (own) price elasticity\n\n\nSurvey of 367 price elasticity estimates\n\n(Dolan and Simon, Power Pricing, New York: The Free Press, 1996)\nAverage = -1.76\nvs. average advertising expenditure elasticity of +0.22\nHigher for industrial products (2—100)\nLower(ish) for consumer goods (1.5—5)\nLowest for luxury (.7—1.5)\n\nElasticity estimates far outside these ranges may indicate a problem with demand model (e.g. omitted variables)"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#price-elasticity-in-the-multiplicative-demand-model",
    "href": "teaching/Pricing Demand Slides.html#price-elasticity-in-the-multiplicative-demand-model",
    "title": "Pricing and Demand",
    "section": "Price elasticity in the multiplicative demand model",
    "text": "Price elasticity in the multiplicative demand model\n\\(Q = A\\times P^{-\\eta}\\)\n\n\\(\\eta\\) has simple and straightforward interpretation—it is the absolute value of the price elasticity of demand\nThe property that one parameter directly measures the price elasticity is very special and not true for other demand models\n\nExample: linear demand model\nElasticity = \\(-\\beta_1 \\frac{P}{Q}\\)"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#price-elasticities-and-the-relationship-between-price-and-sales-changes",
    "href": "teaching/Pricing Demand Slides.html#price-elasticities-and-the-relationship-between-price-and-sales-changes",
    "title": "Pricing and Demand",
    "section": "Price elasticities and the relationship between price and sales changes",
    "text": "Price elasticities and the relationship between price and sales changes\n\nFor any demand model (not just the multiplicative demand model) price and sales changes are related as follows:\n\\(\\%\\triangle Q = price \\space elasticity \\times \\%\\triangle P\\)\n\nThis relationship is only an approximation\nThe approximation is better for small than for large price changes\nApproximations typically reasonable up to 5%-10% price changes\n\nFor our purposes and exact predictions, we will use the predict() function in R\n\nPredict demand at prices \\(P_0\\) and \\(P_1\\), and compute % change"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#profitability-of-slot-machines-in-a-casino",
    "href": "teaching/Pricing Demand Slides.html#profitability-of-slot-machines-in-a-casino",
    "title": "Pricing and Demand",
    "section": "Profitability of slot machines in a casino",
    "text": "Profitability of slot machines in a casino\nShould the casino increase or decrease slot machine “prices”?\n\ncasino &lt;- read.csv(\"casino.csv\")\n\n\n\nhead(casino)\n\n  slot_number month days_active hold denomination location  coin_in\n1           1     1          28 9.84         0.25        J 69090.45\n2           1     2          31 9.84         0.25        J 73333.34\n3           1     3          27 8.52         0.25        J 63591.55\n4           1     4          31 8.52         0.25        J 84263.49\n5           1     5          30 9.84         0.25        J 92990.34\n6           1     6          31 9.84         0.25        J 67822.66\n\nsummary(casino)\n\n  slot_number       month         days_active        hold      \n Min.   :   1   Min.   : 1.000   Min.   : 1.0   Min.   : 4.25  \n 1st Qu.: 701   1st Qu.: 5.000   1st Qu.:30.0   1st Qu.: 7.53  \n Median :1217   Median : 8.000   Median :31.0   Median : 9.99  \n Mean   :1189   Mean   : 8.526   Mean   :29.9   Mean   :10.15  \n 3rd Qu.:1743   3rd Qu.:13.000   3rd Qu.:31.0   3rd Qu.:12.43  \n Max.   :2401   Max.   :16.000   Max.   :31.0   Max.   :16.75  \n  denomination       location            coin_in       \n Min.   :  0.050   Length:30439       Min.   :    303  \n 1st Qu.:  0.250   Class :character   1st Qu.:  47062  \n Median :  0.250   Mode  :character   Median :  77301  \n Mean   :  0.956                      Mean   : 117583  \n 3rd Qu.:  1.000                      3rd Qu.: 137849  \n Max.   :100.000                      Max.   :5111112"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#some-useful-observations-from-the-data",
    "href": "teaching/Pricing Demand Slides.html#some-useful-observations-from-the-data",
    "title": "Pricing and Demand",
    "section": "Some useful observations from the data:",
    "text": "Some useful observations from the data:\n\n2,401 slot machine observed over a range of 16 months\nThe average slot machine has been active for 29.9 days in a given month. As low as 1 day a month and as high as 31 days.\nThe mean hold is 10.15 with a standard deviation of 2.63, and it varies from 4.25 to 16.75\nThere are 20 locations.\nThe average revenue per month for a slot machine is $117,582.57. As low as $303.03, as high as $5,111,112.00.\n\nWhat is the total revenue of the casino chain over the 16 month period?\n\n\nsum(casino$coin_in * casino$hold/100)\n\n[1] 314009232"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#demand-model",
    "href": "teaching/Pricing Demand Slides.html#demand-model",
    "title": "Pricing and Demand",
    "section": "Demand Model",
    "text": "Demand Model\n\n\nWhat is price in this data?\n\nThere are no explicit prices. The hold of the casino can be considered to be like a price.\n\nThe hold is the (long-run) average proportion of the bets that the casino keeps."
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#profit",
    "href": "teaching/Pricing Demand Slides.html#profit",
    "title": "Pricing and Demand",
    "section": "Profit",
    "text": "Profit\n\n\nIf:\n\n\\(Q\\) is the volume of play (referred to as coin_ins by the industry)\n\\(h\\) is the hold and \\(c\\) is the marginal cost\nProfits of the casino are:\n\n\\(pft = Q_h − Q_c\\)\n\nBut \\(c\\) of a casino is very close to zero\n\n\\(pft = Q_h\\)\n\nWithout any marginal costs, we can decide to raise or lower prices from the price elasticity estimate alone"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#demand-model-1",
    "href": "teaching/Pricing Demand Slides.html#demand-model-1",
    "title": "Pricing and Demand",
    "section": "Demand Model",
    "text": "Demand Model\n\nThere is variation in the number of days for which the machine is active.\n\ncoin-ins will naturally be higher for a machine that is active for more days.\n\nUsing coin_in itself can give misleading results.\n\nCreate a new variable Qdaily, which is:\n\n\n\nthe ratio of coin_ins and days_active\n\ncasino$Qdaily &lt;- casino$coin_in/casino$days_active"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#model-building",
    "href": "teaching/Pricing Demand Slides.html#model-building",
    "title": "Pricing and Demand",
    "section": "Model building",
    "text": "Model building\n\nWill the data be useful for modeling slot machine demand?\n\nWe need variation in holds in order to run a regression analysis for this purpose\n\nWe always need variation in the IV and DV!\n\nHow can we check this?\n\n\n\n\nPlot a histogram\n\n\nhist(casino$hold)"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#model-building-1",
    "href": "teaching/Pricing Demand Slides.html#model-building-1",
    "title": "Pricing and Demand",
    "section": "Model building",
    "text": "Model building\n\nWill the data be useful for modeling slot machine demand?\n\nWe need variation in holds in order to run a regression analysis for this purpose\n\nWe always need variation in the IV and DV!\n\nHow can we check this?\n\n\n\n\nScatter plot of Qdaily vs. hold\n\n\nggplot(data = casino, aes( x = hold, y = Qdaily)) +\n         geom_point() +\n         theme_classic()"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#what-is-the-relationship",
    "href": "teaching/Pricing Demand Slides.html#what-is-the-relationship",
    "title": "Pricing and Demand",
    "section": "What is the relationship?",
    "text": "What is the relationship?\n\nIs it linear or is it more “curved”?\n\n\nThere seems to be a downward sloping effect of holds on coin_ins\n\nThis is good news!\n\nThe relationship does not appear to be linear\n\nVery small hold: coin_ins high\n\nA multiplicative/log-linear demand model is likely to be a better fit to these data."
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#simple-demand-model-specification",
    "href": "teaching/Pricing Demand Slides.html#simple-demand-model-specification",
    "title": "Pricing and Demand",
    "section": "Simple demand model specification",
    "text": "Simple demand model specification\nNow we can use Qdaily as our demand variable and our demand model can be written as:\n\n\\(log(Qdaily) = \\beta_0 + \\beta_1 \\times log(hold) + e\\)\nin R:\n\n\n\nlm1 &lt;- lm(log(Qdaily) ~ log(hold), data = casino)"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#simple-demand-model-specification-1",
    "href": "teaching/Pricing Demand Slides.html#simple-demand-model-specification-1",
    "title": "Pricing and Demand",
    "section": "Simple demand model specification",
    "text": "Simple demand model specification\n\nsummary(lm1)\n\n\nCall:\nlm(formula = log(Qdaily) ~ log(hold), data = casino)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6346 -0.4141  0.0156  0.4780  3.2330 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.96853    0.03555  308.52   &lt;2e-16 ***\nlog(hold)   -1.34687    0.01547  -87.05   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7674 on 30437 degrees of freedom\nMultiple R-squared:  0.1993,    Adjusted R-squared:  0.1993 \nF-statistic:  7578 on 1 and 30437 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nInterpret the coefficient for hold – What does this imply for raising or lowering hold percentages?\n\nThis elasticity estimate suggests that the demand is elastic\nThe casino should decrease holds to increase revenues/profits\nIt suggests that consumers are aware of and very responsive to holds.\n\nWhat concerns might you have for the validity of this?\n\nIf we have omitted variables, then we can bias our estimates of price elasticities"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#prediction-tasks",
    "href": "teaching/Pricing Demand Slides.html#prediction-tasks",
    "title": "Pricing and Demand",
    "section": "Prediction tasks",
    "text": "Prediction tasks\n\nLet’s predict Qdaily when hold &lt;- 2\n\n\n\nlogQ_predict &lt;- predict(lm1, newdata = data.frame( hold = 2))\n\nHow do we take this out of log() form?"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#prediction-tasks-1",
    "href": "teaching/Pricing Demand Slides.html#prediction-tasks-1",
    "title": "Pricing and Demand",
    "section": "Prediction tasks",
    "text": "Prediction tasks\nHow do we take this out of log() form?\nIt would be nice if this worked:\n\nexp(logQ_predict)\n\n       1 \n22810.08 \n\n\nBut it doesn’t. Why?\n\n\nIn lm(), we assume that the average error is 0\n\nNot the sum of squared error\n\nWhen we log-transform and exponentiate, it’s actually \\(e^{\\frac{\\sigma^2}{2}}\\)\n\nWhere \\(\\sigma^2\\) is the standard error of the log-linear regression model"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#prediction-tasks-2",
    "href": "teaching/Pricing Demand Slides.html#prediction-tasks-2",
    "title": "Pricing and Demand",
    "section": "Prediction tasks",
    "text": "Prediction tasks\nHow do we take this out of log() form?\n\n\nWhen we log-transform and exponentiate, avg error = \\(e^{\\frac{\\sigma^2}{2}}\\)\nThis means that \\(\\frac{\\sigma^2}{2}\\) is included in our prediction:\n\n\\(Q = e^{a-\\eta log(P)+\\frac{\\sigma^2}{2}}\\)\nIf we just type exp( predict(...)), we get \\(Q = e^{a-\\eta log(P)}\\)\nAnd underestimate demand\nThe fix is somewhat easy"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#prediction-tasks-3",
    "href": "teaching/Pricing Demand Slides.html#prediction-tasks-3",
    "title": "Pricing and Demand",
    "section": "Prediction tasks",
    "text": "Prediction tasks\nHow do we take this out of log() form?\n\\(Q = e^{a-\\eta log(P)+\\frac{\\sigma^2}{2}}\\)\n\nexp(logQ_predict + sigma(lm1) ^ 2 / 2 )\n\n       1 \n30619.65 \n\n\nsigma(lm1) is the standard error of lm1 (\\(\\sigma\\))"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#prediction-tasks-4",
    "href": "teaching/Pricing Demand Slides.html#prediction-tasks-4",
    "title": "Pricing and Demand",
    "section": "Prediction tasks",
    "text": "Prediction tasks\nConfusing… I know. Just remember:\n\n# Same as always\nlogQ_predict &lt;- predict(lm1, newdata = data.frame( hold = 2))\n\n# Take it out of log\nQ_predict &lt;- exp(logQ_predict + sigma(lm1) ^ 2 / 2 )\n\nWhat is our prediction?\n\n\nQ_predict \n\n       1 \n30619.65"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#controlling-for-slot-denominations",
    "href": "teaching/Pricing Demand Slides.html#controlling-for-slot-denominations",
    "title": "Pricing and Demand",
    "section": "Controlling for slot denominations",
    "text": "Controlling for slot denominations\nWe have information on slot denominations in the data.\n\n\nShould we include that information in our analysis?\n\nDo denominations affect the volume of coin-ins?\n\nLikely. Consumers who use high-denomination slot machines are likely systematically different.\n\nAre slot denominations potentially correlated with holds?\n\nHow could we answer?"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#controlling-for-slot-denominations-1",
    "href": "teaching/Pricing Demand Slides.html#controlling-for-slot-denominations-1",
    "title": "Pricing and Demand",
    "section": "Controlling for slot denominations",
    "text": "Controlling for slot denominations\nGenerate a scatter plot of demonination vs. hold:\n\nUse factor(denomination)\n\n\nggplot(data = casino, aes( x = factor(denomination), y = hold)) +\n         geom_point() +\n         theme_classic()"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#controlling-for-slot-denominations-2",
    "href": "teaching/Pricing Demand Slides.html#controlling-for-slot-denominations-2",
    "title": "Pricing and Demand",
    "section": "Controlling for slot denominations",
    "text": "Controlling for slot denominations\n\nlm2 &lt;- lm(log(Qdaily) ~ log(hold) + factor(denomination), data = casino)\nsummary(lm2)\n\n\nCall:\nlm(formula = log(Qdaily) ~ log(hold) + factor(denomination), \n    data = casino)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0291 -0.3731  0.0387  0.4546  2.1024 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               9.96298    0.05725 174.039  &lt; 2e-16 ***\nlog(hold)                -0.66380    0.02121 -31.292  &lt; 2e-16 ***\nfactor(denomination)0.25 -0.73884    0.02428 -30.429  &lt; 2e-16 ***\nfactor(denomination)0.5  -0.67726    0.02819 -24.026  &lt; 2e-16 ***\nfactor(denomination)1    -0.35017    0.02635 -13.289  &lt; 2e-16 ***\nfactor(denomination)2    -0.22966    0.04725  -4.860 1.18e-06 ***\nfactor(denomination)5     0.33564    0.03555   9.442  &lt; 2e-16 ***\nfactor(denomination)10    0.58851    0.05921   9.939  &lt; 2e-16 ***\nfactor(denomination)25    0.71686    0.07777   9.218  &lt; 2e-16 ***\nfactor(denomination)100   1.46711    0.11262  13.026  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7286 on 30429 degrees of freedom\nMultiple R-squared:  0.2784,    Adjusted R-squared:  0.2781 \nF-statistic:  1304 on 9 and 30429 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nInterpret the coefficient for hold\n\nThe coefficient estimate imples a 1% increase in price corresponds to a 0.66% decrease in demand. – What does this estimate imply for the casino?\nThus, the casino could gain in profitability by raising rather than lowering holds – Which model (lm1 or lm2) do you have greater confidence in? Why?\nIn light of lm2 using stronger controls, we will tend to have higher confidence in model lm2"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#if-you-want-more-practice",
    "href": "teaching/Pricing Demand Slides.html#if-you-want-more-practice",
    "title": "Pricing and Demand",
    "section": "If you want more practice",
    "text": "If you want more practice\nThere is also location (location) and time (month) information.\nPractice with adding those if you want more."
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#price-optimization",
    "href": "teaching/Pricing Demand Slides.html#price-optimization",
    "title": "Pricing and Demand",
    "section": "Price optimization",
    "text": "Price optimization\n\n\nWith inelastic demand, predicted profits increase forever as prices increase.\n\nThis emphasizes the role of price constraints — Statistical reliability of model decreases when proposed prices are very different from prices in the data.\nIn the case of inelastic demand, “optimal” price will generally be the maximum price considered under the constraints.\nIn practice, managerial reaction should be to raise prices by a “reasonably large” amount (e.g. ~10-15%)\nAfter demand data is collected using the new prices, the demand model should be re-estimated.\nThis process should be repeated until demand estimates are elastic, at which point truly optimal prices may be calculated."
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#price-optimization-1",
    "href": "teaching/Pricing Demand Slides.html#price-optimization-1",
    "title": "Pricing and Demand",
    "section": "Price optimization",
    "text": "Price optimization\n\n\nAlthough we have inelastic demand in this case, we can still demonstrate the graphical approach to determining optimal prices\nTo do this, we will:\n\nCreate an R function to evaluate casino profits, given predictors and an lm() model\nEvaluate profits over a range of holds (prices), and plot the results."
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#price-optimization-2",
    "href": "teaching/Pricing Demand Slides.html#price-optimization-2",
    "title": "Pricing and Demand",
    "section": "Price optimization",
    "text": "Price optimization\nCreate a function called casino_pft()\n\nInputs: – hold - hold percentage – month - month of observation – denomination - denomination of slot machine – location - location of slot machine – lm_result - log-linear model estimation results (lm output)\nOutput: – Q - predicted demand (daily coin-ins) – profit - predicted profit"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#price-optimization-3",
    "href": "teaching/Pricing Demand Slides.html#price-optimization-3",
    "title": "Pricing and Demand",
    "section": "Price optimization",
    "text": "Price optimization\nCreate a function called casino_pft()\n\ncasino_pft &lt;- function(hold, month, denomination, location, lm_result) {\n  # DF that we will use to make predictions\n  pred_DF = data.frame( month = month, \n                        hold = hold,\n                        denomination = denomination,\n                        location = location)\n  \n  logQ = predict(lm_result, newdata=pred_DF) #Predict log value\n  Q = exp(logQ+sigma(lm_result)^2/2) #Fix it!\n  profit = Q*hold/100\n  return(list(Q = Q, profit = profit))\n}"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#price-optimization-4",
    "href": "teaching/Pricing Demand Slides.html#price-optimization-4",
    "title": "Pricing and Demand",
    "section": "Price optimization",
    "text": "Price optimization\nUse your casino_pft() to evaluate casino profits for:\n\nhold: hold percentages ranging from 10 to 20 (percent)\nmonth = 1\ndenomination = 1\nlocation = “B”\nlm_result = lm3\n\nWhere lm3:\n\nlm3 &lt;- lm(log(Qdaily) ~ log(hold) + month + factor(denomination) + factor(location), data = casino)"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#price-optimization-5",
    "href": "teaching/Pricing Demand Slides.html#price-optimization-5",
    "title": "Pricing and Demand",
    "section": "Price optimization",
    "text": "Price optimization\nCreate data.frame to hold results:\n\nresults &lt;- data.frame(\n  holds = 10:20,\n  pft = rep(0,length(10:20))\n)\n\n\nCalculate and plot!\n\nfor (i in 1:nrow(results)) {\n  results$pft[i] &lt;- casino_pft(hold = results$hold[i],\n                               month = 1,\n                               denomination = 1,\n                               location = \"B\",\n                               lm_result = lm3)$profit\n}\n\nggplot(data = results, \n       aes( x = holds, y = pft)) +\n         geom_point() +\n         theme_classic()"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#discussion",
    "href": "teaching/Pricing Demand Slides.html#discussion",
    "title": "Pricing and Demand",
    "section": "Discussion",
    "text": "Discussion\n\n\nWhat does the plot imply for the optimal pricing for the specified slot machine?\n\nThe plot demonstrates that “optimal” profits occur at the maximum hold evaluated (20). In the case of elastic demand, such methods can be used to find optimal profits that do not occur at constraint boundaries."
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#pricing-and-demand-for-multiple-products-1",
    "href": "teaching/Pricing Demand Slides.html#pricing-and-demand-for-multiple-products-1",
    "title": "Pricing and Demand",
    "section": "Pricing and Demand for Multiple Products",
    "text": "Pricing and Demand for Multiple Products\nRecall from last class: The multiplicative demand model\n\\(log(Q_i) = \\beta_0 + \\beta_1\\times log(P_i)\\)\n\n\n\nThis is often too simple\n\nDoes not account for competing products\n\nBut we can generalize it to allow for the effect of competing product prices"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#pricing-and-demand-for-multiple-products-2",
    "href": "teaching/Pricing Demand Slides.html#pricing-and-demand-for-multiple-products-2",
    "title": "Pricing and Demand",
    "section": "Pricing and Demand for Multiple Products",
    "text": "Pricing and Demand for Multiple Products\nSingle product:\n\n\\(log(Q_i) = \\beta_0 + \\beta_1\\times log(P_i)\\)\n\nMultiple products:\n\n\n\\(log(Q_i) = \\beta_{i0} + \\beta_{i1}\\times log(P_{i}) + \\beta_{i2}\\times log(P_{2})\\)\n\n\\(log(Q_1) = \\beta_{10} + \\beta_{11}\\times log(P_{1}) + \\beta_{12}\\times log(P_{2})\\)\n\\(log(Q_2) = \\beta_{20} + \\beta_{21}\\times log(P_{1}) + \\beta_{22}\\times log(P_{2})\\)\n\n\n\n\n\nThe price coefficients have two subscripts\n\nThe first subscript refers to the equation\n\ni.e. demand function for product \\(i\\)\n\nThe second refers to the specific product/competitor"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#pricing-and-demand-for-multiple-products-3",
    "href": "teaching/Pricing Demand Slides.html#pricing-and-demand-for-multiple-products-3",
    "title": "Pricing and Demand",
    "section": "Pricing and Demand for Multiple Products",
    "text": "Pricing and Demand for Multiple Products\n\n\\(log(Q_1) = \\beta_{10} + \\beta_{11}\\times log(P_{1}) + \\beta_{12}\\times log(P_{2})\\)\n\\(log(Q_2) = \\beta_{20} + \\beta_{21}\\times log(P_{1}) + \\beta_{22}\\times log(P_{2})\\)\n\nThis means that:\n\n\n\\(\\beta_{10}\\) is the \\(log\\) of unit sales of Product 1 when \\(log(P_1) == 0\\) & \\(log(P_2) == 0\\)\n\\(\\beta_{20}\\) is the \\(log\\) of unit sales of Product 2 when \\(log(P_1) == 0\\) & \\(log(P_2) == 0\\)\n\\(\\beta_{11}\\) is the…?\n\nOwn price elasticity of Product 1\n\\(\\beta_{21}\\) is the own price elasticity of Product 2\nHow much the sales of a product change when its own price changes\n\n\\(\\beta_{12}\\) is the…?\n\nCross price elasticity of Product 2 on Product 1\nHow much the sales of a product change when its competitor’s price changes"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#pricing-and-demand-for-multiple-products-4",
    "href": "teaching/Pricing Demand Slides.html#pricing-and-demand-for-multiple-products-4",
    "title": "Pricing and Demand",
    "section": "Pricing and Demand for Multiple Products",
    "text": "Pricing and Demand for Multiple Products\nThe coefficients in these equations are price elasticities, just as in the simple multiplicative demand model discussed before:\n\\(\\beta_{ik} = \\frac{\\%\\triangle Q_i}{\\%\\triangle P_k}\\)"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#own-and-cross-price-elasticities",
    "href": "teaching/Pricing Demand Slides.html#own-and-cross-price-elasticities",
    "title": "Pricing and Demand",
    "section": "Own and cross price elasticities",
    "text": "Own and cross price elasticities\n\n\nOwn price elasticities are typically negative\n\nWhen the price of something increases, all else equal, sales fall\nThis is why we expected \\(\\beta_1\\) to be negative in the single product case\n\nCross-price elasticities can be positive or negative\n\nWhen/why?\nPositive when products are substitutes for one another\n\n(e.g. Coke vs. Pepsi)\nPrice of Coke increases, people switch to Pepsi\n\nNegative when products are compliments for one another\n\n(e.g. razors and blades)"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#implications-of-large-cross-price-elasticities",
    "href": "teaching/Pricing Demand Slides.html#implications-of-large-cross-price-elasticities",
    "title": "Pricing and Demand",
    "section": "Implications of large cross-price elasticities",
    "text": "Implications of large cross-price elasticities\nWhen price elasticity is large and positive:\n\n\nCompetitor’s product with high cross price elasticity?\n\nCompetitive price change will have a large impact on the demand for our product\nPrice competition will be tough — Low equilibrium prices\n\nProduct in our own product line with high price elasticity?\n\nCannibalization of sales from other products in our product line\nNeed to manage the prices of all products in product line simultaneously"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#implications-of-large-cross-price-elasticities-1",
    "href": "teaching/Pricing Demand Slides.html#implications-of-large-cross-price-elasticities-1",
    "title": "Pricing and Demand",
    "section": "Implications of large cross-price elasticities",
    "text": "Implications of large cross-price elasticities\nWhen price elasticity is large and negative:\n\n\nCompetitor’s product with low cross price elasticity (&lt; -2)?\n\nPrice change will have a large impact on the demand for our product\nTough spot to be in–can’t collude on prices\n\ne.g., video games and consoles\n\n\nProduct in our own product line with low cross price elasticity (&lt; -2)?\n\nProbably parts of a set\n\ne.g., battery and charger\n\nNeed to manage the prices of all products in product line simultaneously\nPotentially sell in bundles?"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#implications-of-small-cross-price-elasticities",
    "href": "teaching/Pricing Demand Slides.html#implications-of-small-cross-price-elasticities",
    "title": "Pricing and Demand",
    "section": "Implications of small cross-price elasticities",
    "text": "Implications of small cross-price elasticities\n\n\nProduct with low cross price elasticity?\n\nPrice change will have a small impact on the demand for our product\nProducts may not be true substitutes\n\ne.g., Kashi cereal and Cookie Crisp\n\nPrice competition will not be tough — Higher equilibrium prices\n\nHow can we get to this point?\n\nAdvertising–make our product seem different\n\ne.g., Gasoline, cell phone carriers, insurance providers, etc.\n\nIf both products are ours, we could create optimally different products\n\ne.g., combine with a conjoint analysis"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#predicting-changes-in-demand",
    "href": "teaching/Pricing Demand Slides.html#predicting-changes-in-demand",
    "title": "Pricing and Demand",
    "section": "Predicting changes in demand",
    "text": "Predicting changes in demand\nTo predict change in demand, we can use the change in prices:\n\n\n\\(log(Q_1) = \\beta_{10} + \\beta_{11}\\times log(P_{1}) + \\beta_{12}\\times log(P_{2})\\)\nCame from: \\(Q_1 = A^{\\beta_{10}} P_1^{\\beta_{11}}P_2^{\\beta_{12}}\\)\nThis was gross, but we can use it again:\n\n\\(\\frac{Q'_1}{Q_1} = \\frac{A^{\\beta_{10}}P{'}_{12}^{\\beta_{11}}P{'}_2^{\\beta_{12}}}{A^{\\beta_{10}}P_1^{\\beta_{11}}P_2^{\\beta_{12}}}\\)\n\\(P{'}_{i} = P_i\\times (1 + \\triangle P_i)\\)\n\\(\\frac{Q'_1}{Q_1} = \\frac{A^{\\beta_{10}}P_1\\times (1 + \\triangle P_1)^{\\beta_{11}}P_2\\times (1 + \\triangle P_2)^{\\beta_{12}}}{A^{\\beta_{10}}P_1^{\\beta_{11}}P_2^{\\beta_{12}}}\\)\n\\(\\frac{Q'_1}{Q_1} = \\frac{P_1\\times (1 + \\triangle P_1)^{\\beta_{11}}P_2\\times (1 + \\triangle P_2)^{\\beta_{12}}}{P_1^{\\beta_{11}}P_2^{\\beta_{12}}}\\)\n\\(\\frac{Q'_1}{Q_1} = (1 + \\triangle P_i)^{\\beta_{11}}(1 + \\triangle P_i)^{\\beta_{12}}\\)"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#predicting-changes-in-demand-1",
    "href": "teaching/Pricing Demand Slides.html#predicting-changes-in-demand-1",
    "title": "Pricing and Demand",
    "section": "Predicting changes in demand",
    "text": "Predicting changes in demand\nTo predict change in demand, we can use the change in prices:\n\\(\\frac{Q'_1}{Q_1} = (1 + \\triangle P_i)^{\\beta_{11}}(1 + \\triangle P_i)^{\\beta_{12}}\\)\n\nOr, in R:\n\nchange_in_Q1 &lt;- (1 + change_in_P1) ^ beta11 * (1 + change_in_P2) ^ beta12"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#workshop-2",
    "href": "teaching/Pricing Demand Slides.html#workshop-2",
    "title": "Pricing and Demand",
    "section": "Workshop",
    "text": "Workshop\nWe’ve got data about three cereals:\n\nFrosted Flakes\nFroot Loops\nKashi GoLean Crunch\n\nRead in the data:\n\ndata &lt;- read.csv(\"cereals_demand.csv\")"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#estimate",
    "href": "teaching/Pricing Demand Slides.html#estimate",
    "title": "Pricing and Demand",
    "section": "Estimate",
    "text": "Estimate\nDemand for Froot Loops: Single-Product Case\n\n\\(log(Q_{fl}) = \\beta_{0} + \\beta_{1}\\times log(P_{fl})\\)\n\n\n\nfl_single &lt;- lm( data = data, log(sales_fl) ~ log(price_fl))\n\nsummary(fl_single)$coef |&gt; round(3)\n\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)      9.006      0.534  16.858        0\nlog(price_fl)   -1.134      0.270  -4.195        0"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#conclusions",
    "href": "teaching/Pricing Demand Slides.html#conclusions",
    "title": "Pricing and Demand",
    "section": "Conclusions",
    "text": "Conclusions\nDemand for Froot Loops: Single-Product Case\n\n\nWhat is the own-price elasticity?\n\n-1.134\n1% increase in price reduces sales by 1.134%\nDon’t increase prices!! Reduce them!\n\nLook at the \\(R^2\\) from this model. What is it?\n\n~14%\nWe explain 14% of the variation in sales with Froot Loop prices alone\nCan we do better?"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#estimate-1",
    "href": "teaching/Pricing Demand Slides.html#estimate-1",
    "title": "Pricing and Demand",
    "section": "Estimate",
    "text": "Estimate\nDemand for Froot Loops: Multiple-Product Case\n\n\\(log(Q_{fl}) = \\beta_{fl0} + \\beta_{fl1}\\times log(P_{fl}) + \\beta_{fl2}\\times log(P_{ff}) + \\beta_{fl3}\\times log(P_{k})\\)\n\n\n\nfl_multiple &lt;- lm( data = data, log(sales_fl) ~ log(price_fl) + log(price_ff) + log(price_k))\n\nsummary(fl_multiple)$coef |&gt; round(3)\n\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)      6.323      0.867   7.291    0.000\nlog(price_fl)   -1.605      0.262  -6.124    0.000\nlog(price_ff)    1.255      0.223   5.637    0.000\nlog(price_k)     0.566      0.279   2.029    0.045"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#conclusions-1",
    "href": "teaching/Pricing Demand Slides.html#conclusions-1",
    "title": "Pricing and Demand",
    "section": "Conclusions",
    "text": "Conclusions\nDemand for Froot Loops: Multiple-Product Case\n\n\nWhat is the own-price elasticity?\n\n-1.6\n1% increase in price reduces sales by 1.6%\nDon’t increase prices!! Reduce them!"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#conclusions-2",
    "href": "teaching/Pricing Demand Slides.html#conclusions-2",
    "title": "Pricing and Demand",
    "section": "Conclusions",
    "text": "Conclusions\nDemand for Froot Loops: Multiple-Product Case\n\n\nWhat is cross-price elasticity?\n\nFrosted Flakes:\n\n1.3\nFrosted flakes prices impact us a lot!\nTheir price increases, our sales do too\n\nKashi:\n\n.6\nKashi prices don’t impact a lot\nTheir price increases, our sales do too\nNot as competitive though"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#conclusions-3",
    "href": "teaching/Pricing Demand Slides.html#conclusions-3",
    "title": "Pricing and Demand",
    "section": "Conclusions",
    "text": "Conclusions\nDemand for Froot Loops: Multiple-Product Case\n\n\nWhat is \\(R^2\\)?\n\n.362\nWe explain 14% of the variation in sales with all prices"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#with-those-around-you",
    "href": "teaching/Pricing Demand Slides.html#with-those-around-you",
    "title": "Pricing and Demand",
    "section": "With those around you:",
    "text": "With those around you:\nRepeat this for Frosted flakes and Kashi\n\nEstimate single and multiple product demand models\nNote differences in elasticities\nAnd \\(R^2\\)"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#estimate-2",
    "href": "teaching/Pricing Demand Slides.html#estimate-2",
    "title": "Pricing and Demand",
    "section": "Estimate",
    "text": "Estimate\nDemand for Frosted Flakes: Single-Product Case\n\n\nff_single &lt;- lm( data = data, log(sales_ff) ~ log(price_ff))\n\nsummary(ff_single)$coef |&gt; round(3)\n\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)      8.782      0.665  13.213    0.000\nlog(price_ff)   -1.239      0.345  -3.590    0.001"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#conclusions-4",
    "href": "teaching/Pricing Demand Slides.html#conclusions-4",
    "title": "Pricing and Demand",
    "section": "Conclusions",
    "text": "Conclusions\nDemand for Frosted Flakes: Single-Product Case\n\n\nWhat is the own-price elasticity?\n\n-1.24\n1% increase in price reduces sales by 1.24%\nDon’t increase prices!! Reduce them!\n\nLook at the \\(R^2\\) from this model. What is it?\n\n~10%"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#estimate-3",
    "href": "teaching/Pricing Demand Slides.html#estimate-3",
    "title": "Pricing and Demand",
    "section": "Estimate",
    "text": "Estimate\nDemand for Frosted Flakes: Multiple-Product Case\n\n\nff_multiple &lt;- lm( data = data, log(sales_ff) ~ log(price_fl) + log(price_ff) + log(price_k))\n\nsummary(ff_multiple)$coef |&gt; round(3)\n\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)      6.248      1.417   4.411    0.000\nlog(price_fl)    1.326      0.428   3.098    0.003\nlog(price_ff)   -1.674      0.364  -4.603    0.000\nlog(price_k)     0.355      0.456   0.779    0.438"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#conclusions-5",
    "href": "teaching/Pricing Demand Slides.html#conclusions-5",
    "title": "Pricing and Demand",
    "section": "Conclusions",
    "text": "Conclusions\nDemand for Frosted Flakes: Multiple-Product Case\n\n\nWhat is the own-price elasticity?\n\n-1.3\n1% increase in price reduces sales by 1.3%\nDon’t increase prices!! Reduce them!"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#conclusions-6",
    "href": "teaching/Pricing Demand Slides.html#conclusions-6",
    "title": "Pricing and Demand",
    "section": "Conclusions",
    "text": "Conclusions\nDemand for Frosted Flakes: Multiple-Product Case\n\n\nWhat is cross-price elasticity?\n\nKashi:\n\n.35\nPrices don’t impact us a lot\n\nFroot loops:\n\n1.67\nPrices impact a lot"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#conclusions-7",
    "href": "teaching/Pricing Demand Slides.html#conclusions-7",
    "title": "Pricing and Demand",
    "section": "Conclusions",
    "text": "Conclusions\nDemand for Frosted Flakes: Multiple-Product Case\n\n\nWhat is \\(R^2\\)?\n\n.19\nWe explain 19% of the variation in sales with all prices"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#estimate-4",
    "href": "teaching/Pricing Demand Slides.html#estimate-4",
    "title": "Pricing and Demand",
    "section": "Estimate",
    "text": "Estimate\nDemand for Kashi: Single-Product Case\n\n\nk_single &lt;- lm( data = data, log(sales_k) ~ log(price_k))\n\nsummary(k_single)$coef |&gt; round(3)\n\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)     7.126      0.377  18.914        0\nlog(price_k)   -1.502      0.178  -8.446        0"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#conclusions-8",
    "href": "teaching/Pricing Demand Slides.html#conclusions-8",
    "title": "Pricing and Demand",
    "section": "Conclusions",
    "text": "Conclusions\nDemand for Kashi: Single-Product Case\n\n\nWhat is the own-price elasticity?\n\n-1.5\n1% increase in price reduces sales by 1.5%\nDon’t increase prices!! Reduce them!\n\nLook at the \\(R^2\\) from this model. What is it?\n\n~40%"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#estimate-5",
    "href": "teaching/Pricing Demand Slides.html#estimate-5",
    "title": "Pricing and Demand",
    "section": "Estimate",
    "text": "Estimate\nDemand for Kashi: Multiple-Product Case\n\n\nk_multiple &lt;- lm( data = data, log(sales_k) ~ log(price_fl) + log(price_ff) + log(price_k))\n\nsummary(k_multiple)$coef |&gt; round(3)\n\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)      5.704      0.539  10.579    0.000\nlog(price_fl)    0.336      0.163   2.060    0.042\nlog(price_ff)    0.265      0.138   1.917    0.058\nlog(price_k)    -1.384      0.173  -7.980    0.000"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#conclusions-9",
    "href": "teaching/Pricing Demand Slides.html#conclusions-9",
    "title": "Pricing and Demand",
    "section": "Conclusions",
    "text": "Conclusions\nDemand for Kashi: Multiple-Product Case\n\n\nWhat is the own-price elasticity?\n\n-1.4\n1% increase in price reduces sales by 1.4%\nDon’t increase prices!! Reduce them!"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#conclusions-10",
    "href": "teaching/Pricing Demand Slides.html#conclusions-10",
    "title": "Pricing and Demand",
    "section": "Conclusions",
    "text": "Conclusions\nDemand for Kashi: Multiple-Product Case\n\n\nWhat is cross-price elasticity?\n\nFrosted Flakes:\n\n.27\nFrosted flakes prices don’t impact us a lot\n\nFroot loops:\n\n.34\nFroot loops prices don’t impact a lot"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#conclusions-11",
    "href": "teaching/Pricing Demand Slides.html#conclusions-11",
    "title": "Pricing and Demand",
    "section": "Conclusions",
    "text": "Conclusions\nDemand for Kashi: Multiple-Product Case\n\n\nWhat is \\(R^2\\)?\n\n.47\nWe explain 47% of the variation in sales with all prices"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#change-in-prices",
    "href": "teaching/Pricing Demand Slides.html#change-in-prices",
    "title": "Pricing and Demand",
    "section": "Change in prices",
    "text": "Change in prices\nNow assume that:\n\nFroot Loops increases its price by 5%\nKashi reduces its price by 15%\n\nCalculate the percentage change in demand for each of the three products"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#change-in-prices-1",
    "href": "teaching/Pricing Demand Slides.html#change-in-prices-1",
    "title": "Pricing and Demand",
    "section": "Change in prices",
    "text": "Change in prices\nFor Froot Loops:\n\nDefine price changes\n\n\nchange_in_fl &lt;- 0.05\nchange_in_ff &lt;- 0\nchange_in_k &lt;- -0.15"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#change-in-prices-2",
    "href": "teaching/Pricing Demand Slides.html#change-in-prices-2",
    "title": "Pricing and Demand",
    "section": "Change in prices",
    "text": "Change in prices\nFor Froot Loops:\n\nDefine price changes\n\n\nchange_in_fl &lt;- 0.05\nchange_in_ff &lt;- 0\nchange_in_k &lt;- -0.15\n\n\nDefine demand changes\n\n\n\nbeta_fl_fl &lt;- fl_multiple$coefficients[[2]]\nbeta_fl_ff &lt;- fl_multiple$coefficients[[3]]\nbeta_fl_k &lt;- fl_multiple$coefficients[[4]]"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#change-in-prices-3",
    "href": "teaching/Pricing Demand Slides.html#change-in-prices-3",
    "title": "Pricing and Demand",
    "section": "Change in prices",
    "text": "Change in prices\nFor Froot Loops:\n\nCalculate ratio of sales\n\n\n\nfl_ratio &lt;- (1 + change_in_fl) ^ beta_fl_fl * (1 + change_in_ff) ^ beta_fl_ff * (1 + change_in_k) ^ beta_fl_k\nfl_ratio\n\n[1] 0.8433951\n\nfl_pct &lt;- 100*(fl_ratio-1)\nfl_pct\n\n[1] -15.66049"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#repeat-this-for-the-other-two-products",
    "href": "teaching/Pricing Demand Slides.html#repeat-this-for-the-other-two-products",
    "title": "Pricing and Demand",
    "section": "Repeat this for the other two products",
    "text": "Repeat this for the other two products\n\nFrosted Flakes\n\nbeta_ff_fl &lt;- ff_multiple$coefficients[[2]]\nbeta_ff_ff &lt;- ff_multiple$coefficients[[3]]\nbeta_ff_k &lt;- ff_multiple$coefficients[[4]]\n\nff_ratio &lt;- (1 + change_in_fl) ^ beta_ff_fl * (1 + change_in_ff) ^ beta_ff_ff * (1 + change_in_k) ^ beta_ff_k\nff_ratio\n\n[1] 1.00707\n\nff_pct &lt;- 100*(ff_ratio-1)\nff_pct\n\n[1] 0.7069734\n\n\nKashi\n\nbeta_k_fl &lt;- k_multiple$coefficients[[2]]\nbeta_k_ff &lt;- k_multiple$coefficients[[3]]\nbeta_k_k &lt;- k_multiple$coefficients[[4]]\n\nk_ratio &lt;- (1 + change_in_fl) ^ beta_k_fl * (1 + change_in_ff) ^ beta_k_ff * (1 + change_in_k) ^ beta_k_k\nk_ratio\n\n[1] 1.27296\n\nk_pct &lt;- 100*(k_ratio-1)\nk_pct\n\n[1] 27.29603"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#estimated-profit",
    "href": "teaching/Pricing Demand Slides.html#estimated-profit",
    "title": "Pricing and Demand",
    "section": "Estimated Profit",
    "text": "Estimated Profit\nAssume that the base prices of three products are:\n\nFL = $2.99\nFF = $3.25\nK = $4.99\n\nAnd base sales levels are:\n\nFL = 566\nFF = 540\nK = 302\n\nAssuming the same percentage price changes, what do we expect i) demand and ii) profits for each product at the new prices to be?"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#estimated-profit-1",
    "href": "teaching/Pricing Demand Slides.html#estimated-profit-1",
    "title": "Pricing and Demand",
    "section": "Estimated Profit",
    "text": "Estimated Profit\n\nDefine baseline prices and demands\n\n\nprice_FL &lt;- 2.99\nprice_FF &lt;- 3.25\nprice_K &lt;- 4.99\n\ndemand_FL &lt;- 566\ndemand_FF &lt;- 540\ndemand_K &lt;- 302"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#estimated-profit-2",
    "href": "teaching/Pricing Demand Slides.html#estimated-profit-2",
    "title": "Pricing and Demand",
    "section": "Estimated Profit",
    "text": "Estimated Profit\n\nDefine new prices and demands\n\n\nprice_FL_2 &lt;- price_FL * (1 + change_in_fl)\nprice_FF_2 &lt;- price_FF * (1 + change_in_ff)\nprice_K_2 &lt;- price_K * (1 + change_in_k)\n\ndemand_FL_2 &lt;- demand_FL * fl_ratio\ndemand_FF_2 &lt;- demand_FF * ff_ratio\ndemand_K_2 &lt;- demand_K * k_ratio\n\ndemand_FL_2; demand_FF_2; demand_K_2\n\n[1] 477.3616\n\n\n[1] 543.8177\n\n\n[1] 384.434"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#estimated-profit-3",
    "href": "teaching/Pricing Demand Slides.html#estimated-profit-3",
    "title": "Pricing and Demand",
    "section": "Estimated Profit",
    "text": "Estimated Profit\n\nEstimate profit\n\n\nUsing marginal costs of:\n\n$1.50 per box of Frosted Flakes and Froot Loops\n$2.00 per box of Kashi\n\n\n\nmcost_ff &lt;- 1.5\nmcost_fl &lt;- 1.5\nmcost_k &lt;- 2\n\npft_ff_2 &lt;- demand_FF_2*(price_FF_2-mcost_ff); pft_ff_2\n\n[1] 951.6809\n\npft_fl_2 &lt;- demand_FL_2*(price_FL_2-mcost_fl); pft_fl_2\n\n[1] 782.6344\n\npft_k_2 &lt;- demand_K_2*(price_K_2-mcost_k); pft_k_2\n\n[1] 861.7088"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#estimated-profit-4",
    "href": "teaching/Pricing Demand Slides.html#estimated-profit-4",
    "title": "Pricing and Demand",
    "section": "Estimated Profit",
    "text": "Estimated Profit\n\nCompare to baseline:\n\n\npft_ff_1 &lt;- demand_FF*(price_FF-mcost_ff); pft_ff_1\n\n[1] 945\n\npft_fl_1 &lt;- demand_FL*(price_FL-mcost_fl); pft_fl_1\n\n[1] 843.34\n\npft_k_1 &lt;- demand_K*(price_K-mcost_k); pft_k_1\n\n[1] 902.98"
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#conclusions-12",
    "href": "teaching/Pricing Demand Slides.html#conclusions-12",
    "title": "Pricing and Demand",
    "section": "Conclusions",
    "text": "Conclusions\n\n\nWe can use demand models to predict demand for single products, as well as multiple.\nAdding parameters for other products allows us to estimate demand in a competitive landscape.\nWe use these models (rather than conjoint) when we have observational data about sales and product attributes\n\nPrice is one attribute. We could have done this with others."
  },
  {
    "objectID": "teaching/Pricing Demand Slides.html#conclusions-13",
    "href": "teaching/Pricing Demand Slides.html#conclusions-13",
    "title": "Pricing and Demand",
    "section": "Conclusions",
    "text": "Conclusions\n\n\nElasticities &gt; 0 indicate as attribute increases, so do sales\nElasticities &lt; 0 indicate as attribute increases, sales decrease\nElasticities &lt; -1 indicate that we should decrease prices, as sales will increase more to make up\nWe could have extended this example a lot, to simulate profits, entire markets, etc."
  },
  {
    "objectID": "teaching/06 effect sizes.html#thus-far",
    "href": "teaching/06 effect sizes.html#thus-far",
    "title": "Effect Sizes",
    "section": "Thus far",
    "text": "Thus far\nSo far, we have learned about:\n\nMeans\nVariance\nStatistical significance\n\n(among other things)"
  },
  {
    "objectID": "teaching/06 effect sizes.html#thus-far-1",
    "href": "teaching/06 effect sizes.html#thus-far-1",
    "title": "Effect Sizes",
    "section": "Thus far",
    "text": "Thus far\nWe’ve learned to say things like:\n\n\nThe difference in clicking between group A and B is 2%\n\nAnd this is significant because p &lt; .001\n\nWith every $10,000 increase in income, customers spend $25 more in our stores\n\nAnd this slope is significantly different from 0 because p = .02\n\nCustomers who are 25-34 are more interested in our product than those who are 35-44\n\n\\(M_{25-34}\\) = 4.85/6\n\\(M_{25-34}\\) = 4.32/6\nThis might be due to chance, as p = .09"
  },
  {
    "objectID": "teaching/06 effect sizes.html#thus-far-2",
    "href": "teaching/06 effect sizes.html#thus-far-2",
    "title": "Effect Sizes",
    "section": "Thus far",
    "text": "Thus far\nHave we learned to say things like:\n\n\nThe difference in clicking between group A and B is 2%\n\nThis is a big difference?\n\nWith every $10,000 increase in income, customers spend $25 more in our stores\n\nThis is a big difference?\n\nCustomers who are 25-34 are more interested in our product than those who are 35-44\n\n\\(M_{25-34}\\) = 4.85/6\n\\(M_{25-34}\\) = 4.32/6\nThis is a big difference?"
  },
  {
    "objectID": "teaching/06 effect sizes.html#thus-far-3",
    "href": "teaching/06 effect sizes.html#thus-far-3",
    "title": "Effect Sizes",
    "section": "Thus far",
    "text": "Thus far\nNo!\nFor the clearest example, let’s focus on the third:\n\nThe one that uses a 0-6 scale\n\n\n\nWhat is a difference of .53 on a 0-6 scale?\n\nIs that big?\nDoes it matter in this context?\nTo answer this, we are going to learn about effect sizes"
  },
  {
    "objectID": "teaching/06 effect sizes.html#effect-sizes",
    "href": "teaching/06 effect sizes.html#effect-sizes",
    "title": "Effect Sizes",
    "section": "Effect sizes",
    "text": "Effect sizes\nEffect sizes put our results into a standard format.\n\n\nThey do not tell us if our result is statistically significant or not.\n\nWe use them after that\n\nThey tell us about how big our results are\n\nAgain, in a standardized format"
  },
  {
    "objectID": "teaching/06 effect sizes.html#effect-sizes-1",
    "href": "teaching/06 effect sizes.html#effect-sizes-1",
    "title": "Effect Sizes",
    "section": "Effect sizes",
    "text": "Effect sizes\nEffect sizes put our results into a standard format.\nThere are two kinds of effect sizes, broadly:\n\n\nStandardized differences\n\nThese give us a standardized way to say whether the difference between groups is big\n\nVariance explained\n\nThese tell us whether some variable explains a lot or a little of our DV"
  },
  {
    "objectID": "teaching/06 effect sizes.html#effect-sizes-2",
    "href": "teaching/06 effect sizes.html#effect-sizes-2",
    "title": "Effect Sizes",
    "section": "Effect sizes",
    "text": "Effect sizes\nEffect sizes put our results into a standard format.\nWe will learn two today\n\n\nStandardized differences\n\nCohen’s d\n\n\\(\\frac{(M_A - M_B)}{SD_{AB}}\\)\n\n\nVariance explained\n\n\\(R^2\\)\n\n\\(1 - \\frac{SSR}{n - p - 1} \\div \\frac{SST}{n - 1}\\)\n\nThese tell us whether some variable explains a lot or a little of our DV"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d",
    "href": "teaching/06 effect sizes.html#cohens-d",
    "title": "Effect Sizes",
    "section": "Cohen’s d",
    "text": "Cohen’s d\n\\(\\frac{(M_A - M_B)}{SD_{AB}}\\)\n\n\\(M_A\\): Mean of group A\n\\(M_B\\): Mean of group B\n\\(SD_{AB}\\): Pooled standard deviation\n\nAveraging the standard deviation is fine\n\n\nThis tells us how large the difference between groups is in terms of total variance in the data."
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples",
    "href": "teaching/06 effect sizes.html#cohens-d---examples",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nHeights of men and women in the US:\nAre men and women different heights on average?\n\n\n\\(M_{Male}\\) = 69 inches\n\\(M_{Female}\\) = 64 inches\n\\(SD_{Height}\\) = 2.75 inches\nCohen’s d?\n\n1.81"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples-1",
    "href": "teaching/06 effect sizes.html#cohens-d---examples-1",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nHeights of men and women in the US:\n\nCohen’s d = 1.81"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples-2",
    "href": "teaching/06 effect sizes.html#cohens-d---examples-2",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nHeights of men and women in the US:\n\nCohen’s d = 1.81"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples-3",
    "href": "teaching/06 effect sizes.html#cohens-d---examples-3",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nAre people are more aggressive toward individuals who have provoked them?\n\n\n\\(M_{Provoked}\\) = 8.232/10\n\\(M_{Unprovoked}\\) = 4.4/10\n\\(SD_{Aggression}\\) = 3.22\nCohen’s d?\n\n1.19"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples-4",
    "href": "teaching/06 effect sizes.html#cohens-d---examples-4",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nAre people are more aggressive toward individuals who have provoked them?\n\nCohen’s d = 1.19"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples-5",
    "href": "teaching/06 effect sizes.html#cohens-d---examples-5",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nAre people are more aggressive toward individuals who have provoked them?\n\nCohen’s d = 1.19"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples-6",
    "href": "teaching/06 effect sizes.html#cohens-d---examples-6",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nAre people who are seen as more credible are also more persuasive?\n\n\n\\(M_{Credible}\\) = 5.42/10\n\\(M_{Not}\\) = 4.76/10\n\\(SD_{Persuasion}\\) = 3.29\nCohen’s d?\n\n.20"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples-7",
    "href": "teaching/06 effect sizes.html#cohens-d---examples-7",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nAre people who are seen as more credible are also more persuasive?\n\nCohen’s d = .20"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples-8",
    "href": "teaching/06 effect sizes.html#cohens-d---examples-8",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nAre people who are seen as more credible are also more persuasive?\n\nCohen’s d = .20"
  },
  {
    "objectID": "teaching/06 effect sizes.html#contextualize-your-effect-sizes",
    "href": "teaching/06 effect sizes.html#contextualize-your-effect-sizes",
    "title": "Effect Sizes",
    "section": "Contextualize your effect sizes",
    "text": "Contextualize your effect sizes\nSometimes you can look to other research\n\nOr benchmarks like the things above\n\nSometimes you cannot\n\nOne good comparison is covariates"
  },
  {
    "objectID": "teaching/06 effect sizes.html#contextualizing-with-covariates",
    "href": "teaching/06 effect sizes.html#contextualizing-with-covariates",
    "title": "Effect Sizes",
    "section": "Contextualizing with covariates",
    "text": "Contextualizing with covariates\nHypothesis: Mode of ordering (smartphone vs. desktop) will influence people’s portion choices\n\\(Portion Size = \\beta_{Device}xDevice + \\beta_{Hunger}Hunger + \\beta_{Dieting}Dieting\\)"
  },
  {
    "objectID": "teaching/06 effect sizes.html#contextualizing-with-covariates-1",
    "href": "teaching/06 effect sizes.html#contextualizing-with-covariates-1",
    "title": "Effect Sizes",
    "section": "Contextualizing with covariates",
    "text": "Contextualizing with covariates\nHypothesis: Mode of ordering (smartphone vs. desktop) will influence people’s portion choices\n\\(Portion Size = \\beta_{Device}xDevice + \\beta_{Hunger}Hunger + \\beta_{Dieting}Dieting\\)\n\n\n…And use common sense…"
  },
  {
    "objectID": "teaching/06 effect sizes.html#r-squared",
    "href": "teaching/06 effect sizes.html#r-squared",
    "title": "Effect Sizes",
    "section": "R-Squared",
    "text": "R-Squared\n\\(1 - \\frac{SSR}{n - p - 1} \\div \\frac{SST}{n - 1}\\)\nThis tells you:\n\n\nFor an entire model, how much of all of the variance you are explaining\n\nWe get this result from lm()\n\nFor each individual effect, how much of all of the variance it explains\n\nWe can get this result from anova()"
  },
  {
    "objectID": "teaching/06 effect sizes.html#r-squared-1",
    "href": "teaching/06 effect sizes.html#r-squared-1",
    "title": "Effect Sizes",
    "section": "R-Squared",
    "text": "R-Squared\nFrom anova()\n\ncustomerData &lt;- read.csv('customerData.csv')\n\nm_1 &lt;- lm( data = customerData, sat.service ~ 1) # Just the mean\nm_2 &lt;- lm( data = customerData, sat.service ~ email) # Effect of email\nm_3 &lt;- lm( data = customerData, sat.service ~ email + income) # Effect of email and income\n\nanova(m_1, m_2, m_3)\n\nAnalysis of Variance Table\n\nModel 1: sat.service ~ 1\nModel 2: sat.service ~ email\nModel 3: sat.service ~ email + income\n  Res.Df     RSS Df Sum of Sq        F  Pr(&gt;F)    \n1    590 1187.70                                  \n2    589 1179.40  1      8.30   5.9544 0.01497 *  \n3    588  819.51  1    359.89 258.2261 &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "teaching/06 effect sizes.html#r-squared-2",
    "href": "teaching/06 effect sizes.html#r-squared-2",
    "title": "Effect Sizes",
    "section": "R-Squared",
    "text": "R-Squared\nFrom anova()\n\n\nAnalysis of Variance Table\n\nModel 1: sat.service ~ 1\nModel 2: sat.service ~ email\nModel 3: sat.service ~ email + income\n  Res.Df     RSS Df Sum of Sq        F  Pr(&gt;F)    \n1    590 1187.70                                  \n2    589 1179.40  1      8.30   5.9544 0.01497 *  \n3    588  819.51  1    359.89 258.2261 &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\\(R^2_{email}\\)?\n\n\\(1 - \\frac{1592}{658 - 1 - 1} \\div \\frac{1606}{658 - 1}\\)\n.009\n\n\\(R^2_{income}\\)?\n\n\\(1 - \\frac{945}{658 - 2 - 1} \\div \\frac{1606}{658 - 1}\\)\n.407"
  },
  {
    "objectID": "teaching/06 effect sizes.html#r-squared-3",
    "href": "teaching/06 effect sizes.html#r-squared-3",
    "title": "Effect Sizes",
    "section": "R-Squared",
    "text": "R-Squared\nFrom lm()\n\nsummary(m_2)\n\n\nCall:\nlm(formula = sat.service ~ email, data = customerData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9813 -0.7347  0.0187  1.0187  4.0187 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.98131    0.09673  41.159   &lt;2e-16 ***\nemailyes    -0.24656    0.12111  -2.036   0.0422 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.415 on 589 degrees of freedom\n  (409 observations deleted due to missingness)\nMultiple R-squared:  0.006987,  Adjusted R-squared:  0.005301 \nF-statistic: 4.144 on 1 and 589 DF,  p-value: 0.04222"
  },
  {
    "objectID": "teaching/06 effect sizes.html#effect-size-conclusion",
    "href": "teaching/06 effect sizes.html#effect-size-conclusion",
    "title": "Effect Sizes",
    "section": "Effect Size Conclusion",
    "text": "Effect Size Conclusion\n\n\nThere are lots of effect size measures out there\nThey are useful, in that it’s nice to contextualize our effects\nThey come in two forms:\n\nStandardized differences\n\nThese give us a standardized way to say whether the difference between groups is big\n\nVariance explained\n\nThese tell us whether some variable explains a lot or a little of our DV"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#relationships-between-multiple-variables",
    "href": "teaching/03 Correlation and Regression.html#relationships-between-multiple-variables",
    "title": "Correlation and Linear Regression",
    "section": "Relationships between multiple variables",
    "text": "Relationships between multiple variables\nWe’ve just started to talk about relationships between multiple variables.\n\n\nDo different advertisements lead to different interest?\nHow do different features of shoes lead to different ratings?\nDo stores closer to population centers sign up more customers than further stores?\n\n\n\nWhy might we care a lot about relationships between multiple variables?\n\n\n\nOften, we can manipulate factors related to them.\netc."
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#relationships-between-multiple-variables-1",
    "href": "teaching/03 Correlation and Regression.html#relationships-between-multiple-variables-1",
    "title": "Correlation and Linear Regression",
    "section": "Relationships between multiple variables",
    "text": "Relationships between multiple variables\nIdentifying multivariate relationships helps us as marketers.\n\n\ne.g., If stores in population centers do better than further ones, what is an obvious implication?\n\nMove people closer to our stores\n\nNo!\n\nPlace new stores near population centers\n\n\n\n\nIn this module, we focus on:\n\nUnderstanding the relationships between pairs of variables (multivariate data)\nVisualizing these relationships\nComputing statistics that describe their associations\n\nCorrelation coefficients\nRegression coefficients"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#relationships-between-multiple-variables-2",
    "href": "teaching/03 Correlation and Regression.html#relationships-between-multiple-variables-2",
    "title": "Correlation and Linear Regression",
    "section": "Relationships between multiple variables",
    "text": "Relationships between multiple variables\nI’ve taken real data from rei.com and added a few simulated variables for class.\nDownload it from Canvas, and load it into R!\n\nreiData &lt;- read.csv('rei_products.csv')"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#our-setting",
    "href": "teaching/03 Correlation and Regression.html#our-setting",
    "title": "Correlation and Linear Regression",
    "section": "Our Setting",
    "text": "Our Setting\n\n\nFor today’s class, we are going to take the role of a market analyst at REI\nSpecifically, our job is to identify new types of shoes that we should stock\nWe are going to use data from REI stores to make this decision"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#these-data",
    "href": "teaching/03 Correlation and Regression.html#these-data",
    "title": "Correlation and Linear Regression",
    "section": "These Data",
    "text": "These Data\n\n\nHow many shoes?\n1022\nHow many brands?\n\n54\n\nWhat’s a good way to look at these data?\nDescribe these variables:\n\nprice\navg_rating\nsales\nmens\navg_size\navg_running\ncushion\nweight"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#how-can-we-predict-sales",
    "href": "teaching/03 Correlation and Regression.html#how-can-we-predict-sales",
    "title": "Correlation and Linear Regression",
    "section": "How Can We Predict sales?",
    "text": "How Can We Predict sales?\n\n\nWe want to know what variables impact sales\nIf our goal is to increase future sales, these variables are worth knowing\nTwo general types of predictor variables\n\nGroups (discrete)\nContinuous\n\nLuckily, we can test them the same way"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors",
    "text": "Continuous Predictors\nWith continuous predictors (independent variables), we are seeing if some numeric variable predicts outcomes\n\nIs there a meaningful relationship between these two variables?\n\nDoes x cause y?"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#outline-for-today",
    "href": "teaching/03 Correlation and Regression.html#outline-for-today",
    "title": "Correlation and Linear Regression",
    "section": "Outline for Today",
    "text": "Outline for Today\n\nData = Model + Error\nWhy is the mean our best guess (model) when we know nothing else?\nUsing lm() to find better (linear) models\nTesting whether that model is better"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#data-model-error",
    "href": "teaching/03 Correlation and Regression.html#data-model-error",
    "title": "Correlation and Linear Regression",
    "section": "Data = Model + Error",
    "text": "Data = Model + Error\n\nThe whole point of this is to find better models to predict our data\n“model” can be a single guess, a guess at a relationship, or something else\nRegardless, we are trying to minimize Error\n\nWhich is the sum of squared error"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors-1",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors-1",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors",
    "text": "Continuous Predictors\nIn our REI data, our outcome is sales"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors-2",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors-2",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors",
    "text": "Continuous Predictors\nIn our REI data, our outcome is sales\n\nWithout knowing anything else, what is our best guess at the sales a new shoe will bring?\n\n\n\nThe mean of sales (10.05)"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors-3",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors-3",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors",
    "text": "Continuous Predictors\nIn our REI data, our outcome is sales\n\nWithout knowing anything else, what is our best guess at the sales a new shoe will bring?\n\nThe mean of sales (10.05)\n\nHow wrong will we be normally?\n\n\n\nThe standard deviation of sales (0.92)"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors-4",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors-4",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors",
    "text": "Continuous Predictors\nIn our REI data, our outcome is sales\n\nWithout knowing anything else, what is our best guess at the sales a new shoe will bring?\n\nThe mean of sales (10.05)\n\nHow wrong will we be normally?\n\nThe standard deviation of sales (0.92)\n\nHow could we get better guesses?\n\n\n\nUse some other variable!\n\nLike weight"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors-5",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors-5",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors",
    "text": "Continuous Predictors\nIn our REI data, our outcome is sales\n\n\nWe can use weight to predict sales better\n\nWhy would we care whether weight predicts sales?\nBecause we can make new shoes with different weights!\n\nWhat is one way we have looked at continuous predictors so far (in R)?"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors-scatterplots",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors-scatterplots",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors: Scatterplots",
    "text": "Continuous Predictors: Scatterplots\n\n\nggplot(data = reiData,\n       aes(x = weight, y = sales)) +\n  geom_point(alpha = .5) +\n  geom_smooth(method = 'lm', se = F, size = 1) +\n  theme_bw()"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors-6",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors-6",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors",
    "text": "Continuous Predictors\nIs this scatterplot showing a strong relationship?\n\n\nScatterplots provide a lot of visual information\nBut they’re very ~vibesy~\n\nNot precise\n\nAnd when there are more than a few variables, they’re a mess\nTherefore, it’s nice to also have a number"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors-7",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors-7",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors",
    "text": "Continuous Predictors\nAre these scatterplots showing strong relationships?\nTwo ways to know with numbers:\n\n\nRegression\n\nUsing lm() in R\nBueno\n\nCorrelations:\n\nUsing cor() in R"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression",
    "href": "teaching/03 Correlation and Regression.html#linear-regression",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nRegression does the following:\n\nIt helps us to understand how two (or more) variables vary together\nWe can make clear predictions from regression coefficients\nWe can test regression coefficients against a null hypothesis"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-1",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-1",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nSo what is linear regression?\n\nWhy did I say that the mean of a variable is our best guess at any given value of that variable?\nLet’s try some guesses of reiData$sales\n\n\nTo understand this, let’s look the first 10 values of reiData\n\nreiDataSmall &lt;- head(reiData, 20)\n\nAnd I’m going to only keep the columns I think are useful for this example\n\nreiDataSmall &lt;- reiDataSmall[, c('weight', 'sales')]\nreiDataSmall\n\n     weight     sales\n1  3.160536 10.069211\n2  4.970014 10.131396\n3  6.759033  8.856539\n4  3.068526 10.783086\n5  5.254373 11.149486\n6  4.769999 10.239619\n7  3.831546 10.078726\n8  3.805244 10.128758\n9  3.172416 10.696915\n10 2.901195  9.825507\n11 2.461316 10.569462\n12 2.737338 10.440997\n13 2.982250  9.584194\n14 5.910042 11.216148\n15 5.154571  9.845226\n16 6.090791  9.356569\n17 5.153109 10.137007\n18 4.059393  9.513671\n19 5.565751  9.480360\n20 2.473582 11.151987"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nIf we are interested in the sales column, let’s see what it looks like:\n\nggplot(data = reiDataSmall, # Scatterplot\n       aes(x = 0, \n           # ^ This is just a hacky way to get everything on the same spot on the x-axis\n           y = sales)) +\n  geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-1",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-1",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\n\n\nWe are going to try to make the BEST guess we can at each value of sales\nThe best guess will be the one with the lowest possible squared error (residuals) between:\n\nOur guess and the real data\n\nWhy squared?\n\nThis makes every error positive\n\nPenalizing us for guessing too high and too low\n\nThis penalizes us for very wrong guesses more than slightly wrong ones"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-2",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-2",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nI’ll start by guessing the mean, or average\n\nguess &lt;- mean(reiDataSmall$sales)\nreiDataSmall$guess &lt;- guess\nreiDataSmall\n\n     weight     sales    guess\n1  3.160536 10.069211 10.16274\n2  4.970014 10.131396 10.16274\n3  6.759033  8.856539 10.16274\n4  3.068526 10.783086 10.16274\n5  5.254373 11.149486 10.16274\n6  4.769999 10.239619 10.16274\n7  3.831546 10.078726 10.16274\n8  3.805244 10.128758 10.16274\n9  3.172416 10.696915 10.16274\n10 2.901195  9.825507 10.16274\n11 2.461316 10.569462 10.16274\n12 2.737338 10.440997 10.16274\n13 2.982250  9.584194 10.16274\n14 5.910042 11.216148 10.16274\n15 5.154571  9.845226 10.16274\n16 6.090791  9.356569 10.16274\n17 5.153109 10.137007 10.16274\n18 4.059393  9.513671 10.16274\n19 5.565751  9.480360 10.16274\n20 2.473582 11.151987 10.16274"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-3",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-3",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nHow wrong was I?\n\nSubtract the real data from the guess\n\n\nreiDataSmall$how_wrong &lt;- reiDataSmall$guess - reiDataSmall$sales\n\n\n\nSquare them to make them all positive\n\n\nreiDataSmall$how_wrong2 &lt;- (reiDataSmall$guess - reiDataSmall$sales) ^ 2\nreiDataSmall\n\n     weight     sales    guess   how_wrong   how_wrong2\n1  3.160536 10.069211 10.16274  0.09353188 0.0087482124\n2  4.970014 10.131396 10.16274  0.03134738 0.0009826582\n3  6.759033  8.856539 10.16274  1.30620416 1.7061693156\n4  3.068526 10.783086 10.16274 -0.62034322 0.3848257155\n5  5.254373 11.149486 10.16274 -0.98674322 0.9736621774\n6  4.769999 10.239619 10.16274 -0.07687579 0.0059098874\n7  3.831546 10.078726 10.16274  0.08401714 0.0070588794\n8  3.805244 10.128758 10.16274  0.03398506 0.0011549846\n9  3.172416 10.696915 10.16274 -0.53417160 0.2853392931\n10 2.901195  9.825507 10.16274  0.33723672 0.1137286034\n11 2.461316 10.569462 10.16274 -0.40671836 0.1654198253\n12 2.737338 10.440997 10.16274 -0.27825397 0.0774252726\n13 2.982250  9.584194 10.16274  0.57854943 0.3347194427\n14 5.910042 11.216148 10.16274 -1.05340448 1.1096609896\n15 5.154571  9.845226 10.16274  0.31751697 0.1008170242\n16 6.090791  9.356569 10.16274  0.80617426 0.6499169379\n17 5.153109 10.137007 10.16274  0.02573597 0.0006623402\n18 4.059393  9.513671 10.16274  0.64907221 0.4212947353\n19 5.565751  9.480360 10.16274  0.68238311 0.4656467078\n20 2.473582 11.151987 10.16274 -0.98924365 0.9786030031"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-4",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-4",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nThe column how_wrong2 is equal to guess - sales, squared - …i.e., the guess, minus the real data, squared\n\nHow wrong was I in total?\n\nsum(reiDataSmall$how_wrong2)\n\n[1] 7.791746"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-5",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-5",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nTo see if that’s the best guess we could have made:\n\n\nLet’s try some others!"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-6",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-6",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nI’ll guess something else\n\nguess &lt;- 11\nreiDataSmall$guess &lt;- guess\nreiDataSmall\n\n     weight     sales guess   how_wrong   how_wrong2\n1  3.160536 10.069211    11  0.09353188 0.0087482124\n2  4.970014 10.131396    11  0.03134738 0.0009826582\n3  6.759033  8.856539    11  1.30620416 1.7061693156\n4  3.068526 10.783086    11 -0.62034322 0.3848257155\n5  5.254373 11.149486    11 -0.98674322 0.9736621774\n6  4.769999 10.239619    11 -0.07687579 0.0059098874\n7  3.831546 10.078726    11  0.08401714 0.0070588794\n8  3.805244 10.128758    11  0.03398506 0.0011549846\n9  3.172416 10.696915    11 -0.53417160 0.2853392931\n10 2.901195  9.825507    11  0.33723672 0.1137286034\n11 2.461316 10.569462    11 -0.40671836 0.1654198253\n12 2.737338 10.440997    11 -0.27825397 0.0774252726\n13 2.982250  9.584194    11  0.57854943 0.3347194427\n14 5.910042 11.216148    11 -1.05340448 1.1096609896\n15 5.154571  9.845226    11  0.31751697 0.1008170242\n16 6.090791  9.356569    11  0.80617426 0.6499169379\n17 5.153109 10.137007    11  0.02573597 0.0006623402\n18 4.059393  9.513671    11  0.64907221 0.4212947353\n19 5.565751  9.480360    11  0.68238311 0.4656467078\n20 2.473582 11.151987    11 -0.98924365 0.9786030031"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-7",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-7",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nHow wrong was I?\n\nSubtract the real data from the guess\n\n\nreiDataSmall$how_wrong &lt;- reiDataSmall$guess - reiDataSmall$sales\n\n\n\nSquare them to make them all positive\n\n\nreiDataSmall$how_wrong2 &lt;- (reiDataSmall$guess - reiDataSmall$sales) ^ 2\nreiDataSmall\n\n     weight     sales guess  how_wrong how_wrong2\n1  3.160536 10.069211    11  0.9307886 0.86636747\n2  4.970014 10.131396    11  0.8686041 0.75447313\n3  6.759033  8.856539    11  2.1434609 4.59442468\n4  3.068526 10.783086    11  0.2169135 0.04705148\n5  5.254373 11.149486    11 -0.1494865 0.02234620\n6  4.769999 10.239619    11  0.7603810 0.57817920\n7  3.831546 10.078726    11  0.9212739 0.84874557\n8  3.805244 10.128758    11  0.8712418 0.75906230\n9  3.172416 10.696915    11  0.3030852 0.09186061\n10 2.901195  9.825507    11  1.1744935 1.37943490\n11 2.461316 10.569462    11  0.4305384 0.18536330\n12 2.737338 10.440997    11  0.5590028 0.31248410\n13 2.982250  9.584194    11  1.4158062 2.00450713\n14 5.910042 11.216148    11 -0.2161477 0.04671984\n15 5.154571  9.845226    11  1.1547737 1.33350233\n16 6.090791  9.356569    11  1.6434310 2.70086548\n17 5.153109 10.137007    11  0.8629927 0.74475644\n18 4.059393  9.513671    11  1.4863290 2.20917378\n19 5.565751  9.480360    11  1.5196399 2.30930530\n20 2.473582 11.151987    11 -0.1519869 0.02310002"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-8",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-8",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nThe column how_wrong2 is equal to guess - sales, squared - …i.e., the guess, minus the real data, squared\n\nHow wrong was I in total?\n\nsum(reiDataSmall$how_wrong2)\n\n[1] 21.81172"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-9",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-9",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nI’ll guess something else\n\nguess &lt;- 9\nreiDataSmall$guess &lt;- guess\nreiDataSmall\n\n     weight     sales guess  how_wrong how_wrong2\n1  3.160536 10.069211     9  0.9307886 0.86636747\n2  4.970014 10.131396     9  0.8686041 0.75447313\n3  6.759033  8.856539     9  2.1434609 4.59442468\n4  3.068526 10.783086     9  0.2169135 0.04705148\n5  5.254373 11.149486     9 -0.1494865 0.02234620\n6  4.769999 10.239619     9  0.7603810 0.57817920\n7  3.831546 10.078726     9  0.9212739 0.84874557\n8  3.805244 10.128758     9  0.8712418 0.75906230\n9  3.172416 10.696915     9  0.3030852 0.09186061\n10 2.901195  9.825507     9  1.1744935 1.37943490\n11 2.461316 10.569462     9  0.4305384 0.18536330\n12 2.737338 10.440997     9  0.5590028 0.31248410\n13 2.982250  9.584194     9  1.4158062 2.00450713\n14 5.910042 11.216148     9 -0.2161477 0.04671984\n15 5.154571  9.845226     9  1.1547737 1.33350233\n16 6.090791  9.356569     9  1.6434310 2.70086548\n17 5.153109 10.137007     9  0.8629927 0.74475644\n18 4.059393  9.513671     9  1.4863290 2.20917378\n19 5.565751  9.480360     9  1.5196399 2.30930530\n20 2.473582 11.151987     9 -0.1519869 0.02310002"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-10",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-10",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nHow wrong was I?\n\nSubtract the real data from the guess\n\n\nreiDataSmall$how_wrong &lt;- reiDataSmall$guess - reiDataSmall$sales\n\n\n\nSquare them to make them all positive\n\n\nreiDataSmall$how_wrong2 &lt;- (reiDataSmall$guess - reiDataSmall$sales) ^ 2\nreiDataSmall\n\n     weight     sales guess  how_wrong how_wrong2\n1  3.160536 10.069211     9 -1.0692114 1.14321296\n2  4.970014 10.131396     9 -1.1313959 1.28005662\n3  6.759033  8.856539     9  0.1434609 0.02058103\n4  3.068526 10.783086     9 -1.7830865 3.17939738\n5  5.254373 11.149486     9 -2.1494865 4.62029208\n6  4.769999 10.239619     9 -1.2396190 1.53665537\n7  3.831546 10.078726     9 -1.0787261 1.16365003\n8  3.805244 10.128758     9 -1.1287582 1.27409505\n9  3.172416 10.696915     9 -1.6969148 2.87952000\n10 2.901195  9.825507     9 -0.8255065 0.68146104\n11 2.461316 10.569462     9 -1.5694616 2.46320975\n12 2.737338 10.440997     9 -1.4409972 2.07647300\n13 2.982250  9.584194     9 -0.5841938 0.34128242\n14 5.910042 11.216148     9 -2.2161477 4.91131075\n15 5.154571  9.845226     9 -0.8452263 0.71440747\n16 6.090791  9.356569     9 -0.3565690 0.12714145\n17 5.153109 10.137007     9 -1.1370073 1.29278555\n18 4.059393  9.513671     9 -0.5136710 0.26385794\n19 5.565751  9.480360     9 -0.4803601 0.23074587\n20 2.473582 11.151987     9 -2.1519869 4.63104763"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-11",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-11",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nThe column how_wrong2 is equal to guess - sales, squared - …i.e., the guess, minus the real data, squared\n\nHow wrong was I in total?\n\nsum(reiDataSmall$how_wrong2)\n\n[1] 34.83118"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-12",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-12",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nTo see if that’s the best guess we could have made:\n\n\nLet’s try some others!\nLet’s simulate a thousand other guesses:\n\nFrom the minimum sales to the maximum\n\n\n\n\n\n# Set up simulation with a data frame to hold results\nsims &lt;- 1000\nsimulation.results &lt;- data.frame(\n  guess = c(mean(reiDataSmall$sales),\n            seq(length.out = sims-1, \n              from = min(reiDataSmall$sales),\n              to = max(reiDataSmall$sales))),\n  how_wrong2 = rep(NA, times = sims)\n)"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-13",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-13",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\n\nRun simulation with for() loop\n\n\nfor( i in 1:sims){\n  simulation.results$how_wrong2[i] &lt;- sum(\n    (simulation.results$guess[i] - reiDataSmall$sales) ^2 )\n}"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-14",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-14",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\n\nWhat does it look like if I plot how wrong I was by what my guess was?\n\n\nggplot(data = simulation.results,\n       aes(x = guess,\n           y = how_wrong2)) +\n  geom_point()"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-15",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-15",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\n\nWhat is the minimum amount I was wrong?\n\n\nmin(simulation.results$how_wrong2)\n\n[1] 7.791746\n\n\n\nWhat was my guess at that point?\n\n\nbestguess &lt;- min(simulation.results$how_wrong2)\nsimulation.results[simulation.results$how_wrong2 == bestguess,]\n\n     guess how_wrong2\n1 10.16274   7.791746\n\n\n\nAnd what was the mean again?\n\n\nmean(reiDataSmall$sales)\n\n[1] 10.16274"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-16",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-16",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\n\nggplot(data = simulation.results,\n       aes(x = guess,\n           y = how_wrong2)) +\n  geom_point() + \n  geom_point(data = simulation.results[simulation.results$guess == mean(reiDataSmall$sales),],\n             aes(x = guess,\n                 y = how_wrong2),\n             color = 'red',\n             size = 4)"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#fun-fact",
    "href": "teaching/03 Correlation and Regression.html#fun-fact",
    "title": "Correlation and Linear Regression",
    "section": "Fun fact:",
    "text": "Fun fact:\n\nThe median will minimize the sum of absolute error:\n\n\nsimulation.results.median &lt;- data.frame(\n  guess = c(median(reiDataSmall$sales),\n            seq(length.out = sims-1, \n              from = min(reiDataSmall$sales),\n              to = max(reiDataSmall$sales))),\n  how_wrong = rep(NA, times = sims)\n)\n\nfor( i in 1:sims){\n  simulation.results.median$how_wrong[i] &lt;- sum(abs(simulation.results.median$guess[i] - reiDataSmall$sales))\n}"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#fun-fact-1",
    "href": "teaching/03 Correlation and Regression.html#fun-fact-1",
    "title": "Correlation and Linear Regression",
    "section": "Fun fact:",
    "text": "Fun fact:\n\nThe median will minimize the sum of absolute error:\n\n\nggplot(data = simulation.results.median,\n       aes(x = guess,\n           y = how_wrong)) +\n  geom_point()"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#fun-fact-2",
    "href": "teaching/03 Correlation and Regression.html#fun-fact-2",
    "title": "Correlation and Linear Regression",
    "section": "Fun fact:",
    "text": "Fun fact:\nThe median will minimize the sum of absolute error:\n\nWhat is the minimum amount I was wrong?\n\n\nmin(simulation.results.median$how_wrong)\n\n[1] 9.777342\n\n\n\nWhat was my guess at that point?\n\n\nbestguess.median &lt;- min(simulation.results.median$how_wrong)\nsimulation.results.median[simulation.results.median$how_wrong == bestguess.median,]\n\n       guess how_wrong\n1   10.13008  9.777342\n541 10.13092  9.777342"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-2",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-2",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nIf we did not know a shoe’s weight, what would be our best guess of sales?\n\nThe mean sales!\n\nmean(reiDataSmall$sales)\n\n[1] 10.16274"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-3",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-3",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nGraphically, that “guess” looks like this:\n\nggplot(reiDataSmall,\n       aes(x = 1, \n           y = sales)) +\n  geom_point() +\n  geom_hline(yintercept = mean(reiDataSmall$sales))"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-4",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-4",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nBut what if we have information about their weight?\n\n\nggplot(reiDataSmall,\n       aes(x = weight, \n           y = sales)) +\n  geom_point()"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-5",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-5",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nHow does our old guess look?\n\n\n\nggplot(reiDataSmall,\n       aes(x = weight, \n           y = sales)) +\n  geom_point() +\n  geom_hline(yintercept = mean(reiDataSmall$sales))"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-6",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-6",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nThat flat line is no longer our best guess, is it?\n\n\n\nIn this case, the mean is no longer the best possible guess\n\nThis is because there are large residuals between the line we guess and many points\n\nEspecially on the edges"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-7",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-7",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nInstead, what is our new best guess?\n\n\nIt’s the one that minimizes the sum of squared residuals, given that we now know weight\nWe can find it the same way we found the best guess without weight\nBut rather than guessing one number, we’ll guess that the slope of this line is:"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-8",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-8",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat is our new best guess?\n\nI’m going to test a bunch of different intercepts and slopes\n\n\nsims &lt;- 10000\nsimulation.results &lt;- data.frame(\n   guess.intercept = runif(n = sims,\n               min = 400,\n               max = 500),\n   guess.slope = runif(n = sims,\n               min = 0,\n               max = 15),\n   how_wrong2 = rep(NA, times = sims)\n)\n \nfor( i in 1:sims){\n   for( j in 1:nrow(reiDataSmall)){\n     reiDataSmall$how_wrong2[j] &lt;- ((simulation.results$guess.intercept[i] + simulation.results$guess.slope[i] * reiDataSmall$weight[j]) - reiDataSmall$sales[j])^2\n   }\n   simulation.results$how_wrong2[i] &lt;- sum(reiDataSmall$how_wrong2)\n }"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-9",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-9",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat is our new best guess?\n\nI’m going to test a bunch of different intercepts and slopes\nWhat gets us the lowest squared error?\n\n\nsimulation.results[simulation.results$how_wrong2 == min(simulation.results$how_wrong2),]\n\n     guess.intercept guess.slope how_wrong2\n3684        400.3522  0.09396755    3051150\n\n\nTo get the exact numbers\n\nUse lm() to find the coefficient for weight\n\n\nlm(data = reiDataSmall,\n   formula = sales ~ weight)\n\n\nCall:\nlm(formula = sales ~ weight, data = reiDataSmall)\n\nCoefficients:\n(Intercept)       weight  \n    10.9061      -0.1764"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-10",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-10",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nThat’s how we end up with this line:"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-11",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-11",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nThese best guesses are now different at each level of weight:\n\nweight.lm &lt;- lm(data = reiDataSmall,\n   formula = sales ~ weight)\nreiDataSmall$bestguess.lm &lt;- weight.lm$coef[\"(Intercept)\"] + \n  weight.lm$coef[\"weight\"]  * reiDataSmall$weight\nreiDataSmall$how_wrong2.lm &lt;- ( reiDataSmall$bestguess.lm - reiDataSmall$sales)^2\n\nreiDataSmall[, c('weight', 'bestguess.lm')]\n\n     weight bestguess.lm\n1  3.160536    10.348593\n2  4.970014    10.029385\n3  6.759033     9.713786\n4  3.068526    10.364824\n5  5.254373     9.979221\n6  4.769999    10.064669\n7  3.831546    10.230221\n8  3.805244    10.234861\n9  3.172416    10.346497\n10 2.901195    10.394343\n11 2.461316    10.471942\n12 2.737338    10.423249\n13 2.982250    10.380044\n14 5.910042     9.863555\n15 5.154571     9.996827\n16 6.090791     9.831670\n17 5.153109     9.997085\n18 4.059393    10.190026\n19 5.565751     9.924291\n20 2.473582    10.469778"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-12",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-12",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nAnd what are the residuals if we take weight into account?\n\nggplot(reiDataSmall,\n       aes(x = weight, \n           y = sales)) +\n  geom_point() +\n  geom_smooth(formula = 'y~x',\n              method = 'lm', \n              se = F, color = 'black') +\n  geom_segment(aes(xend = weight, yend = reiDataSmall$bestguess.lm), \n               color = \"red\",\n               linetype = 'dashed',\n               size = 1) +\n  geom_point()"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-13",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-13",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nAnd what are the residuals if we take weight into account?\n\n\nreiDataSmall[, c('weight', 'bestguess.lm', 'how_wrong2.lm')]\n\n     weight bestguess.lm how_wrong2.lm\n1  3.160536    10.348593  0.0780539703\n2  4.970014    10.029385  0.0104062769\n3  6.759033     9.713786  0.7348715017\n4  3.068526    10.364824  0.1749433569\n5  5.254373     9.979221  1.3695209324\n6  4.769999    10.064669  0.0306075101\n7  3.831546    10.230221  0.0229505609\n8  3.805244    10.234861  0.0112577147\n9  3.172416    10.346497  0.1227925882\n10 2.901195    10.394343  0.3235748427\n11 2.461316    10.471942  0.0095101693\n12 2.737338    10.423249  0.0003150111\n13 2.982250    10.380044  0.6333775757\n14 5.910042     9.863555  1.8295062522\n15 5.154571     9.996827  0.0229828344\n16 6.090791     9.831670  0.2257205210\n17 5.153109     9.997085  0.0195782530\n18 4.059393    10.190026  0.4574564464\n19 5.565751     9.924291  0.1970749260\n20 2.473582    10.469778  0.4654095167\n\n\n\nsum(reiDataSmall$how_wrong2.lm)\n\n[1] 6.739911"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-14",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-14",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nIs that smaller than before?\n\nSSE from mean:\n\nsum(reiDataSmall$how_wrong2)\n\n[1] 4761308\n\n\n\n\nSSE including weight:\n\nsum(reiDataSmall$how_wrong2.lm)\n\n[1] 6.739911"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#but-is-it-worth-drawing-that-line",
    "href": "teaching/03 Correlation and Regression.html#but-is-it-worth-drawing-that-line",
    "title": "Correlation and Linear Regression",
    "section": "But is it worth drawing that line?",
    "text": "But is it worth drawing that line?\n\n\nIs the reduction in squared error worth it?\nWe’re always going to reduce error when we add more things into a model\nBut we only want to add things that are really worth it\n\nOverfitting old data makes us likely to make worse predictions in the future\nComplexity makes our results hard to explain"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#but-is-it-worth-drawing-that-line-1",
    "href": "teaching/03 Correlation and Regression.html#but-is-it-worth-drawing-that-line-1",
    "title": "Correlation and Linear Regression",
    "section": "But is it worth drawing that line?",
    "text": "But is it worth drawing that line?\n\n\n\n\nggplot(reiDataSmall,\n       aes(x = weight, \n           y = sales)) +\n  geom_point() +\n  geom_hline(yintercept = mean(reiDataSmall$sales))\n\n\n\n\n\n\n\n\n\n\nggplot(reiDataSmall,\n       aes(x = weight, \n           y = sales)) +\n  geom_point() +\n  geom_smooth(formula = 'y~x',\n              method = 'lm', \n              se = F)"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#testing-linear-regression",
    "href": "teaching/03 Correlation and Regression.html#testing-linear-regression",
    "title": "Correlation and Linear Regression",
    "section": "Testing Linear Regression",
    "text": "Testing Linear Regression\nUse summary() around lm() to find the coefficient and p-value weight\n\nsummary(lm(data = reiDataSmall,\n   formula = sales ~ weight))\n\n\nCall:\nlm(formula = sales ~ weight, data = reiDataSmall)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.85725 -0.45172 -0.04418  0.21882  1.35259 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  10.9061     0.4642  23.496 5.87e-15 ***\nweight       -0.1764     0.1053  -1.676    0.111    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6119 on 18 degrees of freedom\nMultiple R-squared:  0.135, Adjusted R-squared:  0.08694 \nF-statistic: 2.809 on 1 and 18 DF,  p-value: 0.111\n\n\n\n\nWhat is the meaning of this p-value?\nIt’s the same as any p-value\n\n“What is the probability of seeing a slope this far (or more) from zero?”\n\nIf there was no true relationship between weight and sales"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-15",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-15",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat (I think) you should take away from this lesson if nothing else.\n\n\nAll linear regression is is answering the question:\n\n“If I know something about variable x, what is my best guess about the value of y?”\nSpecifically, what is the best line I can draw through my data\n\nAnd for the most part, “fancier” statistics essentially boil down to linear regression."
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nWe can use regression as just an extension of a t-test\n\n\nDoes cushion impact sales?\nLet’s simplify this by removing the “Medium” cushion group\n\n\nreiData &lt;- reiData[reiData$cushion != \"Medium\",]\n\nFirst, let’s plot this"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors-1",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors-1",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nDoes cushion impact sales?\nPlot!\n\n\nreiData |&gt;\n  group_by(cushion) |&gt;\n  summarise(sales = mean(sales, na.rm = T)) |&gt; # Not sure if we have missing data\n  ggplot(\n    aes(x = cushion,\n        y = sales)) +\n  geom_bar(stat = \"identity\")"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors-2",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors-2",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nWhat we are basically doing is comparing these plots:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd answering the question:\nIs it worth knowing that we can separate the data into these two groups?"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#group-predictors",
    "href": "teaching/03 Correlation and Regression.html#group-predictors",
    "title": "Correlation and Linear Regression",
    "section": "Group Predictors",
    "text": "Group Predictors\nSo let’s see!\nUsing the lm() function in R:\n\nlm( \n  data = reiData, \n  formula = sales ~ cushion ) |&gt; # The dv is sales, IV is \"cushion\"\n  summary() # Summarise it\n\n\nCall:\nlm(formula = sales ~ cushion, data = reiData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3956 -0.4987  0.0878  0.6200  3.1414 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.32820    0.04648 222.193   &lt;2e-16 ***\ncushionLow  -0.69344    0.07466  -9.288   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9583 on 692 degrees of freedom\nMultiple R-squared:  0.1108,    Adjusted R-squared:  0.1096 \nF-statistic: 86.26 on 1 and 692 DF,  p-value: &lt; 2.2e-16\n\n\n\nWhat do you notice about this result, compared to the one from the t-test?\n\n\n\nt.test( \n  reiData[reiData$cushion == 'Low', 'sales'], \n  reiData[reiData$cushion == 'High', 'sales']) \n\n\n    Welch Two Sample t-test\n\ndata:  reiData[reiData$cushion == \"Low\", \"sales\"] and reiData[reiData$cushion == \"High\", \"sales\"]\nt = -8.9923, df = 509.88, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.8449394 -0.5419355\nsample estimates:\nmean of x mean of y \n 9.634764 10.328201 \n\n\nIt’s the same thing!"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors-3",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors-3",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nWhy is the result the same thing?\n\nBecause a regression and a t-test are mostly the same thing\n\nTake some data\nCompare the difference in two groups to the difference that would happen from chance alone"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors-4",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors-4",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nLet’s extend this.\nRead the entire data back in, which has the third group\n\nreiData &lt;- read.csv('rei_products.csv')"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors-5",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors-5",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nI want to know:\n\nIf cushion is worth knowing for sales:\n\nIf the three levels differ from eachother"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#answer",
    "href": "teaching/03 Correlation and Regression.html#answer",
    "title": "Correlation and Linear Regression",
    "section": "Answer",
    "text": "Answer\n\nNull hypothesis: That there is no difference in sales between cushion levels.\n\nlm(\n  data = reiData, \n  sales ~ cushion) |&gt;\n  summary()\n\n\nCall:\nlm(formula = sales ~ cushion, data = reiData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3956 -0.4759  0.0694  0.5737  3.1414 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   10.32820    0.04280 241.313  &lt; 2e-16 ***\ncushionLow    -0.69344    0.06875 -10.087  &lt; 2e-16 ***\ncushionMedium -0.28409    0.06485  -4.381 1.31e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8823 on 1019 degrees of freedom\nMultiple R-squared:  0.09084,   Adjusted R-squared:  0.08905 \nF-statistic: 50.91 on 2 and 1019 DF,  p-value: &lt; 2.2e-16\n\nreiData |&gt;\n  group_by(cushion) |&gt;\n  summarise(sales = mean(sales, na.rm = T)) |&gt;\n  ggplot(\n    aes(x = cushion,\n        y = sales)) +\n  geom_bar(stat = 'identity', fill = c('seagreen', 'goldenrod', 'seagreen3')) +\n  theme_bw()\n\n\n\n\n\n\n\n\nConclusion: There is likely a difference between conditions, as the likelihood that a difference this large arises by chance alone is &lt; .001%."
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#answer-1",
    "href": "teaching/03 Correlation and Regression.html#answer-1",
    "title": "Correlation and Linear Regression",
    "section": "Answer",
    "text": "Answer\nTo make the plot less gross\n\nreiData$cushion &lt;- factor(reiData$cushion, levels = c(\"Low\", \"Medium\", \"High\"))\nlm(\n  data = reiData, \n  sales ~ cushion) |&gt;\n  summary()\n\n\nCall:\nlm(formula = sales ~ cushion, data = reiData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3956 -0.4759  0.0694  0.5737  3.1414 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    9.63476    0.05380  179.09  &lt; 2e-16 ***\ncushionMedium  0.40935    0.07258    5.64  2.2e-08 ***\ncushionHigh    0.69344    0.06875   10.09  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8823 on 1019 degrees of freedom\nMultiple R-squared:  0.09084,   Adjusted R-squared:  0.08905 \nF-statistic: 50.91 on 2 and 1019 DF,  p-value: &lt; 2.2e-16\n\nreiData |&gt;\n  group_by(cushion) |&gt;\n  summarise(sales = mean(sales, na.rm = T)) |&gt;\n  ggplot(\n    aes(x = cushion,\n        y = sales)) +\n  geom_bar(stat = 'identity', fill = c('seagreen', 'goldenrod', 'seagreen3')) +\n  theme_bw()"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors-6",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors-6",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nBut what if I want to know:\n\nIf it is worth knowing if cushion is “High” specifically\n\nVs. Low and Medium\nYou’ll have to construct a “dummy code”\nGoogle this\n\nTest with lm()\nPlot the condition means"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors-7",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors-7",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nAnswer\n\nNull hypothesis: That there is no difference in sales between cushion high and the combination of low and medium.\n\nreiData$cushionHigh &lt;- ifelse(reiData$cushion == \"High\", 1, 0)\n\nlm(\n  data = reiData, \n  sales ~ cushionHigh) |&gt;\n  summary()\n\n\nCall:\nlm(formula = sales ~ cushionHigh, data = reiData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6205 -0.4556  0.0827  0.5721  3.1414 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  9.85967    0.03665 268.998  &lt; 2e-16 ***\ncushionHigh  0.46853    0.05684   8.243 5.12e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8956 on 1020 degrees of freedom\nMultiple R-squared:  0.06246,   Adjusted R-squared:  0.06154 \nF-statistic: 67.95 on 1 and 1020 DF,  p-value: 5.116e-16\n\nreiData |&gt;\n  group_by(cushionHigh) |&gt;\n  summarise(sales = mean(sales, na.rm = T)) |&gt;\n  ggplot(\n    aes(x = cushionHigh,\n        y = sales)) +\n  geom_bar(stat = 'identity', fill = c('seagreen', 'goldenrod')) +\n  theme_bw()\n\n\n\n\n\n\n\n\nConclusion: There is likely a difference."
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors-8",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors-8",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nBut what if I want to know whether the relationship between the three conditions is linear?\n\ni.e., Low is worst, then medium, then high?\n\n\n\nWe can construct a contrast code\n\n\nreiData$cushionLinear &lt;- ifelse( reiData$cushion == \"Low\", -1,\n                                 ifelse(reiData$cushion == \"Medium\", 0, 1))\n\nsummary(lm(data = reiData, sales ~ cushionLinear))\n\n\nCall:\nlm(formula = sales ~ cushionLinear, data = reiData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4211 -0.4745  0.0694  0.5697  3.1253 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   10.00230    0.02809  356.11   &lt;2e-16 ***\ncushionLinear  0.34204    0.03408   10.04   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8824 on 1020 degrees of freedom\nMultiple R-squared:  0.08985,   Adjusted R-squared:  0.08896 \nF-statistic: 100.7 on 1 and 1020 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-controlling-for-variables",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-controlling-for-variables",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression: Controlling for variables",
    "text": "Linear Regression: Controlling for variables\nControlling for variables\nSometimes we think that some… other variable predicts our outcome, and we want to take its effect away.\n\ne.g., If we thought shoes sold more when they were high cushion, but we know that cushion effects weight\nTo “control” for this weight, we can add (+) it to our model:\n\n\nlm(\n  data = reiData, \n  formula = sales ~ cushion + weight) |&gt;\n  summary()\n\n\nCall:\nlm(formula = sales ~ cushion + weight, data = reiData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1702 -0.4305  0.0788  0.5436  3.3486 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    8.63808    0.11750  73.516   &lt;2e-16 ***\ncushionMedium  0.06406    0.07865   0.815    0.416    \ncushionHigh   -0.13318    0.10961  -1.215    0.225    \nweight         0.41007    0.04343   9.442   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8465 on 1018 degrees of freedom\nMultiple R-squared:  0.164, Adjusted R-squared:  0.1616 \nF-statistic: 66.59 on 3 and 1018 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#controlling-for-variables",
    "href": "teaching/03 Correlation and Regression.html#controlling-for-variables",
    "title": "Correlation and Linear Regression",
    "section": "Controlling for variables",
    "text": "Controlling for variables\nRules for controlling for variables:\nFirst:\n\nTest the effect of the control on the DV alone\n\n\nlm(\n  data = reiData, \n  formula = sales ~ weight) |&gt;\n  summary()\n\n\nCall:\nlm(formula = sales ~ weight, data = reiData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1616 -0.4220  0.0828  0.5490  3.2713 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.78927    0.09499   92.53   &lt;2e-16 ***\nweight       0.35751    0.02577   13.87   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8484 on 1020 degrees of freedom\nMultiple R-squared:  0.1587,    Adjusted R-squared:  0.1579 \nF-statistic: 192.4 on 1 and 1020 DF,  p-value: &lt; 2.2e-16\n\n\n\nIf it is meaningful, it’s worth controlling for\nIf the control does not predict the DV, it can’t affect our result"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#controlling-for-variables-1",
    "href": "teaching/03 Correlation and Regression.html#controlling-for-variables-1",
    "title": "Correlation and Linear Regression",
    "section": "Controlling for variables",
    "text": "Controlling for variables\nRules for controlling for variables:\nSecond:\n\nTest the effect of the IV on the control\n\n\nlm(\n  data = reiData, \n  weight ~ cushion) |&gt;\n  summary()\n\n\nCall:\nlm(formula = weight ~ cushion, data = reiData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0509 -0.3196 -0.0154  0.2716  3.0308 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    2.43052    0.03723   65.29   &lt;2e-16 ***\ncushionMedium  0.84201    0.05023   16.77   &lt;2e-16 ***\ncushionHigh    2.01580    0.04757   42.37   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6106 on 1019 degrees of freedom\nMultiple R-squared:  0.6494,    Adjusted R-squared:  0.6487 \nF-statistic: 943.7 on 2 and 1019 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#controlling-for-variables-2",
    "href": "teaching/03 Correlation and Regression.html#controlling-for-variables-2",
    "title": "Correlation and Linear Regression",
    "section": "Controlling for variables",
    "text": "Controlling for variables\nIf the control predicts the DV AND IV predicts control:\n\nWe say this “IV is confounded with the control variable”\n\nIt is harder to disentangle:\n\n“IV leads to DV” from\n“control leads to DV, and that is why it looks like IV leads to DV”\n\n\n\n\nIf the control predicts the DV BUT IV DOES NOT predict control:\n\nWe include the control, and it makes our IV appear more reliable\n\nIt reduces the amount of variance in the DV that our IV has to explain"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#controlling-for-variables-3",
    "href": "teaching/03 Correlation and Regression.html#controlling-for-variables-3",
    "title": "Correlation and Linear Regression",
    "section": "Controlling for variables",
    "text": "Controlling for variables\nTo know from this:\n\nWhat a “control variable” is\nWhat two steps we perform to decide whether to include a control\nWhat if the control predicts our DV?\nWhat if the IV also predicts our control?"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-wrap-up",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-wrap-up",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression Wrap Up",
    "text": "Linear Regression Wrap Up\n\n\nWe’re basically always drawing lines through points in statistics:\n\nLogistic regression:\n\nLines, but they have to be between 0 and 1\n\nLasso Regression (machine learning):\n\nLines, but a little more conservative\n\nRandom Forests (machine learning):\n\nLines, but lots of lines, and just to a single end point"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#correlation-1",
    "href": "teaching/03 Correlation and Regression.html#correlation-1",
    "title": "Correlation and Linear Regression",
    "section": "Correlation",
    "text": "Correlation\nAre these scatterplots showing strong relationships?\nTwo ways to know with numbers:\n\n\nRegression\n\nUsing lm() in R\nBueno\n\nCorrelations:\n\nUsing cor() in R"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#correlation-coefficients",
    "href": "teaching/03 Correlation and Regression.html#correlation-coefficients",
    "title": "Correlation and Linear Regression",
    "section": "Correlation Coefficients",
    "text": "Correlation Coefficients\nSpecifically, this is a Pearson correlation coefficient\n\nOften abbreviated with r.\nThis is a continuous metric between -1 and 1.\n\nPerfectly positive (x goes up, y goes up the same): +1\nPerfectly negative: -1\n\n\n\n\n\n\nplot(c(1:50), \n     c(51:100))\n\n\n\n\n\n\n\n\n\n\nplot(c(50:1), \n     c(51:100))"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#correlation-matrices",
    "href": "teaching/03 Correlation and Regression.html#correlation-matrices",
    "title": "Correlation and Linear Regression",
    "section": "Correlation Matrices",
    "text": "Correlation Matrices\nFor more than two variables, you can compute the correlations between all pairs x, y at once as a correlation matrix.\n\nLet’s look at one:\n\n\ncor(reiData[, c(\"sales\", \"price\", \"weight\")], use = 'pairwise.complete.obs') |&gt;\n  round(3)\n\n        sales  price weight\nsales   1.000 -0.132  0.398\nprice  -0.132  1.000  0.747\nweight  0.398  0.747  1.000"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#correlation-coefficients-1",
    "href": "teaching/03 Correlation and Regression.html#correlation-coefficients-1",
    "title": "Correlation and Linear Regression",
    "section": "Correlation Coefficients",
    "text": "Correlation Coefficients\nProblem with correlation:\n\nWe can’t make predictions with correlations\n\nWe can say “y increases as x increases” generally\n\nBut not “y increases this much as x increases this much”"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-vs-noise",
    "href": "teaching/bias_vs_noise.html#bias-vs-noise",
    "title": "Bias vs. Noise",
    "section": "Bias vs Noise",
    "text": "Bias vs Noise\nIn statistics, the terms bias and noise refer to very specific things.\n\nBias:\n\nSystematic errors that consistently push measurements or estimates away from the true value\nIt is predictable, and often stems from flaws in the measurement process or the sampling method.\nBias tends to push measurements or estimates in a particular direction, either overestimating or underestimating the true value.\n\n\n\nNoise:\n\nRandomness or fluctuations in data that can’t be attributed to a systematic cause\nUnpredictable. Can result from various sources, including measurement errors, and chance.\nNoise doesn’t consistently push data in one direction but adds random fluctuations around the true value."
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-vs-noise-1",
    "href": "teaching/bias_vs_noise.html#bias-vs-noise-1",
    "title": "Bias vs. Noise",
    "section": "Bias vs Noise",
    "text": "Bias vs Noise\nHow we handle each also differs:\n\n\nBias:\n\nUsually involves identifying and eliminating or reducing sources of systematic error.\nTechniques such as calibration, randomization, and careful study design can help mitigate bias.\n\nNoise:\n\nInherently random and cannot be eliminated entirely.\nTechniques like averaging, statistical tests, and increasing sample sizes reduce the impact of noise."
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-1",
    "href": "teaching/bias_vs_noise.html#noise-1",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\n“Why isn’t having a small sample size a source of bias?”\nRead in the data customerData.csv\n\ncustomerData &lt;- read.csv(\"customerData.csv\")\n\nLet’s treat this full data set of 1000 observations as the population – the entire set of people we are interested in.\nIn the population, what is the mean of income?\n\n\nmean(customerData$income)\n\n[1] 65476.08"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-2",
    "href": "teaching/bias_vs_noise.html#noise-2",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nImagine we did not know this number for the population\n\nWe could only estimate it by surveying a sample of people.\n\n\n\nTake a random subset of 10 observations by running this code:\n\n\nsample.size &lt;- 10\n\ncustomerDataSmall &lt;- customerData[\n  sample(x = 1:nrow(customerData),\n         size = sample.size,\n         replace = F), \n  ]"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-3",
    "href": "teaching/bias_vs_noise.html#noise-3",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nIf you did not know the population average of income, how would you estimate it with your survey sample?\n\n\nYou would see what the mean in the sample is!\n\n\nmean(customerDataSmall$income)\n\n[1] 67742.5\n\n\n\n\nThat sample estimate is going to be noisy\n\nIt’s going to vary from sample to sample around the population average\n\n“true mean”"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-4",
    "href": "teaching/bias_vs_noise.html#noise-4",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nThat sample estimate is going to be noisy\n\nIt’s going to vary from sample to sample around the population average.\nIf everyone in the class had their own sample (which you do), what might your different estimates look like?"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-5",
    "href": "teaching/bias_vs_noise.html#noise-5",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nAre these estimates biased?\n\n\nAre they more likely to be above or below the true mean?\n\n\n\n% above:\n\nsum( # Sum of the logical argument\n  sample.results$mean &gt; mean(customerData$income))/ # Is the sample mean &gt; than the pop?\n  nsims\n\n[1] 0.51\n\n\n\n\n% below:\n\nsum( # Sum of the logical argument\n  sample.results$mean &lt; mean(customerData$income))/ # Is the sample mean &gt; than the pop?\n  nsims\n\n[1] 0.49"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-6",
    "href": "teaching/bias_vs_noise.html#noise-6",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nWith a small sample, our results are not more likely to fall on one side of the true mean than the other.\n\nAs long as our data don’t have crazy outliers!!\n\nWhat is the benefit of larger samples, then?\n\n\nPrecision!\nHow far off were our estimates, on average?\n\n\nmean(abs(sample.results$mean - mean(customerData$income))) |&gt;\n  round(2)\n\n[1] 5357.99"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-7",
    "href": "teaching/bias_vs_noise.html#noise-7",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nLet’s try with samples of 20:\n\nsample.size &lt;- 20\n\ncustomerDataSmall &lt;- customerData[\n  sample(x = 1:nrow(customerData),\n         size = sample.size,\n         replace = F), \n  ]\n\nmean(customerDataSmall$income)\n\n[1] 70132.8"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-8",
    "href": "teaching/bias_vs_noise.html#noise-8",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nLet’s try with samples of 20:"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-9",
    "href": "teaching/bias_vs_noise.html#noise-9",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nLet’s try with samples of 20:\n\nHow far off was our estimate, on average?\n\n\nmean(abs(sample.results[sample.results$sample.size==20,'mean'] - mean(customerData$income))) |&gt;\n  round(2)\n\n[1] 3638.14"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-10",
    "href": "teaching/bias_vs_noise.html#noise-10",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nLet’s try with samples of 100:\n\nsample.size &lt;- 100\n\ncustomerDataSmall &lt;- customerData[\n  sample(x = 1:nrow(customerData),\n         size = sample.size,\n         replace = F), \n  ]\n\nmean(customerDataSmall$income)\n\n[1] 66418.28"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-11",
    "href": "teaching/bias_vs_noise.html#noise-11",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nLet’s try with samples of 100:"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-12",
    "href": "teaching/bias_vs_noise.html#noise-12",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nLet’s try with samples of 100:\n\nHow far off was our estimate, on average?\n\n\nmean(abs(sample.results[sample.results$sample.size==100,'mean'] - mean(customerData$income))) |&gt;\n  round(2)\n\n[1] 1570.13"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-13",
    "href": "teaching/bias_vs_noise.html#noise-13",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nConclusion:\n\nNoise is random error\nIt causes our estimates to bounce around the true/population mean\nMakes our estimates imprecise\nBut it doesn’t push them in one direction or another\n\nBias on the other hand…"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-1",
    "href": "teaching/bias_vs_noise.html#bias-1",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\n\n\nSystematic errors that consistently push measurements or estimates away from the true value\nIt is predictable, and often stems from flaws in the measurement process or the sampling method.\nBias tends to push measurements or estimates in a particular direction, either overestimating or underestimating the true value.\nIs not made better by increasing sample sizes"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-2",
    "href": "teaching/bias_vs_noise.html#bias-2",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nLet’s imagine that instead of estimating income with a random sample of 10/20/100 people, we sent out a survey, and got 10 responses.\n\nBut young people were more likely to respond\n\n\nsample.size &lt;- 10\n\nunder30 &lt;- customerData[customerData$age &lt;= 29,]\n\ncustomerDataSmall &lt;- under30[\n  sample(x = 1:nrow(under30),\n         size = sample.size,\n         replace = F), \n  ]"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-3",
    "href": "teaching/bias_vs_noise.html#bias-3",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nWhat is our estimate of income from these samples?"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-4",
    "href": "teaching/bias_vs_noise.html#bias-4",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nAre these estimates biased?\n\n\nAre they more likely to be above or below the true mean?\n\n\n\n% above:\n\nsum( # Sum of the logical argument\n  sample.results$mean &gt; mean(customerData$income))/ # Is the sample mean &gt; than the pop?\n  nrow(sample.results)\n\n[1] 0.071\n\n\n\n\n% below:\n\nsum( # Sum of the logical argument\n  sample.results$mean &lt; mean(customerData$income))/ # Is the sample mean &gt; than the pop?\n  nrow(sample.results)\n\n[1] 0.929"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-5",
    "href": "teaching/bias_vs_noise.html#bias-5",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nLet’s try with samples of 20:\n\nsample.size &lt;- 20\n\ncustomerDataSmall &lt;- under30[\n  sample(x = 1:nrow(under30),\n         size = sample.size,\n         replace = F), \n  ]\n\nmean(customerDataSmall$income)\n\n[1] 57041.2"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-6",
    "href": "teaching/bias_vs_noise.html#bias-6",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nLet’s try with samples of 20:"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-7",
    "href": "teaching/bias_vs_noise.html#bias-7",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nAre these estimates biased?\n\n\nAre they more likely to be above or below the true mean?\n\n\n\n% above:\n\nsum( # Sum of the logical argument\n  sample.results$mean &gt; mean(customerData$income))/ # Is the sample mean &gt; than the pop?\n  nrow(sample.results)\n\n[1] 0.042\n\n\n\n\n% below:\n\nsum( # Sum of the logical argument\n  sample.results$mean &lt; mean(customerData$income))/ # Is the sample mean &gt; than the pop?\n  nrow(sample.results)\n\n[1] 0.958"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-8",
    "href": "teaching/bias_vs_noise.html#bias-8",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nLet’s try with samples of 100:\n\nsample.size &lt;- 100\n\ncustomerDataSmall &lt;- under30[\n  sample(x = 1:nrow(under30),\n         size = sample.size,\n         replace = F), \n  ]\n\nmean(customerDataSmall$income)\n\n[1] 56206.99"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-9",
    "href": "teaching/bias_vs_noise.html#bias-9",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nLet’s try with samples of 100:"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-10",
    "href": "teaching/bias_vs_noise.html#bias-10",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nAre these estimates biased?\n\n\nAre they more likely to be above or below the true mean?\n\n\n\n% above:\n\nsum( # Sum of the logical argument\n  sample.results$mean &gt; mean(customerData$income))/ # Is the sample mean &gt; than the pop?\n  nrow(sample.results)\n\n[1] 0.028\n\n\n\n\n% below:\n\nsum( # Sum of the logical argument\n  sample.results$mean &lt; mean(customerData$income))/ # Is the sample mean &gt; than the pop?\n  nrow(sample.results)\n\n[1] 0.972"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-11",
    "href": "teaching/bias_vs_noise.html#bias-11",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nConclusion:\n\nBias is not random error\nIt causes our estimates to be higher or lower than the true/population mean\nMakes our estimates predictably wrong\nIt does not get better with sample size"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nMatt Meister\n",
    "section": "",
    "text": "Matt Meister\n\n\n(.com)\n\n\n\n\nI study how people learn about and evaluate their choices—especially through online information. My work combines large-scale scraped data with experiments to learn how and why people make the judgments and choices they do.\n\n\nI teach applied statistics, marketing analytics, and other marketing courses at both undergraduate and graduate levels. My classes emphasize real (messy) data, decision-making, and gaining comfort with difficult topics.\n\n\n\n  Email: mmeister@usfca.edu \n  GitHub \n  LinkedIn \n  CV \n\n\n\nYou are visitor #loading…\n\n\n*Rough estimate"
  },
  {
    "objectID": "posts/star_ratings_steves.html",
    "href": "posts/star_ratings_steves.html",
    "title": "Star ratings are a bad way to compare products (1)",
    "section": "",
    "text": "This is the first in a series about star ratings/online reviews. I plan to mainly talk about my own research. But who knows. Maybe we’ll get weird."
  },
  {
    "objectID": "posts/star_ratings_steves.html#footnotes",
    "href": "posts/star_ratings_steves.html#footnotes",
    "title": "Star ratings are a bad way to compare products (1)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNot always, of course; When I have to use the restroom, I don’t compare the time cost to the time I would lose by changing my pants. But many of the decisions we care about result from comparisons.↩︎\nIn America, this translates to “Barnes and Noble”.↩︎\nIf only temporarily in some cases.↩︎\nWhich might be more arid and empty than the moon↩︎\nA decent argument for cynicism↩︎\nBefore continuing, it is worth noting that at least two other papers provide compelling and well-specified evidence for the first condition and its impact. “Can Lower(ed) Expert Opinions Lead to Better Consumer Ratings?: The Case of Michelin Stars” by Xingyi Li and colleagues finds that losing Michelin stars has a positive effect on ratings for restaurants. “The Good, the Bad and the Picky: Consumer Heterogeneity and the Reversal of Product Ratings” by Tommaso Bondi, Michelangelo Rossi, and Ryan Stevens finds similarly that people are more critical of movies after they have been nominated for Oscars.↩︎\nSometimes these certifications are influenced by ratings (Superhost status is), sometimes they are not (Heather seems unswayed by the masses), and sometimes it is not clear (eBay’s Top Rated Sellers are a confusing example of this).↩︎\nThe specific assumption we make here is that nothing that influences ratings changes between platforms alongside Superhost status. Because Superhost status is entirely an Airbnb feature, this should be more defensible.↩︎\nAnd another paper, detailed in the next blog post↩︎"
  },
  {
    "objectID": "soap_box.html",
    "href": "soap_box.html",
    "title": "Blog",
    "section": "",
    "text": "Star ratings are a bad way to compare products (1)\n\n\nWe don’t know enough about what goes into other people’s ratings\n\n\n\n\n\nJun 3, 2024\n\n\nMatt Meister\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "teaching/04 Treatment Effects.html",
    "href": "teaching/04 Treatment Effects.html",
    "title": "05 Treatment Effects",
    "section": "",
    "text": "In this semester, I am going to ask you to do a fair bit of work before coming to class. This will make our class time shorter, more manageable, and hopefully less boring.\nI am also going to use this as an opportunity for you to directly earn grade points for your effort/labor, rather than “getting things right” on an exam.\nTherefore, I will ask you to work through the posted slides on Canvas before class. Throughout the slides, I will post Pre-Class Questions for you to work through in R. These will look like this:\n\nIn R, please write code that will read in the .csv from Canvas called sf_listings_2312.csv. Assign this the name bnb.\nYou will then write your answer in a .r script:\n\n\nClick to show code and output\n\n\n\n# Q1\n#bnb &lt;- read.csv(\"sf_listings_2312.csv\")\n\n\n\n\n\nTo earn full points, you need to organize your code correctly. Specifically, you need to:\n\nAnswer questions in order.\n\nIf you answer them out of order, just re-arrange the code after.\n\nPreface each answer with a comment (# Q1/# Q2/# Q3) that indicates exactly which question you are answering.\n\nPlease just write the letter Q and the number in this comment.\n\nMake sure your code runs on its own, on anyone’s computer.\n\nTo do this, I would always include rm(list = ls()) at the top of every .r script. This will clean everything from the environment, allowing you to see if this runs on my computer.\n\n\n\n\n\n\nYou must submit this to Canvas before 9:00am on the day of class. Even if class starts at 10:00am that day, these are always due at 9:00.\nYou must submit this code as a .txt file. This is because Canvas cannot present .R files to me in SpeedGrader. To save as .txt:\n\nClick File -&gt; New File -&gt; Text File\nCopy and paste your completed code to that new text file.\nSave the file as firstname_lastname_module.txt\n\nFor example, my file for Module 01 would be matt_meister_01.txt\nMy file for module 05 would be matt_meister_05.txt\n\n\n\n\n\n\n\nI will grade these for completion.\nYou will receive 1 point for every question you give an honest attempt to answer\nYour grade will be the number of questions you answer, divided by the total number of questions.\n\nThis is why it is important that you number each answer with # Q1.\nAny questions that are not numbered this way will be graded incomplete, because I can’t find them.\n\nYou will receive a 25% penalty for submitting these late.\nI will post my solutions after class."
  },
  {
    "objectID": "teaching/04 Treatment Effects.html#pre-class-code-assignment-instructions",
    "href": "teaching/04 Treatment Effects.html#pre-class-code-assignment-instructions",
    "title": "05 Treatment Effects",
    "section": "",
    "text": "In this semester, I am going to ask you to do a fair bit of work before coming to class. This will make our class time shorter, more manageable, and hopefully less boring.\nI am also going to use this as an opportunity for you to directly earn grade points for your effort/labor, rather than “getting things right” on an exam.\nTherefore, I will ask you to work through the posted slides on Canvas before class. Throughout the slides, I will post Pre-Class Questions for you to work through in R. These will look like this:\n\nIn R, please write code that will read in the .csv from Canvas called sf_listings_2312.csv. Assign this the name bnb.\nYou will then write your answer in a .r script:\n\n\nClick to show code and output\n\n\n\n# Q1\n#bnb &lt;- read.csv(\"sf_listings_2312.csv\")\n\n\n\n\n\nTo earn full points, you need to organize your code correctly. Specifically, you need to:\n\nAnswer questions in order.\n\nIf you answer them out of order, just re-arrange the code after.\n\nPreface each answer with a comment (# Q1/# Q2/# Q3) that indicates exactly which question you are answering.\n\nPlease just write the letter Q and the number in this comment.\n\nMake sure your code runs on its own, on anyone’s computer.\n\nTo do this, I would always include rm(list = ls()) at the top of every .r script. This will clean everything from the environment, allowing you to see if this runs on my computer.\n\n\n\n\n\n\nYou must submit this to Canvas before 9:00am on the day of class. Even if class starts at 10:00am that day, these are always due at 9:00.\nYou must submit this code as a .txt file. This is because Canvas cannot present .R files to me in SpeedGrader. To save as .txt:\n\nClick File -&gt; New File -&gt; Text File\nCopy and paste your completed code to that new text file.\nSave the file as firstname_lastname_module.txt\n\nFor example, my file for Module 01 would be matt_meister_01.txt\nMy file for module 05 would be matt_meister_05.txt\n\n\n\n\n\n\n\nI will grade these for completion.\nYou will receive 1 point for every question you give an honest attempt to answer\nYour grade will be the number of questions you answer, divided by the total number of questions.\n\nThis is why it is important that you number each answer with # Q1.\nAny questions that are not numbered this way will be graded incomplete, because I can’t find them.\n\nYou will receive a 25% penalty for submitting these late.\nI will post my solutions after class."
  },
  {
    "objectID": "teaching/04 Treatment Effects.html#setup",
    "href": "teaching/04 Treatment Effects.html#setup",
    "title": "05 Treatment Effects",
    "section": "Setup",
    "text": "Setup\nLoad in these packages. If you do not have them, you will need to install them.\n\ne.g., install.packages(\"fixest\")\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(fixest)\n\nRead in cola_data.csv from Canvas, and assign it the name DF:\n\n\nClick to show code and output\n\n\n\nDF &lt;- read.csv(\"cola_data.csv\")"
  },
  {
    "objectID": "teaching/04 Treatment Effects.html#causality",
    "href": "teaching/04 Treatment Effects.html#causality",
    "title": "05 Treatment Effects",
    "section": "Causality",
    "text": "Causality\nWe are often attempting to test some causal relationship with our data. Specifically, we want to know if the factor we care about is causing a change in the outcome that matters to us. We can visualize this like this:\n\nThe Potential Outcomes Model\n\n\n\n\n\n\n\nExample 1: Causal effect of getting a college degree on earnings"
  },
  {
    "objectID": "teaching/04 Treatment Effects.html#example-2-causal-effect-of-receiving-a-target-coupon-on-purchasing",
    "href": "teaching/04 Treatment Effects.html#example-2-causal-effect-of-receiving-a-target-coupon-on-purchasing",
    "title": "05 Treatment Effects",
    "section": "Example 2: Causal effect of receiving a Target coupon on purchasing",
    "text": "Example 2: Causal effect of receiving a Target coupon on purchasing\n\n\n\n\n\nThis causal effect (also called a treatment effect) of the treatment Wi on individual i can be written as:\n\\(Y_i(W_i = 1) - Y_i(W_i = 0)\\)\nAll this estimates is the difference in earnings (\\(Y\\)) if individual (\\(i\\)) went to college (\\(W = 1\\)) vs. did not go to college (\\(W = 0\\)). Or the difference in purchase rate (\\(Y\\)) if individual (\\(i\\)) received a coupon (\\(W = 1\\)) vs. did not (\\(W = 0\\))\nIn the absolute best case, we would have a model of parallel worlds, where the treatment is the only (!) difference across the two worlds. Unfortunately, we only have access to a single world… for now.\n\nPotential outcomes vs. actually observed data\nIn any data we only observe the realized outcome for each individual:\n\\(Y_i(W_i = 1)\\) OR \\(Y_i(W_i = 0)\\)\nThis means we cannot directly estimate the individual causal effect (treatment effect) for each individual person. This is because each person only has a single outcome at a single time. Because… again… one universe for now.\nInstead of estimating individual treatment effects, we must estimate the average treatment effect in the population:\n\\(ATE = Avg [Y_i(W_i = 1) - Y_i(W_i = 0)]\\)\nThis is the average difference in outcomes between groups. The average difference in earnings for individuals who went to college vs. those who did not go to college, and the average difference in purchase rate when receiving vs. not receiving coupon. Unfortunately, this introduces the potential for “confounds”.\n\n\nConfounds\nIf you have ever heard the saying that “correlation does not imply causation”, then you have heard about confounds. These are “other explanations” that may cause the relationships we observe. In the examples above:\n\nIndividuals who go to college may have better academic skills and a higher earnings potential than those who do not go to college\nSuppose coupon is targeted to individuals who buy the product at the current purchase occasion.\nIndividuals with a coupon will generally be more likely to buy in future than others.\n\nIn general, this means that we cannot just subtract one group from the other. Treated individuals may be selected in systematic ways, and we only observe one outcome per person. So we can’t just say that treatment causes an outcome… generally.\nBut all hope is not lost, and we do have some solutions. We will cover two this week:\n\nRandomization + experiments\n\n“lab” conditions — fully randomized, controlled experiments\n“field” conditions — partially randomized, controlled experiments\n\nModel based predictions\n\nIdea: Use regression models to predict outcome of treated unit in control condition\nRequires explicit assumptions on omitted factors\nUsed with observational data, field experiments\n\n\nIn class we will cover randomization and experiments. But first, we will talk about model-based predictions below."
  },
  {
    "objectID": "teaching/04 Treatment Effects.html#to-begin",
    "href": "teaching/04 Treatment Effects.html#to-begin",
    "title": "05 Treatment Effects",
    "section": "To begin",
    "text": "To begin\nThese data are observations of cola sales at a series of stores over a series of months. Sometimes, this cola was on promotion. We are going to use this data to analyze the effect of price and promotion on sales. Our goal is to understand these effects so that we can design some pricing policy for our stores.\nFirst, check how many stores and months we have here.\n\n\nClick to show code and output\n\n\n\nlength(unique(DF$store))\n\n[1] 100\n\nlength(unique(DF$month))\n\n[1] 12\n\n\n\n\n\nPre-Class Q1\n\nGenerate a scatterplot of sales vs. price. Then, in a comment discuss whether you think that there is a relationship between price and sales.\n\n\nClick to show code and output\n\n\n\nggplot(data = DF, aes(x = price, y = sales)) +\n  geom_point(alpha = .8, size = 2)\n\n\n\n\n\n\n\n#Possible relationship, but weak if at all\n\n\n\n\nPre-Class Q2\n\nNow, estimate a linear regression of sales as a function of price and promotion. In a comment, interpret the coefficients.\n\n\nClick to show code and output\n\n\n\nsummary(lm(sales ~ price + promo, data=DF))\n\n\nCall:\nlm(formula = sales ~ price + promo, data = DF)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-63.739 -14.311  -0.635  14.518  76.703 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 112.5135     2.7918  40.301  &lt; 2e-16 ***\nprice         2.8881     0.6163   4.686 3.10e-06 ***\npromo        10.9676     1.6460   6.663 4.07e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.26 on 1197 degrees of freedom\nMultiple R-squared:  0.05993,   Adjusted R-squared:  0.05836 \nF-statistic: 38.16 on 2 and 1197 DF,  p-value: &lt; 2.2e-16\n\n#   1 dollar price increase leads to +2.88 units sold\n#   Promotion month leads to +10.97 units sold\n\n\n\nThe estimated price coefficient is positive. This means that an increase in price will lead to more cola sold. That does not make much basic economic sense, since people shouldn’t buy things more if price increases. Let’s investigate this a bit.\nWhat we are worried about here is called Omitted Variable Bias. Omitted variable bias happens when some variable we do not include in our model is biasing our results. This is worrisome because we care about making causal claims—i.e., price increases cause people to buy more/less.\nOne way we have tried to reconcile omitted variable bias in the past is by controlling for things in our models. When we have these other variables, we can remove their impact directly. However, these data do not have many control variables. For example, if we worry that demand for other items at a store might increase demand for cola, we cannot directly control for that here–we don’t have “demand.”\nLuckily, the data we have here is well-suited to addressing potential omitted variable bias through a second path. That path is by looking at Within-person/Within-unit variation in sales. We can do this whenever we have panel data, which is data that has repeated observations of the same units (i.e., stores) over time. This is also commonly called “cross-sectional” data, because we have a cross-section of stores, observed over many time periods.\nThe fact that we have repeated observations on both month and store gives us strong controls for omitted variables. We will do this through store fixed effects and time trends/fixed effects.\n\nOmitted Variable Bias\nHow does adding fixed effects/trends help with omitted variables? The variation absorbed by these parameters (controls) no longer enters the error in our regressions. This means that far fewer things are left to be potentially correlated with price. The “stronger” the set of controls (more parameters), the less concern for bias. We will build up to this step-by-step.\nFirst, let’s estimate a linear regression of sales as a function of price, promotion and month. This would remove omitted variable bias if our omitted variable was something that correlated with both sales and month, and did so linearly. For example, inflation goes up every month, meaning that price has a slightly different meaning each month.\n\n\nClick to show code and output\n\n\n\nsummary(lm(sales~price + promo + month, data=DF))\n\n\nCall:\nlm(formula = sales ~ price + promo + month, data = DF)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-71.152 -13.982   0.184  14.515  67.791 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 103.2991     2.8654  36.050  &lt; 2e-16 ***\nprice         2.5903     0.5956   4.349 1.49e-05 ***\npromo        10.9283     1.5885   6.880 9.64e-12 ***\nmonth         1.6230     0.1718   9.446  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.52 on 1196 degrees of freedom\nMultiple R-squared:  0.1252,    Adjusted R-squared:  0.123 \nF-statistic: 57.06 on 3 and 1196 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nFrom this, we see that each month, sales are increasing by 1.62 units from the prior month. This is linear growth—because it comes from a linear regression—and hence called a linear time trend.\n\nPre-Class Q3\n\nIn a comment, answer whether there is statistical evidence of a linear time trend in sales.\n\n\nClick to show code and output\n\n\n\n#Yes, the coefficient on month is highly significant (and positive).\n\n\n\n\nPre-Class Q4\n\nIn a comment, answer:\nHas the inclusion of a linear time trend “fixed” the sign of the price coefficient? What does this mean?\n\n\nClick to show code and output\n\n\n\n#No, price still has a positive effect on demand.\n#This *suggests* we need further controls for omitted variables (e.g., time fixed effects).\n\n\n\nWhat if the omitted variable is correlated with both sales and month, but not linearly? The previous regression handles omitted variable bias if the omitted variable follows the calendar (e.g., 1 &gt; 2 &gt; 3…). But it may not. For example, if our stores were in vacation destinations, we should see higher sales in the summer and winter, and lower in spring/fall.\nWith a fixed-effect for month, we can control for this non-linear relationship. This effectively removes variation across each individual month, rather than calculating a trend. We will use feols() from the fixest package to calculate this.\n\n\nClick to show code and output\n\n\n\nsummary(feols(sales ~ price + promo | month, data = DF))\n\nOLS estimation, Dep. Var.: sales\nObservations: 1,200\nFixed-effects: month: 12\nStandard-errors: Clustered (month) \n      Estimate Std. Error  t value   Pr(&gt;|t|)    \nprice  2.59026   0.497769  5.20373 2.9278e-04 ***\npromo 10.98585   1.083242 10.14164 6.4225e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 20.4     Adj. R2: 0.122741\n             Within R2: 0.060409\n\n\n\n\n\nPre-Class Q5\n\nIn a comment, answer:\nWhat do you notice about the coefficients for price and promotion in this model, compared to the others?\n\n\nClick to show code and output\n\n\n\n#Price still has a positive effect on demand.\n#This *suggests* we need further controls for omitted variables (e.g., store-specific fixed effects).\n\n\n\n\nPre-Class Q6\n\nNow, let’s try to remove between-store variation with a store fixed effect.\n\n\nClick to show code and output\n\n\n\nsummary(feols(sales ~ price + promo | month + store, data = DF))\n\nOLS estimation, Dep. Var.: sales\nObservations: 1,200\nFixed-effects: month: 12,  store: 100\nStandard-errors: Clustered (month) \n      Estimate Std. Error  t value   Pr(&gt;|t|)    \nprice -28.1558    4.90899 -5.73556 1.3113e-04 ***\npromo  11.5085    1.21824  9.44684 1.3015e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 19.0     Adj. R2: 0.173069\n             Within R2: 0.081193\n\n\n\n\n\nPre-Class Q7\n\nIn a comment, answer whether and why this changed our result.\n\n\nClick to show code and output\n\n\n\n#This changes our result a lot. This must be because different stores lead to different sales and prices."
  },
  {
    "objectID": "teaching/04 Treatment Effects.html#summary",
    "href": "teaching/04 Treatment Effects.html#summary",
    "title": "05 Treatment Effects",
    "section": "Summary",
    "text": "Summary\n\nWhen we did not control for store fixed effects, we got a surprising result\nThe simplest model we ran showed what we think is the wrong result\nSo we dug deeper\n\n“Is this result due to something we can’t observe?”\n\nRemoving month variation did not fix the issue"
  },
  {
    "objectID": "teaching/04 Treatment Effects.html#panel-data-how-does-it-work",
    "href": "teaching/04 Treatment Effects.html#panel-data-how-does-it-work",
    "title": "05 Treatment Effects",
    "section": "Panel Data… How does it work?",
    "text": "Panel Data… How does it work?\n\nWithin-Unit Transformation\nfeols() does not use dummy variables to estimate fixed effects. Instead, it transforms the data to “eliminate” the fixed effect, then runs a regular linear regression.\nRather than sales ~ price + store:\n\nWe subtract the average of sales for each store from each observation.\nAnd we subtract the average of price for each store from each observation.\n\nThis gives us De-meaned y regressed on de-meaned X. This means there is NO intercept in this regression (it is absorbed by fixed effects).\n\nImplications of this transformation:\nAll time-constant factors entering X and Y are removed from the regression. Including all unobserved time-constant omitted variables. This leads to a major reduction in omitted variable bias, as we are only left with time-varying factors. The nice thing is that these fixed effects mean we don’t even need to know what the omitted variables could be, let alone have data for them."
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#this-module",
    "href": "teaching/Logistic Regression Slides.html#this-module",
    "title": "Logistic Regression",
    "section": "This Module",
    "text": "This Module\nThus far, we’ve talked about “linear” regression.\n\n\nWhat made it linear?\n\nDrawing a line through points"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#this-module-1",
    "href": "teaching/Logistic Regression Slides.html#this-module-1",
    "title": "Logistic Regression",
    "section": "This Module",
    "text": "This Module\n\nToday, we will talk about “logistic” regression.\n\n\n\nDependent variable can either be 1 or 0\n\nCoefficients are different"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#why-would-we-draw-that",
    "href": "teaching/Logistic Regression Slides.html#why-would-we-draw-that",
    "title": "Logistic Regression",
    "section": "Why Would We Draw That??",
    "text": "Why Would We Draw That??"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#why-would-we-draw-that-1",
    "href": "teaching/Logistic Regression Slides.html#why-would-we-draw-that-1",
    "title": "Logistic Regression",
    "section": "Why Would We Draw That??",
    "text": "Why Would We Draw That??"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#after-this-module",
    "href": "teaching/Logistic Regression Slides.html#after-this-module",
    "title": "Logistic Regression",
    "section": "After This Module",
    "text": "After This Module\nYou’ll be able to perform, interpret, and communicate:\n\n\nLogistic regression\n\nRegression where the DV can be either 1 or 0\nInterpreting coefficients is a little less fluent than with linear regression\n\nInteractions in logistic regression\n\nWhether and when the effect of one variable depends on the level of another"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#run-this-code",
    "href": "teaching/Logistic Regression Slides.html#run-this-code",
    "title": "Logistic Regression",
    "section": "Run this code",
    "text": "Run this code\n\nrm(list = ls())\n# List of packages\npkgs &lt;- c('dplyr', 'ggplot2')\n\n# Check for packages that are not installed\nnew.pkgs &lt;- pkgs[!(pkgs %in% installed.packages()[,\"Package\"])]\n\n# Install the ones we need\nif(length(new.pkgs)) install.packages(new.pkgs)\n\n#Load them all in\nlapply(pkgs, library, character.only = TRUE)\n\n# Remove lists\nrm(pkgs, new.pkgs)\n\nage.data &lt;- data.frame(\n  age = c(27,30,32,33,35,40,44,45,50,58,59,60),\n  buyer = c(0,0,1,0,0,1,0,1,1,1,1,1))"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#logistic-regression",
    "href": "teaching/Logistic Regression Slides.html#logistic-regression",
    "title": "Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n\nA type of regression analysis used to predict the probability of a binary outcome\n\nLinear regression predicts the specific value of a continuous outcome\n\nOutcome can be one of two categories:\n\nyes/no\ntrue/false\n0/1\nOver 65/under 65\n\nLogistic regression models the relationship between one or more predictor variables (features), and the probability of the binary outcome occurring."
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#logistic-regression-when",
    "href": "teaching/Logistic Regression Slides.html#logistic-regression-when",
    "title": "Logistic Regression",
    "section": "Logistic Regression: When?",
    "text": "Logistic Regression: When?\n\n\n\nWhen we’re dealing with categorical dependent variables\n\nWhere the outcome falls into discrete categories\n\nExamples?\n\nMedical Diagnosis\n\nDetermining whether a patient has a specific disease based on various medical tests and patient characteristics.\n\nPurchase likelihood\n\nPredicting whether a customer will make a purchase based on their browsing history, demographics, and other factors\n\nCredit Scoring\n\nAssessing the likelihood of a loan default based on credit scores, income, and other financial indicators."
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#differences-between-logistic-and-linear-regression",
    "href": "teaching/Logistic Regression Slides.html#differences-between-logistic-and-linear-regression",
    "title": "Logistic Regression",
    "section": "Differences Between Logistic and Linear Regression",
    "text": "Differences Between Logistic and Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear regression can predict less than 0 and greater than 1\n\nLogistic regression can only predict between 0 and 1\n\nLinear regression will usually make less certain predictions\n\nLess likely to be almost 0 or almost 1\n\nCoefficients are interpreted differently\n\nUnit changes (linear) vs odds ratio (logistic)\n\nModels are estimated differently\n\nLeast squares (linear) vs maximum likelihood (logistic)"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#difference-1-predictions-are-between-0-and-1",
    "href": "teaching/Logistic Regression Slides.html#difference-1-predictions-are-between-0-and-1",
    "title": "Logistic Regression",
    "section": "Difference 1: Predictions are Between 0 and 1",
    "text": "Difference 1: Predictions are Between 0 and 1\n\n\nWhat is the prediction for someone 60 years old?\n\n\n\n\n\n\n\n\n\n\n\n\n\nlm(data = age.data,\n   formula = buyer ~ age) %&gt;%\n  predict(newdata = \n            data.frame(age = 60))\n\n      1 \n1.08386 \n\n\n\n\n\n\n\n\n\n\n\n\n\nglm(data = age.data,\n   formula = buyer ~ age,\n   family = 'binomial') %&gt;%\n  predict(newdata = \n            data.frame(age = 60),\n          type = \"response\")\n\n        1 \n0.9855778 \n\n\n\n\n\n\nIn this case, you’ll hear the linear regression called a “linear probability model”\n\nIt says there is a 108.4% chance that someone 60 years old is a buyer\n\nLogistic regression can’t go above 1 or below 0"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#difference-1-predictions-are-between-0-and-1-1",
    "href": "teaching/Logistic Regression Slides.html#difference-1-predictions-are-between-0-and-1-1",
    "title": "Logistic Regression",
    "section": "Difference 1: Predictions are Between 0 and 1",
    "text": "Difference 1: Predictions are Between 0 and 1\n\n\nWhat is the prediction for someone 20 years old?\n\n\n\n\n\n\n\n\n\n\n\n\n\nlm(data = age.data,\n   formula = buyer ~ age) %&gt;%\n  predict(newdata = \n            data.frame(age = 20))\n\n          1 \n-0.07678176 \n\n\n\n\n\n\n\n\n\n\n\n\n\nglm(data = age.data,\n   formula = buyer ~ age,\n   family = 'binomial') %&gt;%\n  predict(newdata = \n            data.frame(age = 20),\n          type = \"response\")\n\n         1 \n0.02532202"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#difference-2-linear-regression-makes-less-certain-predicitons-between-0-and-1",
    "href": "teaching/Logistic Regression Slides.html#difference-2-linear-regression-makes-less-certain-predicitons-between-0-and-1",
    "title": "Logistic Regression",
    "section": "Difference 2: Linear Regression Makes Less Certain Predicitons (Between 0 and 1)",
    "text": "Difference 2: Linear Regression Makes Less Certain Predicitons (Between 0 and 1)\n\n\nBetween 35 and 50, which line is farther from 50%?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBecause logistic regression can bend, it is closer to 0 or 1\n\nWhen it is between 0 and 1\n\nTherefore, logistic regression makes more certain predictions (between 0 and 1)"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#difference-3-coefficients-are-interpreted-differently",
    "href": "teaching/Logistic Regression Slides.html#difference-3-coefficients-are-interpreted-differently",
    "title": "Logistic Regression",
    "section": "Difference 3: Coefficients Are Interpreted Differently",
    "text": "Difference 3: Coefficients Are Interpreted Differently\n\n\n\n\nlm(data = age.data,\n   formula = buyer ~ age) %&gt;%\n  summary() %&gt;% #Summarize the model\n  coef() %&gt;% # Take just coefficients\n  round(4) # Round to 4 decimal places\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  -0.6571     0.4530 -1.4504   0.1776\nage           0.0290     0.0102  2.8327   0.0178\n\n\n\n\nLiner coefficients are simpler\n\n\\(p = -.6571 + .029 \\times age\\)\nWhere:\n\np: Probability of being a buyer\n\n\nOne year increase in age?\n\n2.9% increase in likelihood someone is a buyer\n\n\n\n\n\nglm(data = age.data, \n   formula = buyer ~ age,\n   family = 'binomial') %&gt;%\n  summary() %&gt;% #Summarize the model\n  coef() %&gt;% # Take just coefficients\n  round(4) # Round to 4 decimal places\n\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  -7.5879     4.2767 -1.7742   0.0760\nage           0.1969     0.1099  1.7913   0.0732\n\n\n\n\nLogistic regression coefficients are more complex\nOne year increase in age?\n\n0.1969 increase in the log-odds of being a buyer\n\\(\\log \\left( \\frac{p}{1-p} \\right) = -7.5879 + .1969 \\times age\\)\n\nWhere:\n\np: Probability of being a buyer"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#how-log-odds-relate-to-probabilities",
    "href": "teaching/Logistic Regression Slides.html#how-log-odds-relate-to-probabilities",
    "title": "Logistic Regression",
    "section": "How log-odds relate to probabilities",
    "text": "How log-odds relate to probabilities\n\n\nThis is not-examable\nLog-odds are a way to express probabilities in a logarithmic scale\n\nThis allows us to get a result that is always between 0 and 1\n\n\n\n\nProbability (p):\n\nThe likelihood of an event occurring, ranges from 0 (impossible) to 1 (certain).\n\nOdds (O):\n\nThe odds of an event happening is the ratio of the probability of the event occurring to the probability of the event not occurring.\n\\(O = \\frac{p}{1-p}\\)\nOdds can range from 0 to positive infinity.\n\nLog-Odds (Logit): The log-odds (also known as the logit) is the natural logarithm of the odds. It’s mathematically represented as\n\n\\(\\text{log}(\\frac{p}{1-p})\\)\nLog-odds can range from negative infinity to positive infinity."
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#how-log-odds-relate-to-probabilities-1",
    "href": "teaching/Logistic Regression Slides.html#how-log-odds-relate-to-probabilities-1",
    "title": "Logistic Regression",
    "section": "How log-odds relate to probabilities",
    "text": "How log-odds relate to probabilities\n\n\n\nBack to Probabilities\n\nThe log-odds transform the probability scale (0 to 1) to a scale that spans the entire real number line (negative infinity to positive infinity).\nAdvantages:\n\nLinearity\n\nIn logistic regression, the relationship between predictor variables and log-odds is assumed to be linear.\nThis linear relationship makes it easier to model and analyze relationships between predictors and the outcome.\n\nInterpretability\n\nA one-unit change in a predictor variable leads to a constant change in log-odds, regardless of the initial probability level.\n\nSymmetry\n\nThe log-odds are symmetric around 0.\nThis symmetry simplifies the mathematics in logistic regression."
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#drawbacks-of-log-odds",
    "href": "teaching/Logistic Regression Slides.html#drawbacks-of-log-odds",
    "title": "Logistic Regression",
    "section": "Drawbacks of Log-Odds?",
    "text": "Drawbacks of Log-Odds?\n\nWTF WAS THAT???"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#difference-3-coefficients-are-interpreted-differently-1",
    "href": "teaching/Logistic Regression Slides.html#difference-3-coefficients-are-interpreted-differently-1",
    "title": "Logistic Regression",
    "section": "Difference 3: Coefficients Are Interpreted Differently",
    "text": "Difference 3: Coefficients Are Interpreted Differently\nWhat you need to know about coefficients:\n\n\nLogistic regression coefficients are not the same as linear ones\n\nThey relate to some complicated math\nThat math makes the regression itself possible\nBut it makes the results hard to interpret\n\nMy recommendation is to:\n\nPlot your data\nLook at the z-value in your regression results\n\nLike a t-value\n\nUse the predict function to understand specific predictions\n\n\n\n\n\nglm(data = age.data,\n   formula = buyer ~ age,\n   family = 'binomial') %&gt;%\n  predict(newdata = \n            data.frame(age = 20),\n          type = \"response\")\n\n         1 \n0.02532202"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#differences-between-logistic-and-linear-regression-1",
    "href": "teaching/Logistic Regression Slides.html#differences-between-logistic-and-linear-regression-1",
    "title": "Logistic Regression",
    "section": "Differences Between Logistic and Linear Regression",
    "text": "Differences Between Logistic and Linear Regression\n\n\nLinear regression can predict less than 0 and greater than 1\n\nLogistic regression can only predict between 0 and 1\n\nLinear regression will usually make less certain predictions\n\nLess likely to be almost 0 or almost 1\n\nCoefficients are interpreted differently\n\nUnit changes (linear) vs odds ratio (logistic)\n\nModels are estimated differently\n\nLeast squares (linear) vs maximum likelihood (logistic)"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#my-take-on-logistic-regression",
    "href": "teaching/Logistic Regression Slides.html#my-take-on-logistic-regression",
    "title": "Logistic Regression",
    "section": "My Take on Logistic Regression",
    "text": "My Take on Logistic Regression\n\n\nLogistic regression is great for prediction\n\nPrecise between 0 and 1\nCan’t give you an impossible number\nMLE is very flexible\n\nNot prone to errors we’ll talk about later\n\n\nBut interpretation/communication?\n\nNot so much\n\nI think linear probability models are often good enough\nI am not expecting you to know it in great depth this semester\n\nLet’s just get some practice, some comfort\nNext semester we’ll focus on it more"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough",
    "href": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough",
    "title": "Logistic Regression",
    "section": "Linear Probability Models are Often Good Enough",
    "text": "Linear Probability Models are Often Good Enough\nHow do we interpret each result?\n\n\n\n\n\n\n\n\n\n\n\n\nlm(data = age.data,\n   formula = buyer ~ age) %&gt;%\n  summary() %&gt;%\n  coef() %&gt;%\n  round(4)\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  -0.6571     0.4530 -1.4504   0.1776\nage           0.0290     0.0102  2.8327   0.0178\n\n\n\n\n\n\n\n\n\n\n\n\n\nglm(data = age.data,\n   formula = buyer ~ age,\n   family = 'binomial') %&gt;%\n  summary() %&gt;%\n  coef() %&gt;%\n  round(4)\n\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  -7.5879     4.2767 -1.7742   0.0760\nage           0.1969     0.1099  1.7913   0.0732"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-1",
    "href": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-1",
    "title": "Logistic Regression",
    "section": "Linear Probability Models are Often Good Enough",
    "text": "Linear Probability Models are Often Good Enough\n\nFor interpretation:\n\nHow do we interpret each result?\n\n\n\n\nIt’s the same!\n\nOlder people are more likely to be buyers\nSignificance is different, but not really"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-2",
    "href": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-2",
    "title": "Logistic Regression",
    "section": "Linear Probability Models are Often Good Enough",
    "text": "Linear Probability Models are Often Good Enough\n\nFor prediction:\n\nHow do our predictions differ?\nLet’s say we have a new group of 10 customers\n\n\n\nage.data.2 &lt;- data.frame(\n  age = c(25, 28, 30, 35, 40,\n         48, 52, 60, 65, 65))\n\n\n\nLinear regression\n\n\nlinear.reg &lt;- lm(data = age.data, formula = buyer ~ age)\n\n\n\n\nPredict buyer from new ages\n\n\nage.data.2$buyer.linear &lt;- predict(\n  linear.reg,\n  newdata = age.data.2,\n  type = 'response'\n)\n\n\nBut buyer has to be 0 or 1! So set it as such:"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-3",
    "href": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-3",
    "title": "Logistic Regression",
    "section": "Linear Probability Models are Often Good Enough",
    "text": "Linear Probability Models are Often Good Enough\n\nBut buyer has to be 0 or 1! So set it as such:\n\n\n\nage.data.2$buyer.linear &lt;- ifelse(\n  age.data.2$buyer.linear &gt;= .5, 1, 0\n)\n\n\nLet’s look at the result:\n\n\n\n\n\n\n\n   age buyer.linear\n1   25            0\n2   28            0\n3   30            0\n4   35            0\n5   40            1\n6   48            1\n7   52            1\n8   60            1\n9   65            1\n10  65            1"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-4",
    "href": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-4",
    "title": "Logistic Regression",
    "section": "Linear Probability Models are Often Good Enough",
    "text": "Linear Probability Models are Often Good Enough\n\nLogistic regression\n\n\nlogistic.reg &lt;- glm(data = age.data, formula = buyer ~ age, family = 'binomial')\n\n\n\nPredict buyer from new ages\n\n\nage.data.2$buyer.logistic &lt;- predict(\n  logistic.reg,\n  newdata = age.data.2,\n  type = 'response'\n)\n\n\nBut buyer has to be 0 or 1! So set it as such:"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-5",
    "href": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-5",
    "title": "Logistic Regression",
    "section": "Linear Probability Models are Often Good Enough",
    "text": "Linear Probability Models are Often Good Enough\n\nBut buyer has to be 0 or 1! So set it as such:\n\n\n\nage.data.2$buyer.logistic &lt;- ifelse(\n  age.data.2$buyer.logistic &gt;= .5, 1, 0\n)\n\n\nLet’s look at the result:\n\n\n\n\n\n\n\n   age buyer.linear buyer.logistic\n1   25            0              0\n2   28            0              0\n3   30            0              0\n4   35            0              0\n5   40            1              1\n6   48            1              1\n7   52            1              1\n8   60            1              1\n9   65            1              1\n10  65            1              1"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-6",
    "href": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-6",
    "title": "Logistic Regression",
    "section": "Linear Probability Models are Often Good Enough",
    "text": "Linear Probability Models are Often Good Enough\n\nWhat did you notice?\n\n\n\nThere was no difference!\n\nWhat about if we had more data?"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-7",
    "href": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-7",
    "title": "Logistic Regression",
    "section": "Linear Probability Models are Often Good Enough",
    "text": "Linear Probability Models are Often Good Enough\n\nLet’s simulate 1000 buyers\n\n\nset.seed(101)\nage.data.3 &lt;- data.frame(\n  age = runif(n = 1000,\n              min = 20,\n              max = 65))\n\n\nRun the following:\n\nLinear regression\nLogistic regression\nPredict from linear\nPredict from logistic\nCompare the differences"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-8",
    "href": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-8",
    "title": "Logistic Regression",
    "section": "Linear Probability Models are Often Good Enough",
    "text": "Linear Probability Models are Often Good Enough\n\nLet’s simulate 1000 buyers\n\n\nset.seed(101)\nage.data.3 &lt;- data.frame(\n  age = runif(n = 1000,\n              min = 20,\n              max = 65))\n\nlinear.reg &lt;- lm(data = age.data, formula = buyer ~ age)\nlogistic.reg &lt;- glm(data = age.data, formula = buyer ~ age, family = 'binomial')\n\nage.data.3$buyer.linear &lt;- predict(\n  linear.reg,\n  newdata = age.data.3,\n  type = 'response'\n)\n\nage.data.3$buyer.linear &lt;- ifelse(\n  age.data.3$buyer.linear &gt;= .5, 1, 0\n)\n\nage.data.3$buyer.logistic &lt;- predict(\n  logistic.reg,\n  newdata = age.data.3,\n  type = 'response'\n)\n\nage.data.3$buyer.logistic &lt;- ifelse(\n  age.data.3$buyer.logistic &gt;= .5, 1, 0\n)"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-9",
    "href": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-9",
    "title": "Logistic Regression",
    "section": "Linear Probability Models are Often Good Enough",
    "text": "Linear Probability Models are Often Good Enough\n\nLet’s simulate 1000 buyers\nHow often are our results different?\n\n\nage.data.3$discrepency &lt;- ifelse(\n  age.data.3$buyer.linear == age.data.3$buyer.logistic, 0, 1\n)\n\nround( mean( age.data.3$discrepency), 3)\n\n[1] 0.031\n\n\n\n\nWhat’s another way I might want to look at this?\n\n\n\n\nggplot(age.data.3,\n       aes( x = age,\n            y = discrepency)) +\n  geom_point()"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-10",
    "href": "teaching/Logistic Regression Slides.html#linear-probability-models-are-often-good-enough-10",
    "title": "Logistic Regression",
    "section": "Linear Probability Models are Often Good Enough",
    "text": "Linear Probability Models are Often Good Enough\n\nWith whoever is beside you, come up with some ideas to answer:\n\nWhen are linear probability models probably not “good enough”?\nWhen will linear and logistic regressions give different results?"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#when-will-linear-and-logistic-regressions-give-different-results",
    "href": "teaching/Logistic Regression Slides.html#when-will-linear-and-logistic-regressions-give-different-results",
    "title": "Logistic Regression",
    "section": "When will linear and logistic regressions give different results?",
    "text": "When will linear and logistic regressions give different results?\n\nFar to the left and right of the x-axis?"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#when-will-linear-and-logistic-regressions-give-different-results-1",
    "href": "teaching/Logistic Regression Slides.html#when-will-linear-and-logistic-regressions-give-different-results-1",
    "title": "Logistic Regression",
    "section": "When will linear and logistic regressions give different results?",
    "text": "When will linear and logistic regressions give different results?\n\nFar to the left and right of the x-axis?\nOnly if we don’t round our linear answers down!\n\nIf we round, there is no difference"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#when-will-linear-and-logistic-regressions-give-different-results-2",
    "href": "teaching/Logistic Regression Slides.html#when-will-linear-and-logistic-regressions-give-different-results-2",
    "title": "Logistic Regression",
    "section": "When will linear and logistic regressions give different results?",
    "text": "When will linear and logistic regressions give different results?\nIn the middle?"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#when-will-linear-and-logistic-regressions-give-different-results-3",
    "href": "teaching/Logistic Regression Slides.html#when-will-linear-and-logistic-regressions-give-different-results-3",
    "title": "Logistic Regression",
    "section": "When will linear and logistic regressions give different results?",
    "text": "When will linear and logistic regressions give different results?\nIn the middle?\n\n\nYes!\nSpecifically when there is a strong flip"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#when-will-linear-and-logistic-regressions-give-different-results-4",
    "href": "teaching/Logistic Regression Slides.html#when-will-linear-and-logistic-regressions-give-different-results-4",
    "title": "Logistic Regression",
    "section": "When will linear and logistic regressions give different results?",
    "text": "When will linear and logistic regressions give different results?\nWhen the flip is very strong:\n\nage.data &lt;- data.frame(\n  age = c(27,30,32,33,35,40,44,45,50,58,59,60),\n  buyer = c(0,0,0,0,0,0,1,0,1,1,1,1))"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#when-will-linear-and-logistic-regressions-give-different-results-5",
    "href": "teaching/Logistic Regression Slides.html#when-will-linear-and-logistic-regressions-give-different-results-5",
    "title": "Logistic Regression",
    "section": "When will linear and logistic regressions give different results?",
    "text": "When will linear and logistic regressions give different results?\nWhen the flip is very strong:\n\nDoes this seem likely?\n\n\n\nIt’s not\nYou should be very curious if something flips like this\n\nPeople are not lightbulbs"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#logistic-regression-takeaways",
    "href": "teaching/Logistic Regression Slides.html#logistic-regression-takeaways",
    "title": "Logistic Regression",
    "section": "Logistic Regression: Takeaways",
    "text": "Logistic Regression: Takeaways\n\n\nRegression where the DV can be either 1 or 0\nInterpreting coefficients is a little less fluent than with linear regression\n\nLog-odds instead of unit changes\n\nLog-odds are mathematically convenient\nNot so interpretable\n\nFocus on z-scores, and predict()\n\nLogistic regression is more computationally intensive\n\nMaximum likelihood vs least squares"
  },
  {
    "objectID": "teaching/Logistic Regression Slides.html#logistic-regression-takeaways-1",
    "href": "teaching/Logistic Regression Slides.html#logistic-regression-takeaways-1",
    "title": "Logistic Regression",
    "section": "Logistic Regression: Takeaways",
    "text": "Logistic Regression: Takeaways\n\n\nLogistic regression is more precise\n\nDoes not allow for &gt; 1 or &lt; 0\n“S” shape allows more intermediate data points to predict 0 or 1\n\nThere is often not a meaningful difference between logistic and linear regression\n\nSo it’s good for prediction (where “rounding” can fold into bigger errors)\nBut might not be needed for communication\nRun both, see if your results match!"
  },
  {
    "objectID": "teaching/03 Logistic Regression.html",
    "href": "teaching/03 Logistic Regression.html",
    "title": "03 - Logistic Regression",
    "section": "",
    "text": "In this semester, I am going to ask you to do a fair bit of work before coming to class. This will make our class time shorter, more manageable, and hopefully less boring.\nI am also going to use this as an opportunity for you to directly earn grade points for your effort/labor, rather than “getting things right” on an exam.\nTherefore, I will ask you to work through the posted slides on Canvas before class. Throughout the slides, I will post Pre-Class Questions for you to work through in R. These will look like this:\n\nIn R, please write code that will read in the .csv from Canvas called sf_listings_2312.csv. Assign this the name bnb.\nYou will then write your answer in a .r script:\n\n\nClick to show code and output\n\n\n\n# Q1\n#bnb &lt;- read.csv(\"sf_listings_2312.csv\")\n\n\n\n\n\nTo earn full points, you need to organize your code correctly. Specifically, you need to:\n\nAnswer questions in order.\n\nIf you answer them out of order, just re-arrange the code after.\n\nPreface each answer with a comment (# Q1/# Q2/# Q3) that indicates exactly which question you are answering.\n\nPlease just write the letter Q and the number in this comment.\n\nMake sure your code runs on its own, on anyone’s computer.\n\nTo do this, I would always include rm(list = ls()) at the top of every .r script. This will clean everything from the environment, allowing you to see if this runs on my computer.\n\n\n\n\n\n\nYou must submit this to Canvas before 9:00am on the day of class. Even if class starts at 10:00am that day, these are always due at 9:00.\nYou must submit this code as a .txt file. This is because Canvas cannot present .R files to me in SpeedGrader. To save as .txt:\n\nClick File -&gt; New File -&gt; Text File\nCopy and paste your completed code to that new text file.\nSave the file as firstname_lastname_module.txt\n\nFor example, my file for Module 01 would be matt_meister_01.txt\nMy file for module 05 would be matt_meister_05.txt\n\n\n\n\n\n\n\nI will grade these for completion.\nYou will receive 1 point for every question you give an honest attempt to answer\nYour grade will be the number of questions you answer, divided by the total number of questions.\n\nThis is why it is important that you number each answer with # Q1.\nAny questions that are not numbered this way will be graded incomplete, because I can’t find them.\n\nYou will receive a 25% penalty for submitting these late.\nI will post my solutions after class."
  },
  {
    "objectID": "teaching/03 Logistic Regression.html#pre-class-code-assignment-instructions",
    "href": "teaching/03 Logistic Regression.html#pre-class-code-assignment-instructions",
    "title": "03 - Logistic Regression",
    "section": "",
    "text": "In this semester, I am going to ask you to do a fair bit of work before coming to class. This will make our class time shorter, more manageable, and hopefully less boring.\nI am also going to use this as an opportunity for you to directly earn grade points for your effort/labor, rather than “getting things right” on an exam.\nTherefore, I will ask you to work through the posted slides on Canvas before class. Throughout the slides, I will post Pre-Class Questions for you to work through in R. These will look like this:\n\nIn R, please write code that will read in the .csv from Canvas called sf_listings_2312.csv. Assign this the name bnb.\nYou will then write your answer in a .r script:\n\n\nClick to show code and output\n\n\n\n# Q1\n#bnb &lt;- read.csv(\"sf_listings_2312.csv\")\n\n\n\n\n\nTo earn full points, you need to organize your code correctly. Specifically, you need to:\n\nAnswer questions in order.\n\nIf you answer them out of order, just re-arrange the code after.\n\nPreface each answer with a comment (# Q1/# Q2/# Q3) that indicates exactly which question you are answering.\n\nPlease just write the letter Q and the number in this comment.\n\nMake sure your code runs on its own, on anyone’s computer.\n\nTo do this, I would always include rm(list = ls()) at the top of every .r script. This will clean everything from the environment, allowing you to see if this runs on my computer.\n\n\n\n\n\n\nYou must submit this to Canvas before 9:00am on the day of class. Even if class starts at 10:00am that day, these are always due at 9:00.\nYou must submit this code as a .txt file. This is because Canvas cannot present .R files to me in SpeedGrader. To save as .txt:\n\nClick File -&gt; New File -&gt; Text File\nCopy and paste your completed code to that new text file.\nSave the file as firstname_lastname_module.txt\n\nFor example, my file for Module 01 would be matt_meister_01.txt\nMy file for module 05 would be matt_meister_05.txt\n\n\n\n\n\n\n\nI will grade these for completion.\nYou will receive 1 point for every question you give an honest attempt to answer\nYour grade will be the number of questions you answer, divided by the total number of questions.\n\nThis is why it is important that you number each answer with # Q1.\nAny questions that are not numbered this way will be graded incomplete, because I can’t find them.\n\nYou will receive a 25% penalty for submitting these late.\nI will post my solutions after class."
  },
  {
    "objectID": "teaching/03 Logistic Regression.html#setup",
    "href": "teaching/03 Logistic Regression.html#setup",
    "title": "03 - Logistic Regression",
    "section": "Setup",
    "text": "Setup\nLoad in these packages. If you do not have them, you will need to install them.\n\ne.g., install.packages(\"dplyr\")\n\n\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\nRead in airline_satisfaction.csv from Canvas, and assign it the name data:\n\n\nClick to show code and output\n\n\n\ndata &lt;- read.csv('airline_satisfaction.csv')\n\n\n\nAnd read in airbnb_logistic.csv from Canvas, and assign it the name airbnb.data:\n\n\nClick to show code and output\n\n\n\nairbnb.data &lt;- read.csv('airbnb_logistic.csv')"
  },
  {
    "objectID": "teaching/03 Logistic Regression.html#binary-outcomes",
    "href": "teaching/03 Logistic Regression.html#binary-outcomes",
    "title": "03 - Logistic Regression",
    "section": "Binary Outcomes",
    "text": "Binary Outcomes\nThus far, we’ve spent a lot of time talking about “linear” regression. Linear regression is called linear because it calculates the best equation of a line to fit our data. Effectively–what is the best line we can draw through our data points?\nNow, we will introduce logistic regression. Logistic regression is similar to linear regression, in that we are drawing a line through points in our data, which we do to predict some outcome. However, logistic regression is used in situations where our outcome is either a 0 or a 1 (or a yes/no, or a TRUE/FALSE). With a logistic regression, we are predicting the probability of an outcome. (In linear regression, we are predicting the level of an outcome.)\nFor example, consider last week’s Airbnb data. We might be interested in staying at an Airbnb in North Beach, but terrified that the density of apartments and restaurants–which attracted us in the first place–also means that Airbnbs close to the center of North Beach have a rat problem. Being well-informed statisticians (and newly skilled at text analysis), we could analyze whether reviews are more likely to mention rats if they are closer to the center of North Beach.1\n\nPre-Class Q1\n\nIn the data set airbnb_logistic.csv, you have observations from reviews of the distance of an Airbnb from the center of North Beach (in miles), and whether the review mentions rats. Remember that we can only use a logistic regression on outcomes that are binary. So to start, create a histogram for each variable (distance and rats), to check that rats is binary, and that distance is not crazy.\n\n\nClick to show code and output\n\n\n\nhist(airbnb.data$rats)\n\n\n\n\n\n\n\nhist(airbnb.data$distance)\n\n\n\n\n\n\n\n\n\n\n\nPre-Class Q2\n\nNow, using a linear regression, test whether distance is related to rats. Then, plot this with ggplot(), using geom_smooth(method = \"lm\").\n\n\nClick to show code and output\n\n\n\nsummary(lm(data = airbnb.data, rats ~ distance))\n\n\nCall:\nlm(formula = rats ~ distance, data = airbnb.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.49174 -0.21301 -0.02665  0.15364  0.73748 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.56044    0.05812   9.643 7.21e-16 ***\ndistance    -0.10907    0.01374  -7.940 3.40e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2889 on 98 degrees of freedom\nMultiple R-squared:  0.3915,    Adjusted R-squared:  0.3853 \nF-statistic: 63.05 on 1 and 98 DF,  p-value: 3.399e-12\n\nggplot(data = airbnb.data, \n       aes( x = distance, \n            y = rats)) +\n  geom_smooth(method = 'lm')\n\n\n\n\n\n\n\n\n\n\nYou should see that mention of rats decreases with distance, and that this is statistically significant:\n\n\n\n\n\n\n\n\n\nBecause our DV is either 0 or 1, the coefficient can be interpreted as a change in probability of the outcome with a one-unit change in the IV. And each point on the line shows us the predicted probability of rats at a given distance. In cases like this (i.e., with binary dependent variables), linear regressions are also referred to as “linear probability models”. This is good enough to tell us that rats are more prevalent as we get closer to North Beach. However, there are two oddities you should notice with this relationship:\n\nFirst, linearity:\nA linear regression always predicts straight lines. As a result, it predicts that the relationship between rats and distance is perfectly linear. This regression is saying that the likelihood of encountering rats changes by -10.9%2 with each mile further from North Beach.\nSometimes linearity is correct. Sometimes, people’s behavior does change according to some constant relationship with a predictor. But, often, relationships are not linear.\nOne common example of this in marketing is the relationship between advertising spending and sales. When we increase ad spending, sales also increase. However, this is not linear. When we go from spending $0 to $100, we should see a big bump in sales. But if we go from spending $100,000 to $100,100, it is very unlikely we see any difference at all. This specific case is referred to as “diminishing marginal returns”, and applies to a lot of contexts. You can read more at this link.\nSo in our example, we might be concerned that the relationship between distance and rats is also not linear. We’ll talk about when this is a problem (vs when linearity is “fine enough”) later.\n\n\nSecond, impossible predictions:\nLook back up at the plot you made with your linear prediction. Remember, rats is binary. It can be 0 or 1. Therefore, our predictions are probabilities. They should be between 0 and 1–0 corresponding to “no chance”, and 1 corresponding to “certainty”. Let’s test if we satisfy that here.\n\nPre-Class Q3\n\nUse the predict() function to predict the probability of rats at distances of 0, 1, and 6 miles, using your linear regression. Remember that predict() takes three arguments: (1) An object (which is our model, in this case a linear regression), (2) newdata (which is a data.frame with the exact same variable names as in our regression), and (3) type (which we will always set to response in this class).\n\n\nClick to show code and output\n\n\n\nlinear_model &lt;- lm(data = airbnb.data, rats ~ distance)\nnew_data &lt;- data.frame(distance = c(0, 1, 6))\npredict(object = linear_model,\n        newdata = new_data,\n        type = \"response\")\n\n          1           2           3 \n 0.56044316  0.45137074 -0.09399136 \n\n\n\n\nYou might have expected this after looking at the plot above. When distance is relatively close, your predictions are fine–they are between 0 and 1. But, when distance is 6, your prediction is negative! This happens because a linear regression has to fit a straight line to our data. So, while it fits the best line it can, that linearity can cause it to make impossible predictions–probabilities above 1 and below 0.\nSo maybe we need something better. In class and in your homework, we will see that, often, linear regression is fine in practice. But before we do that, let’s consider the alternative–logistic regression."
  },
  {
    "objectID": "teaching/03 Logistic Regression.html#logistic-regression",
    "href": "teaching/03 Logistic Regression.html#logistic-regression",
    "title": "03 - Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression models the probability of an outcome using an S-shaped curve called the logistic function. This S-shape ensures that probabilities are between 0 and 1. To run a logistic regression in R, we use a function very similar to lm(), with two changes. For example, to test our question above using linear regression, we wrote:\nlm(data = airbnb.data, rats ~ distance)\nBut to test our question using logistic regression, we write:\nglm(data = airbnb.data, rats ~ distance, family = \"binomial\")\nThe two changes are:\n\nUsing glm(), which stands for Generalized Linear Model, and allows for different forms of regression.\nSpecifying family = \"binomial\", which tells R that the dependent variable is binary, so it should use the logistic function.\n\nAdditionally, when we plot this using ggplot(), we change the code in just two ways again. Instead of writing geom_smooth(method = 'lm'), we:\n\nChange method = 'lm' to method = 'glm'\nInside of geom_smooth() (after method = 'glm'), we add the argument method.args = \"binomial\"\nNote that sometimes you will also have to add the argument formula = y ~ x\n\n\nPre-Class Q4\n\nNow, using a logistic regression, test whether distance is related to rats. Then, plot this with ggplot(), using geom_smooth(method = \"glm\", method.args = \"binomial\").\n\n\nClick to show code and output\n\n\n\nsummary(glm(data = airbnb.data, rats ~ distance, family = \"binomial\"))\n\n\nCall:\nglm(formula = rats ~ distance, family = \"binomial\", data = airbnb.data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   2.8001     0.9715   2.882 0.003949 ** \ndistance     -2.5101     0.7139  -3.516 0.000438 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 87.934  on 99  degrees of freedom\nResidual deviance: 30.340  on 98  degrees of freedom\nAIC: 34.34\n\nNumber of Fisher Scoring iterations: 8\n\nggplot(data = airbnb.data, \n       aes( x = distance, \n            y = rats)) +\n  geom_smooth(method = \"glm\", \n              method.args = \"binomial\")\n\n\n\n\n\n\n\n\n\n\nIn this case, the S-shape is backwards, because rats decrease with distance. But you should notice that this model tells a very different story, and makes very different predictions to the linear model. This is super obvious if you look at the two plots side-by-side, including points:\n\n\n\n\n\n\n\n\n\nObviously, the linear regression was not flexible to pick up on the fact that this relationship is actually not linear. The difference in rats from 0-1 mile is much larger than the distance from 3-4, or even 3-6! There are also no rats further than 3 miles away, and very few at 2 miles away. The logistic regression can fit itself to all of this information, whereas the linear regression cannot. Ideally, this makes better predictions.\n\nPre-Class Q5\n\nUse the predict() function to predict the probability of rats at distances of 0, 1, and 6 miles, using your logistic regression. Remember that predict() takes three arguments: (1) An object (which is our model, in this case a logistic regression), (2) newdata (which is a data.frame with the exact same variable names as in our regression), and (3) type (which we will always set to response in this class).\n\n\nClick to show code and output\n\n\n\nlogistic_model &lt;- glm(data = airbnb.data, rats ~ distance, family = \"binomial\")\nnew_data &lt;- data.frame(distance = c(0, 1, 6))\npredict(object = logistic_model,\n        newdata = new_data,\n        type = \"response\") |&gt;\n  round(3)\n\n    1     2     3 \n0.943 0.572 0.000 \n\n\n\n\nConveniently, we don’t get any impossible predictions!"
  },
  {
    "objectID": "teaching/03 Logistic Regression.html#coefficients-in-a-logistic-regression",
    "href": "teaching/03 Logistic Regression.html#coefficients-in-a-logistic-regression",
    "title": "03 - Logistic Regression",
    "section": "Coefficients in a logistic regression",
    "text": "Coefficients in a logistic regression\nSo far, logistic regression has been all good news. Now is when the bad begins. Look at the coefficients we got for that logistic regression:\n\n\nLogistic Regression Coefficients\n\n\n\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\n(Intercept)\n2.80\n0.972\n2.882\n0.004\n\n\ndistance\n-2.51\n0.714\n-3.516\n0.000\n\n\n\n\n\n\nHow is the intercept above 1 if the whole curve is between 0 and 1? And how is the coefficient for distance -2.51 if a one-unit change in distance clearly does not lead to a 2.51 unit decrease in rats?\nUnfortunately, explaining this will take some math.\nThe key to understanding these results is to be aware that both the intercept and “slope” in a logistic regression are expressed in log-odds, not probabilities.\n\nOdds are the ratio of the probability of success (\\(p\\)) to failure (\\(1-p\\)).\n\n\\(\\text{odds} = \\frac{\\text{p}}{1-\\text{p}}\\)\nThe log-odds is the natural logarithm of these odds (\\(\\text{log-odds} = \\log(\\frac{\\text{p}}{1-\\text{p}})\\)).\n\nWe’ve seen the natural logarithm before, when we used log() in R.\n\n\nMeanwhile, probability is equal to \\(\\text{p} = \\frac{\\text{odds}}{1 + \\text{odds}}\\)\n\nStarting with:\n\n\\(\\text{odds} = \\frac{p}{1-p}\\)\n\nMultiply both sides by \\(1-p\\) to eliminate the denominator:\n\n\\(\\text{odds} \\times (1-p) = p\\)\n\nDistribute \\(\\text{odds}\\):\n\n\\(\\text{odds} - \\text{odds} \\times p = p\\)\n\nRearrange to isolate \\(p\\):\n\n\\(\\text{odds} = p + \\text{odds} \\times p\\)\n\nFactor \\(p\\) out of the right-hand side:\n\n\\(\\text{odds} = p \\times (1 + \\text{odds})\\)\n\nDivide through by \\(1 + \\text{odds}\\):\n\n\\(p = \\frac{\\text{odds}}{1 + \\text{odds}}\\)\n\n\n\n\nWhy is the intercept above 1?\nConveniently, the intercept is still the value of something when the independent variable is 0. But this is not the predicted value of the dependent variable (as it is with linear regression). With logistic regression, everything is in log-odds, including the intercept.\nThe intercept is the log-odds of the outcome (\\(\\text{rats} = 1\\)) when the independent variable ($ $) is 0.\n\nIn our example\nThe intercept is 2.8. Remember also that probability equals \\(p = \\frac{\\text{odds}}{1 + \\text{odds}}\\). And, that log-odds are \\(\\text{log}(\\text{odds})\\). That means that when distance is zero:\n\\(\\text{log}(\\text{odds}) = 2.8\\)\nYou can convert log-odds to odds by exponentiation. In R, this is the function exp():\n\\(\\text{odds} = e^{2.8}\\) \\(\\text{odds} = 16.44\\)\nWe can sub this into the probability equation:\n\\(p = \\frac{\\text{16.44}}{1+\\text{16.44}}\\)\nAnd then just regular ole’ math:\n\\(p = \\frac{\\text{16.44}}{\\text{17.44}}\\) \\(p \\approx 0.943\\)\nWhen distance is zero, the predicted probability of a review mentioning a rat is \\(\\approx\\) 94.3%.\n\n\nNote:\nThis math is not a lot of fun. Here is a function that will take log-odds, and give you probability. Save this function somewhere and source it when you need to use log-odds.\n\nprob_from_log_odds &lt;- function(log_odds){\n  odds &lt;- exp(log_odds) # Exponentiate first\n  prob &lt;- odds / (1 + odds)\n  return(prob) \n}\n\nHere is how you can use it:\n\n# Provide regression coefficient directly\nprob_from_log_odds(logistic_model$coefficients[[1]])\n\n[1] 0.942682\n\n# Input a number\nprob_from_log_odds(16.44)\n\n[1] 0.9999999\n\n\n\n\n\nHow can the coefficient for distance be -2.51 if a one-unit change doesn’t lead to a 2.51 unit decrease in rats?\nThis happens because, again, the coefficient is expressed in log-odds, not the probability scale. A coefficient of -2.51 means that for a one-unit increase in distance, the log-odds of rats decreases by 2.51.\nIn terms of odds, the change (expressed by \\(\\Delta\\)) is multiplicative:\n\\(\\Delta \\text{log}(\\text{odds}) = -2.51\\) \\(\\Delta \\text{odds} = e^{-2.51}\\) \\(\\Delta \\text{odds} \\approx 0.081\\)\nThis means the odds of rats are reduced by 8.1% of the prior odds for each one-unit increase in distance. This is what allows the logistic regression to curve.\n\n\nMy suggestion\nLogistic regression coefficients are quite confusing if you’re not well-versed in this math. This interpretability is their biggest flaw. Instead of trying to understand them in detail, I would use the predict() function in R, to see how your predicted probabilities change at different levels of your independent variable.\nWhat you need to take away from this is not the specifics of the math, but the ideas that:\n\nLogistic regression provides coefficients in log-odds, not probabilities.\nThis allows the logistic regression function to not hit 0 or 1.\nThis also allows the logistic regression function to curve, identifying non-linear relationships between our dependent variable and independent variable."
  },
  {
    "objectID": "teaching/03 Logistic Regression.html#airline-satisfaction",
    "href": "teaching/03 Logistic Regression.html#airline-satisfaction",
    "title": "03 - Logistic Regression",
    "section": "Airline Satisfaction",
    "text": "Airline Satisfaction\nThe data set airline_satisfaction.csv on Canvas contains the results of a survey of airline customers. These responses were collected immediately after getting off a plane trip. The airline collected some personal and travel information from each customer, and then asked about their satisfaction with a number of elements of their trip.\nIn class, we will practice testing logistic regressions, comparing them to linear regressions, and show how we can make predictions of new data with these functions. In the pre-class workshop, I would like you to orient yourself with the data.\n\nPre-Class Q6\n\nOur dependent variable is satisfaction. Summarise this variable, and see what (if anything) we have to do in order to use it in a regression.\n\n\nClick to show code and output\n\n\n\nsummary(data$satisfaction)\n\n   Length     Class      Mode \n    25976 character character \n\nunique(data$satisfaction)\n\n[1] \"satisfied\"               \"neutral or dissatisfied\"\n\n#This variable is a character, so we are going to have to turn it into 0/1\ndata$satisfied &lt;- ifelse(data$satisfaction == \"satisfied\", 1, 0)\n\n\n\n\nPre-Class Q7\n\nWhat percentage of customers are satisfied?\n\n\nClick to show code and output\n\n\n\nmean(data$satisfied)\n\n[1] 0.4389821\n\n\n\n\n\nPre-Class Q8\n\nNow, let’s get some ideas for a potential hypothesis. You can look at the other variables we have with str() and summary().\n\n\nClick to show code and output\n\n\n\nstr(data)\n\n'data.frame':   25976 obs. of  26 variables:\n $ X                                : int  0 1 2 3 4 5 6 7 8 9 ...\n $ id                               : int  19556 90035 12360 77959 36875 39177 79433 97286 27508 62482 ...\n $ Gender                           : chr  \"Female\" \"Female\" \"Male\" \"Male\" ...\n $ Customer.Type                    : chr  \"Loyal Customer\" \"Loyal Customer\" \"disloyal Customer\" \"Loyal Customer\" ...\n $ Age                              : int  52 36 20 44 49 16 77 43 47 46 ...\n $ Type.of.Travel                   : chr  \"Business travel\" \"Business travel\" \"Business travel\" \"Business travel\" ...\n $ Class                            : chr  \"Eco\" \"Business\" \"Eco\" \"Business\" ...\n $ Flight.Distance                  : int  160 2863 192 3377 1182 311 3987 2556 556 1744 ...\n $ Inflight.wifi.service            : int  5 1 2 0 2 3 5 2 5 2 ...\n $ Departure.Arrival.time.convenient: int  4 1 0 0 3 3 5 2 2 2 ...\n $ Ease.of.Online.booking           : int  3 3 2 0 4 3 5 2 2 2 ...\n $ Gate.location                    : int  4 1 4 2 3 3 5 2 2 2 ...\n $ Food.and.drink                   : int  3 5 2 3 4 5 3 4 5 3 ...\n $ Online.boarding                  : int  4 4 2 4 1 5 5 4 5 4 ...\n $ Seat.comfort                     : int  3 5 2 4 2 3 5 5 5 4 ...\n $ Inflight.entertainment           : int  5 4 2 1 2 5 5 4 5 4 ...\n $ On.board.service                 : int  5 4 4 1 2 4 5 4 2 4 ...\n $ Leg.room.service                 : int  5 4 1 1 2 3 5 4 2 4 ...\n $ Baggage.handling                 : int  5 4 3 1 2 1 5 4 5 4 ...\n $ Checkin.service                  : int  2 3 2 3 4 1 4 5 3 5 ...\n $ Inflight.service                 : int  5 4 2 1 2 2 5 4 3 4 ...\n $ Cleanliness                      : int  5 5 2 4 4 5 3 3 5 4 ...\n $ Departure.Delay.in.Minutes       : int  50 0 0 0 0 0 0 77 1 28 ...\n $ Arrival.Delay.in.Minutes         : num  44 0 0 6 20 0 0 65 0 14 ...\n $ satisfaction                     : chr  \"satisfied\" \"satisfied\" \"neutral or dissatisfied\" \"satisfied\" ...\n $ satisfied                        : num  1 1 0 1 1 1 1 1 1 1 ...\n\nsummary(data)\n\n       X               id            Gender          Customer.Type     \n Min.   :    0   Min.   :    17   Length:25976       Length:25976      \n 1st Qu.: 6494   1st Qu.: 32170   Class :character   Class :character  \n Median :12988   Median : 65320   Mode  :character   Mode  :character  \n Mean   :12988   Mean   : 65006                                        \n 3rd Qu.:19481   3rd Qu.: 97584                                        \n Max.   :25975   Max.   :129877                                        \n                                                                       \n      Age        Type.of.Travel        Class           Flight.Distance\n Min.   : 7.00   Length:25976       Length:25976       Min.   :  31   \n 1st Qu.:27.00   Class :character   Class :character   1st Qu.: 414   \n Median :40.00   Mode  :character   Mode  :character   Median : 849   \n Mean   :39.62                                         Mean   :1194   \n 3rd Qu.:51.00                                         3rd Qu.:1744   \n Max.   :85.00                                         Max.   :4983   \n                                                                      \n Inflight.wifi.service Departure.Arrival.time.convenient Ease.of.Online.booking\n Min.   :0.000         Min.   :0.000                     Min.   :0.000         \n 1st Qu.:2.000         1st Qu.:2.000                     1st Qu.:2.000         \n Median :3.000         Median :3.000                     Median :3.000         \n Mean   :2.725         Mean   :3.047                     Mean   :2.757         \n 3rd Qu.:4.000         3rd Qu.:4.000                     3rd Qu.:4.000         \n Max.   :5.000         Max.   :5.000                     Max.   :5.000         \n                                                                               \n Gate.location   Food.and.drink  Online.boarding  Seat.comfort  \n Min.   :1.000   Min.   :0.000   Min.   :0.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000  \n Median :3.000   Median :3.000   Median :4.000   Median :4.000  \n Mean   :2.977   Mean   :3.215   Mean   :3.262   Mean   :3.449  \n 3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:5.000  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :5.000  \n                                                                \n Inflight.entertainment On.board.service Leg.room.service Baggage.handling\n Min.   :0.000          Min.   :0.000    Min.   :0.00     Min.   :1.000   \n 1st Qu.:2.000          1st Qu.:2.000    1st Qu.:2.00     1st Qu.:3.000   \n Median :4.000          Median :4.000    Median :4.00     Median :4.000   \n Mean   :3.358          Mean   :3.386    Mean   :3.35     Mean   :3.633   \n 3rd Qu.:4.000          3rd Qu.:4.000    3rd Qu.:4.00     3rd Qu.:5.000   \n Max.   :5.000          Max.   :5.000    Max.   :5.00     Max.   :5.000   \n                                                                          \n Checkin.service Inflight.service  Cleanliness    Departure.Delay.in.Minutes\n Min.   :1.000   Min.   :0.000    Min.   :0.000   Min.   :   0.00           \n 1st Qu.:3.000   1st Qu.:3.000    1st Qu.:2.000   1st Qu.:   0.00           \n Median :3.000   Median :4.000    Median :3.000   Median :   0.00           \n Mean   :3.314   Mean   :3.649    Mean   :3.286   Mean   :  14.31           \n 3rd Qu.:4.000   3rd Qu.:5.000    3rd Qu.:4.000   3rd Qu.:  12.00           \n Max.   :5.000   Max.   :5.000    Max.   :5.000   Max.   :1128.00           \n                                                                            \n Arrival.Delay.in.Minutes satisfaction         satisfied    \n Min.   :   0.00          Length:25976       Min.   :0.000  \n 1st Qu.:   0.00          Class :character   1st Qu.:0.000  \n Median :   0.00          Mode  :character   Median :0.000  \n Mean   :  14.74                             Mean   :0.439  \n 3rd Qu.:  13.00                             3rd Qu.:1.000  \n Max.   :1115.00                             Max.   :1.000  \n NA's   :83                                                 \n\n\n\n\n\nPre-Class Q9\n\nCreate a scatterplot to visualize the relationship between Flight.Distance and satisfied.\n\n\nClick to show code and output\n\n\n\nggplot(data = data, aes(x = Flight.Distance, y = satisfied)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", \n              method.args = \"binomial\") +\n  coord_cartesian(ylim = c(-.1,1.1))\n\n\n\n\n\n\n\n\n\n\n\nPre-Class Q10\n\nIt looks like Flight.Distance might have some outliers. We can see if this is the case with a histogram.\n\n\nClick to show code and output\n\n\n\nhist(data$Flight.Distance)\n\n\n\n\n\n\n\n\n\n\n\nPre-Class Q11\n\nTo fix Flight.Distance, let’s log-transform it with log(). Assign this to the variable data$log_Flight.Distance.\n\n\nClick to show code and output\n\n\n\ndata$log_Flight.Distance &lt;- log(data$Flight.Distance)\nhist(data$log_Flight.Distance)"
  },
  {
    "objectID": "teaching/03 Logistic Regression.html#if-you-stop-here",
    "href": "teaching/03 Logistic Regression.html#if-you-stop-here",
    "title": "03 - Logistic Regression",
    "section": "If you stop here",
    "text": "If you stop here\nYou are ready for class. If you want extra practice and full points, try these questions\n\nPre-Class Q12\n\nYou might have noticed that the logistic regression fit our airbnb.data nicely, but that the coefficients it gave us were… gross. In class, I will talk about how in practice, a linear regression is often good enough, because our relationships are often close enough to linear. True, strong S-shapes are rare in nature, because they suggest that our outcome suddenly flips/turns on at some level.\nHowever, even when our data do follow an S-shape, we can often still manipulate our variables to make a linear regression make sense. For extra practice and a challenge, I would like you to try manipulating the variable distance so that a linear regression will make sense and fit the data. Try doing this without looking at my answer first, and submit something of your own creation!\n\n\nOption 1: Click to show code and output\n\n\n&lt;div&gt; I am eyeballing distance = 1 as a point where the likelihood of rats changes sharply. I'll use this point to split our data. &lt;/div&gt;\n\nairbnb.data$distance_bin &lt;- ifelse(airbnb.data$distance &gt; 1, 1, 0)\nlinear_model_bin &lt;- lm(data = airbnb.data, rats ~ distance_bin)\nsummary(linear_model_bin)\n\n\nCall:\nlm(formula = rats ~ distance_bin, data = airbnb.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.86667 -0.03529 -0.03529 -0.03529  0.96471 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.86667    0.05611   15.45   &lt;2e-16 ***\ndistance_bin -0.83137    0.06086  -13.66   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2173 on 98 degrees of freedom\nMultiple R-squared:  0.6557,    Adjusted R-squared:  0.6522 \nF-statistic: 186.6 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\nggplot(data = airbnb.data, \n       aes( x = distance_bin, \n            y = rats)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  coord_cartesian(ylim = c(-.1,1.1))\n\n\n\n\n\n\n\nnew_data &lt;- data.frame(distance = c(0, 1, 6))\nnew_data$distance_bin &lt;- ifelse(new_data$distance &gt; 1, 1, 0)\npredict(object = linear_model_bin,\n        newdata = new_data,\n        type = \"response\") |&gt;\n  round(3)\n\n    1     2     3 \n0.867 0.867 0.035 \n\n\n\n\n\n\nOption 2: Click to show code and output\n\n\n&lt;div&gt; If we think there is a linear relationship in some subset of distance, but no relationship in another, we could create a variable that just cuts off distance when we think the relationship stops. &lt;/div&gt;\n\nairbnb.data$distance_cut &lt;- ifelse(airbnb.data$distance &gt; 1, 1, \n                                   airbnb.data$distance) # Here, we are keeping distances inside of 1\nlinear_model_cut &lt;- lm(data = airbnb.data, rats ~ distance_cut)\nsummary(linear_model_cut)\n\n\nCall:\nlm(formula = rats ~ distance_cut, data = airbnb.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.47527 -0.06613 -0.06613 -0.06613  0.93387 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.17150    0.09542   12.28   &lt;2e-16 ***\ndistance_cut -1.10536    0.10069  -10.98   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.248 on 98 degrees of freedom\nMultiple R-squared:  0.5515,    Adjusted R-squared:  0.5469 \nF-statistic: 120.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\nggplot(data = airbnb.data, \n       aes( x = distance_cut, \n            y = rats)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  coord_cartesian(ylim = c(-.1,1.1))\n\n\n\n\n\n\n\nnew_data &lt;- data.frame(distance = c(0, 1, 6))\nnew_data$distance_cut &lt;- ifelse(new_data$distance &gt; 1, 1, \n                                   new_data$distance)\npredict(object = linear_model_cut,\n        newdata = new_data,\n        type = \"response\") |&gt;\n  round(3)\n\n    1     2     3 \n1.171 0.066 0.066 \n\n\n\n\n\nPre-Class Q13\n\nThe whole point of the classes you have with me is learning how to make better predictions. That is especially true for logistic regression, where we are taking something we knew well (linear regression), and adding a lot of complexity.\nFor that complexity to be worth it, we have to be making better predictions. To see if we are, we can:\n\nSplit our data into training and testing sets\nTrain a regression on the training set\nMake predictions on the testing set\nSee how close those are to our observations\n\nIn this question, I want you to split your airbnb.data into a training set of 80 rows, and a testing set of 20. Then, train a logistic regression on the training set, and see how well it predicts the test data.\n\n\nClick to show code and output\n\n\n\nset.seed(123)\n#Shuffle airbnb.data\nairbnb.data &lt;- airbnb.data[sample(1:nrow(airbnb.data), size = nrow(airbnb.data)),]\ntrain_df &lt;- airbnb.data[1:80, ]\ntest_df &lt;- airbnb.data[81:100, ]\n\nlogistic_model &lt;- glm(data = train_df, rats ~ distance, family = \"binomial\")\nsummary(logistic_model)\n\n\nCall:\nglm(formula = rats ~ distance, family = \"binomial\", data = train_df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   3.0339     1.1768   2.578  0.00993 **\ndistance     -2.5553     0.8314  -3.074  0.00211 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 71.007  on 79  degrees of freedom\nResidual deviance: 26.341  on 78  degrees of freedom\nAIC: 30.341\n\nNumber of Fisher Scoring iterations: 8\n\ntest_df$prediction &lt;- predict(logistic_model,\n                              newdata = test_df,\n                              type = \"response\")\n\n# MSE\nmean((test_df$prediction - test_df$rats)^2)\n\n[1] 0.03454432\n\n# Make a \"confusion matrix\" by binarizing the DV\ntest_df$predicted_rats &lt;- ifelse(test_df$prediction &gt;= .5, \"Predict Rats\", \"Predict No Rats\")\ntable(test_df$predicted_rats, test_df$rats)\n\n                 \n                   0  1\n  Predict No Rats 16  0\n  Predict Rats     1  3\n\n\n\n\n\nPre-Class Q14\n\nNow, train a linear regression on the training set, and see how well it predicts the test data.\n\n\nClick to show code and output\n\n\n\nlinear_model &lt;- lm(data = train_df, rats ~ distance)\nsummary(linear_model)\n\n\nCall:\nlm(formula = rats ~ distance, data = train_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.45942 -0.22120 -0.03261  0.15540  0.73726 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.56308    0.06713   8.388 1.66e-12 ***\ndistance    -0.10996    0.01605  -6.853 1.50e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2952 on 78 degrees of freedom\nMultiple R-squared:  0.3758,    Adjusted R-squared:  0.3678 \nF-statistic: 46.96 on 1 and 78 DF,  p-value: 1.501e-09\n\ntest_df$prediction &lt;- predict(linear_model,\n                              newdata = test_df,\n                              type = \"response\")\n\n# MSE\nmean((test_df$prediction - test_df$rats)^2)\n\n[1] 0.06913223\n\n# Make a \"confusion matrix\" by binarizing the DV\ntest_df$predicted_rats &lt;- ifelse(test_df$prediction &gt;= .5, \"Predict Rats\", \"Predict No Rats\")\ntable(test_df$predicted_rats, test_df$rats)\n\n                 \n                   0  1\n  Predict No Rats 17  0\n  Predict Rats     0  3"
  },
  {
    "objectID": "teaching/03 Logistic Regression.html#footnotes",
    "href": "teaching/03 Logistic Regression.html#footnotes",
    "title": "03 - Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn this example, our hypothesis is that being further from North Beach leads to a decrease in the mention of “rats” in reviews. The null hypothesis is that distance from North Beach has no effect on the mention of rats.↩︎\nBecause our DV is either 0 or 1, the coefficient can be interpreted as a change in probability of the outcome with a one-unit change in the IV.↩︎"
  },
  {
    "objectID": "teaching/R_pcc.html",
    "href": "teaching/R_pcc.html",
    "title": "Intro to R",
    "section": "",
    "text": "In this semester, I am going to ask you to do a fair bit of work before coming to class. This will make our class time shorter, more manageable, and hopefully less boring.\nI am also going to use this as an opportunity for you to directly earn grade points for your effort/labor, rather than “getting things right” on an exam.\nTherefore, I will ask you to work through these worksheets before class. Throughout, I will post Pre-Class Questions for you to work through in R. These will look like this:\n\nIn R, please write code that will read in the .csv from Canvas called sf_listings_2312.csv. Assign this the name bnb.\nYou will then write your answer in a .r script:\n\n\nClick to show code and output\n\n\n\n# Q0\n#bnb &lt;- read.csv(\"sf_listings_2312.csv\")\n\n\n\n\n\nTo earn full points, you need to organize your code correctly. Specifically, you need to:\n\nAnswer questions in order.\n\nIf you answer them out of order, just re-arrange the code after.\n\nPreface each answer with a comment (# Q1/# Q2/# Q3) that indicates exactly which question you are answering.\n\nPlease just write the letter Q and the number in this comment.\n\nMake sure your code runs on its own, on anyone’s computer.\n\nTo do this, I would always include rm(list = ls()) at the top of every .r script. This will clean everything from the environment, allowing you to see if this runs on my computer.\n\n\n\n\n\n\nYou must submit this to Canvas before the start of class.\nYou must submit this code as a .txt file. This is because Canvas cannot present .R files to me in SpeedGrader. To save as .txt:\n\nClick File -&gt; New File -&gt; Text File\nCopy and paste your completed code to that new text file.\nSave the file as firstname_lastname_module.txt\n\nFor example, my file for Module 01 would be matt_meister_01.txt\nMy file for module 05 would be matt_meister_05.txt\n\n\n\n\n\n\n\nI will grade these for completion.\nYou will receive 1 point for every question you give an honest attempt to answer\nYour grade will be the number of questions you answer, divided by the total number of questions.\n\nThis is why it is important that you number each answer with # Q1.\nAny questions that are not numbered this way will be graded incomplete, because I can’t find them.\n\nYou will receive a 25% penalty for submitting these late."
  },
  {
    "objectID": "teaching/R_pcc.html#pre-class-code-assignment-instructions",
    "href": "teaching/R_pcc.html#pre-class-code-assignment-instructions",
    "title": "Intro to R",
    "section": "",
    "text": "In this semester, I am going to ask you to do a fair bit of work before coming to class. This will make our class time shorter, more manageable, and hopefully less boring.\nI am also going to use this as an opportunity for you to directly earn grade points for your effort/labor, rather than “getting things right” on an exam.\nTherefore, I will ask you to work through these worksheets before class. Throughout, I will post Pre-Class Questions for you to work through in R. These will look like this:\n\nIn R, please write code that will read in the .csv from Canvas called sf_listings_2312.csv. Assign this the name bnb.\nYou will then write your answer in a .r script:\n\n\nClick to show code and output\n\n\n\n# Q0\n#bnb &lt;- read.csv(\"sf_listings_2312.csv\")\n\n\n\n\n\nTo earn full points, you need to organize your code correctly. Specifically, you need to:\n\nAnswer questions in order.\n\nIf you answer them out of order, just re-arrange the code after.\n\nPreface each answer with a comment (# Q1/# Q2/# Q3) that indicates exactly which question you are answering.\n\nPlease just write the letter Q and the number in this comment.\n\nMake sure your code runs on its own, on anyone’s computer.\n\nTo do this, I would always include rm(list = ls()) at the top of every .r script. This will clean everything from the environment, allowing you to see if this runs on my computer.\n\n\n\n\n\n\nYou must submit this to Canvas before the start of class.\nYou must submit this code as a .txt file. This is because Canvas cannot present .R files to me in SpeedGrader. To save as .txt:\n\nClick File -&gt; New File -&gt; Text File\nCopy and paste your completed code to that new text file.\nSave the file as firstname_lastname_module.txt\n\nFor example, my file for Module 01 would be matt_meister_01.txt\nMy file for module 05 would be matt_meister_05.txt\n\n\n\n\n\n\n\nI will grade these for completion.\nYou will receive 1 point for every question you give an honest attempt to answer\nYour grade will be the number of questions you answer, divided by the total number of questions.\n\nThis is why it is important that you number each answer with # Q1.\nAny questions that are not numbered this way will be graded incomplete, because I can’t find them.\n\nYou will receive a 25% penalty for submitting these late."
  },
  {
    "objectID": "teaching/R_pcc.html#getting-started",
    "href": "teaching/R_pcc.html#getting-started",
    "title": "Intro to R",
    "section": "Getting Started",
    "text": "Getting Started\n\nR for Data Science\nThis book: https://r4ds.hadley.nz/ is a great tool for this class. I am going to require you to work through Chapters 9-11 by next class, but I think you should work through it all throughout this semester. If you get stuck on R in class, look back to this book!\nI will not ask specific test questions from this book, but it helped me immensely to learn R.\n\n\nMake sure you have R and RStudio downloaded\nIf you do not, go to this link https://www.r-project.org for R. Download whatever version matches your computer. Then, once this is downloaded, go to https://posit.co/download/rstudio-desktop/ and download RStudio. RStudio is an IDE (integrated development environment), which will help you to organize, run, and edit your code a lot.\n\n\nMy Notes/Code/Slides this semester\nI write my notes/slides/code in .qmd (called “Quarto”) documents. I will upload these as .html documents. .html files are webpages. You open them in your browser, but you do not need an internet connection to open them.\nI write my code in Quarto for four reasons:\n\nI want to be able to share my code with you\nI kind of like the organization of it all\nThis way, I don’t have to write both R scripts AND powerpoint (or these documents)\nThis gives me an excuse to have boring slides\n\nThere are some differences between .R and .qmd, which you only need to know if you download my .qmd files and are confused.\nIn an R script (.R), you can just write code in lines (i.e., nothing special to do). To run that code, you can either: (a) Hit cmd+enter (or cntrl+enter) to run the line of code you are on, or (b) Highlight a section of code and hit cmd/enter (or cntrl/enter). To “comment” (i.e., write text that is not code), you have to place a # at the start of each comment.\n\n#This comment will not run as code\n\nIn a Quarto document (.qmd), you write comments in line. This means that if you want to write code, you do so in chunks. You can create a chunk by: (a) Typing ```{r}, (b) Pressing option + cmd (cntrl) + i, or (c) Clicking Code &gt; Insert chunk. Within a chunk, you behave just like within an R script.\nFor yourselves right now, open a new .R script in RStudio. Do this by opening RStudio, then clicking File &gt; New File &gt; R Script. Then:\n\nMake a new folder on your computer called MSMI_603\n\nI’d prefer you put this in your Documents folder or on your Desktop\n\nFind cereals.csv on Canvas\n\nDownload it, and move it into the MSMI_603 folder\n\nSave this .R file into the MSMI_603 folder"
  },
  {
    "objectID": "teaching/R_pcc.html#starting",
    "href": "teaching/R_pcc.html#starting",
    "title": "Intro to R",
    "section": "StaRting",
    "text": "StaRting\nWe’re going to start by reading data into our workspace.\nFor this introduction to R, we will take the role of a Junior Market Researcher at Safeway Grocery Stores. You have two jobs:\n\nDescribe the relationship between customers’ rating and cereals’ calories\nDescribe how Safeway’s shelves are ordered\n\nTo read in data, we will first have to set our working directory. The working directory is a folder on our computers. Setting it tells R what folder we are: (a) Reading data from, (b) Saving data to, and (c) Saving code to. I repeat: Your working directory is a folder. There are three ways to set it in RStudio:\n\nOption A: Click Session -&gt; Set Working Directory -&gt; To Source File Location\n\nThis only works if your .R script is saved to the right folder\n\nOption B: Click Session -&gt; Set Working Directory -&gt; Choose Directory\n\nThen find the folder you want, and Open the folder\n\nOption C: Write setwd(~/Documents/MSMI_603) in the console (the bottom left panel in RStudio), and then hit enter\n\nIf you have saved this folder to your desktop, you’d write setwd(~/Desktop/MSMI_603)\n\n\n\nRead in cereals.csv\nWe are going to read our first data frame into R. Very exciting! To do so, please and run write the following code:\n\nread.csv(\"cereals.csv\")\n\nHere’s an explanation:\n\nread.csv()\n\nIs a function. We can tell because it is to the left of ().\nInside of the (), we write arguments, which tell the function specifically what to do.\nIt reads a .csv file into R.\nA .csv file is a spreadsheet.\n\n\"cereals.csv\"\n\nIs the file we want to read in.\nNote that you need the \"\", and you need the .csv\n\n\nIf you ran that code, you should have seen a bunch of numbers and letters pop up in your console. However, you won’t see that data anywhere else. So… What’s going on?\nread.csv() will read in a .csv, but it won’t do anything else unless we specifically tell R we want it to keep the data. This probably seems silly (why would we read it in if we didn’t want to use it?), but R will almost never assume things for us. We will usually have to tell R what we want explicitly every time, and in a lot of detail. This is going to be annoying at first, but it is what makes everything work.\nSo, we need to tell R we want to keep these data. We can do that by assigning the data to some name (similar to how naming a stray cat may make your parents more likely to let you keep it). We can do this with &lt;- or =.\n\nPre-Class Q1\n\nIn R, please read in the .csv from Canvas called cereals.csv. Assign this the name cereals.\n\n\nClick to show code and output\n\n\n\n# Q1\ncereals &lt;- read.csv(\"cereals.csv\")\n\n\n\nNote that when you assign something, it won’t print out into the console. For example, this will print out:\n\n1 + 1\n\n[1] 2\n\n\nBut this will not:\n\ntwo &lt;- 1 + 1 # add one and one\n\nWhen you assign a name to something, you are telling R that you want to use it later. So R doesn’t print it out. Instead, you will see the name of this object appear in the Environment pane in the top right of your RStudio. (Look there now! You should see cereals, as “75 obs. of 17 variables” because it has 75 rows and 17 columns).\n\n\nComments\nI mentioned comments above, and you may have noticed this #:\n\ntwo &lt;- 1 + 1 # add one and one\n\nThis is how we comment in R. Anything to the right of the # will be ignored by R. Anything to the left will run like code. This can be a very useful way to remind yourself (or team members) what some code does. Best practice is to comment a lot.\n\nPre-Class Q2\n\nIn R, please read in the .csv from Canvas called cereals.csv. Assign this the name cereals. This time, write a note to yourself that explains what this code does. Do not use the same comment as I do.\n\n\nClick to show code and output\n\n\n\n# Q2\ncereals &lt;- read.csv(\"cereals.csv\") # read in cereals.csv\n\n\n\nWhile we are here, note that you can also write code on multiple lines, as long as the lines above are an incomplete command:\n\ncereals &lt;- read.csv( # Use read.csv\n  \"cereals.csv\") # to bring this file in\n# Save it as \"cereals\"\n\n\nPre-Class Q3\n\nThis example does not work because the first line is complete. The second line is then incomplete, which throws an error. I would like you to change what I have to get something that will work.\n\n# Q3\n(1+1)\n/2\n\n\n\nBack to cereals\nIt is usually good to have a unique identifier (ID) for each unit of analysis in our data. In this case, the unit of analysis is a given cereal. You may notice that we do not have a unique ID yet. We have name, but there may be duplicates. We don’t know yet. So, let’s add a new column called id.\nI want id to be a unique number for each row in cereals. Here’s the worst way I could do this, which you should not do:\n\ncereals$id &lt;- c(1,2,3,4,5,6,7,8,9,\n        10,11,12,13,14,15,16,17,18,19,\n        20,21,22,23,24,25,26,27,28,29,\n        30,31,32,33,34,35,36,37,38,39,\n        40,41,42,43,44,45,46,47,48,49,\n        50,51,52,53,54,55,56,57,58,59,\n        60,61,62,63,64,65,66,67,68,69,\n        70,71,72,73,74,75)\n\nThis is terrible because it is so manual. There are a lot of things about R that are difficult, but we push through them so that we can avoid doing hard things manually. So we will figure out a better way. But first, here is an explanation of what that just did.\nIn R, data are often contained in vectors. Vectors are effectively like columns in an Excel file, or columns in a table. All of the columns in our data frame cereals are individual vectors. Therefore, to add a column, we need to first create a vector, and then attach it to cereals.\nWe can create vectors with a function, called c(). The c refers to “concatenate”. For example. to concatenate the numbers 1, 2, and 3 into a vector, you could write this:\n\nc(1, 2, 3)\n\n[1] 1 2 3\n\n\nTherefore, the code cereals$id &lt;- c(...) says:\n\nMake a vector that goes from 1 to 75\nThen assign that vector to the cereals data frame\nIn the place where column id is\n\nIf a column doesn’t exist, we create it\nIf it does exist, we overwrite it\n\n\nWhen we see a (, we know that the thing to the left is the name of a function, and the thing(s) inside of the () are arguments.\nWhen we see a $, we should know that the thing to the left is the name of a data.frame, and the thing to the right is the name of a column.1\nThis $ also works for accessing columns.\nc() works well when we have small vectors. But there are better ways to make longer vectors:\n\nseq() function\n\nseq refers to “sequence”\n\nrep() function\n\nrep refers to “repeat”\n\n\n\nPre-Class Q4\n\nCreate a sequence that goes from 1 to 3, and does so by 1:\n\nHint: Run ?seq() in your console to pull up help\n\n\n\nClick to show code and output\n\n\n\n#Q4\nseq(from=1, to=3, by=1)\n\n[1] 1 2 3\n\n\n\n\n\nPre-Class Q5\n\nRepeat the number 1 75 times:\n\nHint: Run ?rep() in your console to pull up help\n\n\n\nClick to show code and output\n\n\n\n#Q5\nrep(x = 1, times = 75)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[39] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\n\n\nNow, use this to create the id column\n\ncereals$id &lt;- seq(from=1, to=75, by=1)\n\nIn this case, you could do this simpler, with:\n\ncereals$id &lt;- 1:75\n\nWhich will work when you want sequential numbers that increase by 1.\n\n\nNesting functions and the order of operations\nA better solution is to use a function inside of seq() so that you don’t have to even know how many rows there are in cereals (again, we are trying to minimize manual effort). The function nrow() will take a data.frame and tell you how many rows it has.\n\ncereals$id &lt;- seq(from=1, to=nrow(cereals), by=1)\n\nWhat was that??\n\nR works by:\n\nStarting immediately right of the assignment arrow\nReading left to right\nBut following the order of operations\n\n“Create a sequence” (seq()\n\n“Start at 1” (from = 1)\n“End at nrow(cereals)” (nrow(cereals))\n\nWhat is that?? ?nrow()\n75!\n\n“Go by 1 at a time” (by = 1)\n“End the function” ())\n\n\nThe way I think of it is that R reads inside to out, left to right.\n\n\nFunctions\nFunctions are super useful. They also can be very easy to learn when you understand the rhythm/grammar of them.\nEach function in R takes some set of arguments. These tell the function what to do. We separate arguments with commas.\nFor example, seq() can take the arguments from, to, by, length.out, and along.with.\nWe’ll try some examples of using these arguments. Take note of what the different arguments do.\n\nseq( #Create a sequence\n  from=0, #starting at 0\n  to=10, #ending at 10\n  by=2) #by 2 (at a time)\n\n[1]  0  2  4  6  8 10\n\n\n\nseq( #Create a sequence\n  from=0, #starting at 0\n  by=2, #by 2 (at a time)\n  length.out=6) # Make it 6 items long\n\n[1]  0  2  4  6  8 10\n\n\n\nseq(#Create a sequence\n  from=0, #starting at 0\n  to=10, #ending at 10\n  length.out=6) # Make it 6 items long\n\n[1]  0  2  4  6  8 10\n\n\n\nseq( #Create a sequence\n  to=100, #To 10\n  by=2, #By 2s\n  length.out=6 ) # Make it 6 items long\n\n[1]  90  92  94  96  98 100\n\n\n\nseq( #Create a sequence\n  0, 10, 2 ) #???\n\n[1]  0  2  4  6  8 10\n\n\nYou might be curious why that last one worked. To find out, look at the help by running ?seq().\nIf you input the arguments in the “default” order, you don’t need to explicitly name them.\nIn my code, I write everything out. It’s much better if you ever share your code (including with your future self). Sometimes that’s super unnecessary. Sometimes it’s obvious what each argument is. Ultimately, this choice is up to you.\n\n\nWhy did I recommend using ?seq()?\nI also recommend you go to a function’s documentation online (Google it) if you have problems. For specific questions about specific functions, I do not recommend going to chatGPT first.\n\nCouple reasons\nMain: functions are updated constantly\n\nLLMs are not amazing at picking this up\nThis can give extremely confusing errors\n\nSecond:\n\nEven if a function has not been updated, a LLM does not know how it “works”\nIt knows how people have talked about it (and functions like it) on stackOverflow\nIt makes predictions of how the function works, based on some information it has seen\nThis also can cause some strange errors\n\n\nInstead, it is best to start with ? in R. I normally scroll to the bottom and look at examples. If this does not help, go to the function’s documentation (Google it). If this is hard to read, go to an LLM.\nAdditionally, if you have problems with a function you already know a bit, go to an LLM! They can do the last mile pretty well. But be ware of “wishful” coding, which is when GPT gives you code that looks correct, but is not.\n\n\nWhat I do recommend chatGPT for\nLLMs work by analysing a ton of data. When you ask an LLM a question, it predicts what words should go together in an answer, based on what words have appeared together in similar answers. This means that it doesn’t know anything–it predicts what a good answer would be.\nFor coding, this means that you need to ask narrow, direct questions (i.e., “I have this very specific problem, how can I solve it in R?”, or “I have this code that should work, but need help with this one bit”). You also need to give as much information as possible (i.e., “I am trying to plot group means using ggplot2 in R”). If your questions are too broad or don’t contain enough info, you are giving an LLM too much room. And at this point, you are not comfortable enough in R to know if it is wrong.\n\nPre-Class Q6\n\nPrint out the column sodium from cereals.\n\n\nClick to show code and output\n\n\n\n# Q6\ncereals$sodium\n\n [1] 130  15 140 200 180 125 210 200 210 220 290 210 140 180 280 290  90 180 140\n[20]  80 220 140 190 125 200   0 160 240 135  45 280 140 170  75 220 250 180 170\n[39] 170 260 150 180   0  95 150 150 220 190 220 170 170 200   0   0 135   0 210\n[58] 140   0 240 290   0   0   0  70 230  15 200 190 200 250 140 230 200 200\n\n\n\n\n\nPre-Class Q7\n\nUse the function mean to find the mean of the column calories.\n\n\nClick to show code and output\n\n\n\n# Q7\nmean(cereals$calories)\n\n[1] 107.4667\n\n\n\n\n\n\nPipes\nLet’s say I want to split these cereals into five groups, so I write:\n\nrep( #Repeat\n  seq( #A sequence\n    from=1, #Starting at 1\n    to=5, # Ending at 5\n    by=1), # By 1\n  times= nrow(cereals)/5 ) # As many times as we can\n\nThis may be a little confusing, because nested parentheses can get messy. Instead, we can use a pipe (|&gt;) to clean it up. A pipe PIPES the result of one function into another. You can read them as if they say “and then…”. Here’s what our code would look like with a pipe:\n\nseq( from=1, to=5, by=1) |&gt;\n  rep(times = nrow(cereals)/5 ) #Repeat as many times as we can\n\nEach line needs to be complete before the pipe, and the pipe has to end the line.\nLet’s assign that group as a column:\n\ncereals$group &lt;- seq( from=1, to=5, by=1) |&gt;\n  rep(times = nrow(cereals)/5 ) #Repeat as many times as we can\n\n\n\nNaming Objects\nThose names also didn’t mean anything. They can be almost anything.\nOver the next few weeks, see what works, what doesn’t.\nIn general:\n\nNames should be useful, readable (for someone else), and follow some common convention\n\nVariable names should be nouns and function names should be verbs.\nAvoid re-using names of common functions and variables.\nSeparate words with _, ., or capital letters\n\n\n\n\nData Types\nR has many different types of data. We’ll focus on:\n\nNumbers\nCharacters (strings)\nLogical\n\nYou can check what type something is with str().\n\nstr(cereals)\n\n'data.frame':   75 obs. of  19 variables:\n $ name        : chr  \"100% Bran\" \"100% Natural Bran\" \"All-Bran with Extra Fiber\" \"Almond Delight\" ...\n $ mfr         : chr  \"N\" \"Q\" \"K\" \"R\" ...\n $ type        : chr  \"C\" \"C\" \"C\" \"C\" ...\n $ calories    : int  70 120 50 110 110 110 130 90 90 120 ...\n $ protein     : int  4 3 4 2 2 2 3 2 3 1 ...\n $ fat         : int  1 5 0 2 2 0 2 1 0 2 ...\n $ sodium      : int  130 15 140 200 180 125 210 200 210 220 ...\n $ fiber       : num  10 2 14 1 1.5 1 2 4 5 0 ...\n $ carbo       : num  5 8 8 14 10.5 11 18 15 13 12 ...\n $ sugars      : int  6 8 0 8 10 14 8 6 5 12 ...\n $ potass      : int  280 135 330 -1 70 30 100 125 190 35 ...\n $ vitamins    : int  25 0 25 25 25 25 25 25 25 25 ...\n $ shelf       : int  3 3 3 3 1 2 3 1 3 2 ...\n $ weight      : num  1 1 1 1 1 1 1.33 1 1 1 ...\n $ cups        : num  0.33 1 0.5 0.75 0.75 1 0.75 0.67 0.67 0.75 ...\n $ rating      : num  68.4 34 93.7 34.4 29.5 ...\n $ sodiumPerCal: num  1.857 0.125 2.8 1.818 1.636 ...\n $ id          : num  1 2 3 4 5 6 7 8 9 10 ...\n $ group       : num  1 2 3 4 5 1 2 3 4 5 ...\n\n\n\n\nNumbers:\n\nstr(cereals$calories)\n\n int [1:75] 70 120 50 110 110 110 130 90 90 120 ...\n\n\n\n\nStrings (characters/text)\n\nstr(cereals$name)\n\n chr [1:75] \"100% Bran\" \"100% Natural Bran\" \"All-Bran with Extra Fiber\" ...\n\n\nQuotes (” or ’) make strings\n\naString &lt;- \"5\"\nstr(aString)\n\n chr \"5\"\n\n\n\n\nLogical\n\nTRUE or T for true\nFALSE or F for false.\n\nIf you don’t use all caps, you’ll get an error.\n\nlogical &lt;- T\nstr(logical)\n\n logi TRUE\n\n\nLogical data matters because it is the result of a logical test, like this:\n\ncereals$fat == 0\n\n [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE\n[13] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE\n[25]  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE\n[37]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[49] FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE\n[61]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n[73] FALSE FALSE FALSE\n\n\nThis code tests for each observation in cereals$fat, whether that observation is 0. This results in a logical vector that is TRUE if fat is 0. So, if I asked you to make a vector that indicated whether a cereal was fat free, you could just write:\n\ncereals$fat_free &lt;- cereals$fat == 0\n\nTo finish the pre-class code, make two new columns in cereals. First,\n\nPre-Class Q8\n\nlow_cal that is TRUE if calories is less than 100\n\n\nClick to show code and output\n\n\n\n# Q8\ncereals$low_cal &lt;- cereals$calories &lt; 100\n\n\n\n\nPre-Class Q9\n\nnot_shelf3 that is TRUE if shelf is NOT 3\n\n\nClick to show code and output\n\n\n\n# Q9\ncereals$not_shelf3 &lt;- cereals$shelf != 3"
  },
  {
    "objectID": "teaching/R_pcc.html#footnotes",
    "href": "teaching/R_pcc.html#footnotes",
    "title": "Intro to R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLater in the semester, this will not be exactly true. Sometimes, the thing to the left will be the name of a list, and the thing to the right an item in that list. We will make this more specific later.↩︎"
  },
  {
    "objectID": "teaching/01 Prediction.html",
    "href": "teaching/01 Prediction.html",
    "title": "01 - Prediction",
    "section": "",
    "text": "In this semester, I am going to ask you to do a fair bit of work before coming to class. This will make our class time shorter, more manageable, and hopefully less boring.\nI am also going to use this as an opportunity for you to directly earn grade points for your effort/labor, rather than “getting things right” on an exam.\nTherefore, I will ask you to work through the posted slides on Canvas before class. Throughout the document, I will post Pre-Class Questions for you to work through in R. These will look like this:\n\nIn R, please write code that will read in the .csv from Canvas called sf_listings_2312.csv. Assign this the name bnb.\nYou will then write your answer in a .r script:\n\n\nClick to show code and output\n\n\n\n# Q1\nbnb &lt;- read.csv(\"sf_listings_2312.csv\")\n\n\n\n\n\nTo earn full points, you need to organize your code correctly. Specifically, you need to:\n\nAnswer questions in order.\n\nIf you answer them out of order, just re-arrange the code after.\n\nPreface each answer with a comment (# Q1/# Q2/# Q3) that indicates exactly which question you are answering.\n\nPlease just write the letter Q and the number in this comment.\n\nMake sure your code runs on its own, on anyone’s computer.\n\nTo do this, I would always include rm(list = ls()) at the top of every .r script. This will clean everything from the environment, allowing you to see if this runs on my computer.\n\n\n\n\n\n\nYou must submit this to Canvas before 9:00am on the day of class. Even if class starts at 10:00am that day, these are always due at 9:00.\nYou must submit this code as a .txt file. This is because Canvas cannot present .R files to me in SpeedGrader. To save as .txt:\n\nClick File -&gt; New File -&gt; Text File\nCopy and paste your completed code to that new text file.\nSave the file as firstname_lastname_module.txt\n\nFor example, my file for Module 01 would be matt_meister_01.txt\nMy file for module 05 would be matt_meister_05.txt\n\n\n\n\n\n\n\nI will grade these for completion.\nYou will receive 1 point for every question you give an honest attempt to answer\nYour grade will be the number of questions you answer, divided by the total number of questions.\n\nThis is why it is important that you number each answer with # Q1.\nAny questions that are not numbered this way will be graded incomplete, because I can’t find them.\n\nYou will receive a 25% penalty for submitting these late.\nI will post my solutions after class."
  },
  {
    "objectID": "teaching/01 Prediction.html#pre-class-code-assignment-instructions",
    "href": "teaching/01 Prediction.html#pre-class-code-assignment-instructions",
    "title": "01 - Prediction",
    "section": "",
    "text": "In this semester, I am going to ask you to do a fair bit of work before coming to class. This will make our class time shorter, more manageable, and hopefully less boring.\nI am also going to use this as an opportunity for you to directly earn grade points for your effort/labor, rather than “getting things right” on an exam.\nTherefore, I will ask you to work through the posted slides on Canvas before class. Throughout the document, I will post Pre-Class Questions for you to work through in R. These will look like this:\n\nIn R, please write code that will read in the .csv from Canvas called sf_listings_2312.csv. Assign this the name bnb.\nYou will then write your answer in a .r script:\n\n\nClick to show code and output\n\n\n\n# Q1\nbnb &lt;- read.csv(\"sf_listings_2312.csv\")\n\n\n\n\n\nTo earn full points, you need to organize your code correctly. Specifically, you need to:\n\nAnswer questions in order.\n\nIf you answer them out of order, just re-arrange the code after.\n\nPreface each answer with a comment (# Q1/# Q2/# Q3) that indicates exactly which question you are answering.\n\nPlease just write the letter Q and the number in this comment.\n\nMake sure your code runs on its own, on anyone’s computer.\n\nTo do this, I would always include rm(list = ls()) at the top of every .r script. This will clean everything from the environment, allowing you to see if this runs on my computer.\n\n\n\n\n\n\nYou must submit this to Canvas before 9:00am on the day of class. Even if class starts at 10:00am that day, these are always due at 9:00.\nYou must submit this code as a .txt file. This is because Canvas cannot present .R files to me in SpeedGrader. To save as .txt:\n\nClick File -&gt; New File -&gt; Text File\nCopy and paste your completed code to that new text file.\nSave the file as firstname_lastname_module.txt\n\nFor example, my file for Module 01 would be matt_meister_01.txt\nMy file for module 05 would be matt_meister_05.txt\n\n\n\n\n\n\n\nI will grade these for completion.\nYou will receive 1 point for every question you give an honest attempt to answer\nYour grade will be the number of questions you answer, divided by the total number of questions.\n\nThis is why it is important that you number each answer with # Q1.\nAny questions that are not numbered this way will be graded incomplete, because I can’t find them.\n\nYou will receive a 25% penalty for submitting these late.\nI will post my solutions after class."
  },
  {
    "objectID": "teaching/01 Prediction.html#prediction",
    "href": "teaching/01 Prediction.html#prediction",
    "title": "01 - Prediction",
    "section": "Prediction",
    "text": "Prediction\nLoad in these packages. If you do not have them, you will need to install them.\n\ne.g., install.packages(\"tidytext\")\n\n\n\nClick to show code and output\n\n\n\nlibrary(tidytext)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n\nAnd read in these data:\n\n\nClick to show code and output\n\n\n\nbnb &lt;- read.csv(\"sf_listings_2312.csv\")\n\n\n\n\nWhy are we doing this?\nThe goal of data analysis/science is to make predictions about the future.\nWe do not just analyze data to find some neat facts about Airbnb properties (for example). We analyze data to understand how the world works. Because that will give us our best guess about how the world will work in the future.\nAnd we have some control over the future! The things we discover about the past are helpful only because they can inform the choices we make for the future.\n\nFor example:\nOne of the common praises of Airbnbs is that they are often larger than hotels, allowing friends to stay together. If it is true (that staying together is a benefit of Airbnbs), we should find that Airbnbs that accommodate more people get higher ratings than Airbnbs who accommodate fewer people.\n\nPre-Class Q2\n\nIn R, test with linear regression whether listings that accommodate more people receive higher ratings.\nNote: The variable for average rating is called review_scores_rating.\n\n\nClick to show code and output\n\n\n\nsummary(lm(data = bnb, review_scores_rating ~ accommodates))\n\n\nCall:\nlm(formula = review_scores_rating ~ accommodates, data = bnb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8139 -0.0364  0.1267  0.2130  0.2936 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.679486   0.010714 436.767   &lt;2e-16 ***\naccommodates 0.026885   0.002819   9.537   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4394 on 6173 degrees of freedom\n  (1881 observations deleted due to missingness)\nMultiple R-squared:  0.01452,   Adjusted R-squared:  0.01436 \nF-statistic: 90.95 on 1 and 6173 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nPre-Class Q3\n\nDo listings who accommodate more people receive higher ratings? Answer this in a comment.\n\n\nClick to show code and output\n\n\n\n# Yes. The coefficient is &gt; 0, and the p-value is very low. This suggests that places who accommodate more people receive significantly higher ratings.\n\n\n\nThis result tells us something about the future because the p-value indicates that the relationship between review_scores_rating and accommodates is stronger than we would expect from chance alone. Therefore, it is not due to randomness that larger places received higher ratings.\nOne condition that we have to satisfy in order to use past data to predict the future is that the results we observe must not be due to chance. Because chance is random. This is easy to test by looking at p-values. We have just done this for a single variable (accommodates), but we can also do it for multiple variables."
  },
  {
    "objectID": "teaching/01 Prediction.html#model-comparison",
    "href": "teaching/01 Prediction.html#model-comparison",
    "title": "01 - Prediction",
    "section": "Model Comparison",
    "text": "Model Comparison\nThe world is a complicated place. While we often care about individual variables, it is more common that we want to create (semi-) complex models, which use multiple variables to predict an outcome. To know whether adding this complexity is worth it, or which of a series of models will best predict the future, we have to compare those models.\nWe will cover two forms of model comparison. First, using anova(). Second, splitting our data into training and testing sets.\n\nAnova\nAnova is an acronym for analysis of variance. It takes two models, and analyzes the degree to which the variance reduced by the more complex one is larger than chance.\nAdvantages:\n\nRelatively simple to code and run\nRequires less data than training/testing sets\nProvides a p-value, which is familiar\n\nDisadvantages\n\nNot very common anymore (because we have more data than we used to)\nCannot compare models of equal complexity\n\ne.g., Cannot compare rating ~ price to rating ~ accommodates\n\n\nThat second disadvantage is pretty substantial. As a result, you won’t see this as much.\nIf I wanted to see whether a model with accommodates and price predicted review_scores_rating better than a model with accommodates alone, I could do the following.\nFirst, let’s look at the variables we have:\n\n\nClick to show code and output\n\n\n\nhist(bnb$review_scores_rating)\n\n\n\n\n\n\n\nhist(bnb$accommodates)\n\n\n\n\n\n\n\nhist(bnb$price)\n\nError in hist.default(bnb$price): 'x' must be numeric\n\n\n\n\nWhoa! price threw an error. This is because it is not numeric. It has that annoying $ before it, and also has a , when prices are in the thousands. We’ll need to remove those with the following code:\n\nPre-Class Q4\n\nJust copy this into R.\n\n\nClick to show code and output\n\n\n\n# First, remove the dollar sign\nbnb$price &lt;- str_remove(bnb$price, \"\\\\$\")\n# You need the \"\\\\\" because $ is a special character with regular expressions, but we just want the symbol gone.\n\n# Second, remove the comma\nbnb$price &lt;- str_remove(bnb$price, \",\")\n\n# Third, make this a number\nbnb$price &lt;- as.numeric(bnb$price)\n\n\n\n\nPre-Class Q5\n\nSee whether a model with accommodates and price predicted review_scores_rating better than a model with accommodates alone.\n\n\nClick to show code and output\n\n\n\nmodel_simple &lt;- lm(data = bnb, review_scores_rating ~ accommodates)\nmodel_complex &lt;- lm(data = bnb, review_scores_rating ~ accommodates + price)\nanova(model_simple, model_complex)\n\nAnalysis of Variance Table\n\nModel 1: review_scores_rating ~ accommodates\nModel 2: review_scores_rating ~ accommodates + price\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   6173 1192.0                                  \n2   6172 1185.8  1     6.181 32.171 1.476e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nThat p-value being extremely small tells me that the variance reduced by the more complex model was well worth the added complexity. This should make us confident that using the complex model will lead to better predictions in the future than using the simpler model.\n\n\nTrain/Test Splits\nAnother way we can do this model comparison is by splitting our data into two sets (training and testing), and seeing how well each model that we fit to the training set predicts the testing set.\nAdvantages:\n\nSuper common (especially in machine learning)\nVery flexible–can use different variables, models, etc\nRather than a p-value, we can directly compare the error between models\n\nThis is because the p-value is approximating what we are actually doing here\n\n\nDisadvantages\n\nNeeds more data to be reliable\nMore complex to code\n\n\nFirst: split our data into a training and a testing set\n\nUsually, the training set is 70%-80% of the data.\nWe do this by randomly sampling our data with sample().\n\nWe should randomly sample the id, as this is unique to each row.\n\nBecause there is randomness, we will use set.seed() to get consistent results.\n\n\nPre-Class Q6\n\nCreate splits of bnb into training and testing data.\n\n\nClick to show code and output\n\n\n\nset.seed(28)\nsamp_size &lt;- round(.7 * nrow(bnb)) # I am rounding this to get a whole number\ntrain &lt;- sample(bnb$id, size = samp_size, replace = FALSE)\n\nbnb_train &lt;- bnb[bnb$id %in% train,] # Index bnb where the id is inside train\nbnb_test &lt;- bnb[!(bnb$id %in% train),] # Index bnb where the id is NOT inside train\n\n\n\n\n\nSecond: train each model on the training set\n\nPre-Class Q7\n\n\n\nClick to show code and output\n\n\n\nmodel_simple_train &lt;- lm(data = bnb_train, review_scores_rating ~ accommodates)\nmodel_complex_train &lt;- lm(data = bnb_train, review_scores_rating ~ accommodates + price)\n\n\n\nNote: You need to use bnb_train as the data, not bnb. Also, if you look at model_simple and model_simple_train, you will see the coefficients are slightly different. Think of why this is.\n\n\nThird: test each model on the testing set.\n\nPre-Class Q8\n\nDo this by:\n\nMaking predictions about review_scores_rating from each model.\nCalculating the squared error for each model.\nComparing those numbers.\n\n\n\nClick to show code and output\n\n\n\n#a) predictions\nbnb_test$prediction_simple &lt;- predict(object = model_simple_train,\n                             newdata = bnb_test, \n                             type = \"response\")\n\nbnb_test$prediction_complex &lt;- predict(object = model_complex_train,\n                             newdata = bnb_test, \n                             type = \"response\")\n\n#b) calculate error\nbnb_test$error_simple &lt;- (bnb_test$prediction_simple - bnb_test$review_scores_rating)^2\n\nbnb_test$error_complex &lt;- (bnb_test$prediction_complex - bnb_test$review_scores_rating)^2\n\n#c) compare error\nmean(bnb_test$error_simple, na.rm = T)\n\n[1] 0.2002133\n\nmean(bnb_test$error_complex, na.rm = T)\n\n[1] 0.1999402\n\n\n\n\n\npredict() spits out a prediction for each row in the new data when we specify type = \"response\".\n\nThis is why we can just attach it as a column.\n\n\nIn this case, because we are actually making predictions, we do not need p-values. We can just look at the reduction in error itself.\nOne thing you will notice is that we barely reduced any error from the average prediction. If that seems odd, you can try different things:\n\nCross-validation (described here)\n\nThis repeats the process we just did (splitting, training, testing) multiple times on the same set of data, potentially reducing the impact of randomness in our sampling\n\nVariable selection\n\nprice is not normally distributed at all. So we may be getting some weirdness in the results as a result. You could see how results change if you transform price somehow."
  },
  {
    "objectID": "teaching/01 Prediction.html#actually-making-predictions",
    "href": "teaching/01 Prediction.html#actually-making-predictions",
    "title": "01 - Prediction",
    "section": "Actually Making Predictions",
    "text": "Actually Making Predictions\nThe simple model (from the anova() example) predicts review_scores_rating as the following function:\n\nreview_scores_ratingi = 4.68 + 0.027 \\(\\times\\) accommodatesi\n\nIn this function, \\(i\\) denotes any individual listing.\n\n\nThe complex model predicts review_scores_rating as the following function:\n\nreview_scores_ratingi = 4.68 + 0.027 \\(\\times\\) accommodatesi + -0.000016 \\(\\times\\) pricei\n\n\nWhy does this matter?\nWith the simple model, we predict that the cheapest and the most expensive 4-person Airbnb would have the exact same ratings.\n\nPre-Class Q9\n\nSee these predictions with predict(). To use predict(), we need to create a new data.frame that has the exact same variables as the variables in our models.\n\n\nClick to show code and output\n\n\n\n# Minimum price for a four-person bnb\nmin_fourperson &lt;- min(bnb[bnb$accommodates == 4, ]$price)\n# Maximum price for a four-person bnb\nmax_fourperson &lt;- max(bnb[bnb$accommodates == 4, ]$price)\n\nnew_data &lt;- data.frame(\n  accommodates = c(4, 4),\n  price = c(min_fourperson, max_fourperson)\n)\n\npredict(object = model_simple, #Predict takes an \"object\" (a model)\n        newdata = new_data, # and it takes \"newdata\"\n        type = 'response') \n\n       1        2 \n4.787026 4.787026 \n\n\n\n\n\nPre-Class Q10\n\nWith the complex model, we do not predict that the cheapest and the most expensive 4-person Airbnb would have the exact same ratings. On your own, I want you to make those predictions for the complex model.\n\nMaking variables behave\nNow, if you look at the distribution of price, you will see that it is a bit… crazy. For example, the maximum price of a four-person Airbnb is $50,000 per night!\nYou will probably get better predictions if you enforce some normality on price. On your own, I would like you to:\n\nPre-Class Q11\n\n\nCreate a new variable called bnb$log_price, which will be the natural log of bnb$price (Google this).\nCreate a new model called model_complex_log, using log_price in place of price.\nUse `anova() to compare the simple model to this new complex model. Is it still better?\n\nYou will probably get better predictions if you enforce some normality on price."
  },
  {
    "objectID": "teaching/01 Prediction.html#final-questions",
    "href": "teaching/01 Prediction.html#final-questions",
    "title": "01 - Prediction",
    "section": "Final Questions",
    "text": "Final Questions\nThese questions are for added points/difficulty. They count towards the total score.\n\nPre-Class Q12\n\nYou are not going to be able to test the original model_complex against model_complex_log using anova(). But, you can test these two using the training/testing example from earlier. Try to do that here.\n\nPre-Class Q13\n\nI would like you to challenge yourself here, by attempting to test a totally new set of models. Predict review_scores_rating with new variables, using both anova() and training/testing splits."
  },
  {
    "objectID": "teaching/01 Prediction.html#in-class",
    "href": "teaching/01 Prediction.html#in-class",
    "title": "01 - Prediction",
    "section": "In Class",
    "text": "In Class\nIn class, we are going to try out making predictions of review_scores_rating using whatever variables we want. You will then hand in your best models as your homework.\nIf you just use the variables that are already in the data set, you will struggle. Instead, I suggest creating new ones, cleaning the ones you have, etc.\nFor example, you might want to distinguish places that accommodate more than four people categorically:\n\n\nClick to show code and output\n\n\n\nbnb$accommodates_5plus &lt;- ifelse(bnb$accommodates &gt; 4, 1, 0)\n\n\n\nAnd then see if this predicts ratings:\n\n\nClick to show code and output\n\n\n\nmodel_5plus &lt;- lm(data = bnb, review_scores_rating ~ accommodates_5plus)\nsummary(model_5plus)\n\n\nCall:\nlm(formula = review_scores_rating ~ accommodates_5plus, data = bnb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8582 -0.0361  0.1339  0.2439  0.2539 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        4.746147   0.006202 765.315  &lt; 2e-16 ***\naccommodates_5plus 0.112090   0.014503   7.728 1.26e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4405 on 6173 degrees of freedom\n  (1881 observations deleted due to missingness)\nMultiple R-squared:  0.009583,  Adjusted R-squared:  0.009423 \nF-statistic: 59.73 on 1 and 6173 DF,  p-value: 1.262e-14\n\n\n\n\nYou might also want to pull out some specific amenity. Any time you are handling text, make it lower case!\n\n\nClick to show code and output\n\n\n\nbnb$amenities &lt;- tolower(bnb$amenities)\nbnb$kitchen &lt;- ifelse(str_detect(bnb$amenities, 'kitchen'), 1, 0)"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#outline",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#outline",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Outline",
    "text": "Outline\n\n\nHypotheses and Predictions\n\nImportance of formulating clear hypotheses\nExamples of clear hypotheses in market research\n\nTests of Hypotheses\n\nA weighted coin?\nSimulation\np-values"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#what-is-a-hypothesis-1",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#what-is-a-hypothesis-1",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "What is a hypothesis?",
    "text": "What is a hypothesis?\nIn speaking terms:\n\nA hypothesis is a specific prediction about the world.\nThis prediction can be about the past or future\n\ni.e., People bought product y for reason x\nor People will buy product y if x is true\n\nExamples?"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#what-is-a-hypothesis-2",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#what-is-a-hypothesis-2",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "What is a hypothesis?",
    "text": "What is a hypothesis?\nIn statistical terms:\n\nA hypothesis is a specific prediction about how variables behave.\nEither:\n\nHow one variable behaves (prediction)\nHow multiple variables relate to eachother (hypothesis)\n\nCompared to chance"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#why-are-clear-hypotheses-important",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#why-are-clear-hypotheses-important",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Why are clear hypotheses important?",
    "text": "Why are clear hypotheses important?\nYour hypotheses guide the rest of the research process. Ideally:\n\n\nHypotheses inform research design, including:\n\nData collection\n\nWhat kind of data collection is not affected by our hypotheses?\n\nSecondary, when the data already exist\n\n\nData manipulation/wrangling\n\nWhat data we include, how we summarize it\n\nVisualization\n\nWhat we want to look for in the data\n\nStatistical testing\n\nDoes the data tell us anything important?"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#examples-of-unclear-hypotheses",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#examples-of-unclear-hypotheses",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Examples of unclear hypotheses?",
    "text": "Examples of unclear hypotheses?\n\n\nPeople like shoes\n\nNo clear comparison, no defined scope/setting\n\nPeople give shoes high ratings\n\nAgain, not a clear comparison. What is high?\nMore clear?\nPeople rate shoes higher than 3/5 on average"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#hypothesis-testing",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#hypothesis-testing",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\nWhy do we make predictions?\n\nBecause we want to understand how the world works\nAs marketers, we want to understand what information is worth knowing about products, customers, markets, etc.\nBecause if we understand what is worth knowing, we can understand:\n\nWhat data to collect\nWho/where to target\nWhat new offerings may be viable\netc.\n\n\nSo when we formulate hypotheses and run tests, think:\n\nWhat information may be worth knowing? (Hypotheses)\nIs that information worth knowing? (Tests)"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#hypothesis-testing-the-role-of-chance",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#hypothesis-testing-the-role-of-chance",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Hypothesis Testing: The Role of “Chance”",
    "text": "Hypothesis Testing: The Role of “Chance”\n\n\nAll statistical tests are in some way comparing observed data to random chance.\nIncluding simple examples:\n\nIs this coin “fair”?\n\nFlip the coin a number of times\nCount heads and tails\nTest if heads come up roughly 50% of the time\n\n\nAnd more complicated ones\n\nDo customers prefer product A or product B?\n\nCollect many responses for both products\nSummarize them in some way\nCompare this result to a “random draw”"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#tests-of-hypotheses-1",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#tests-of-hypotheses-1",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Tests of Hypotheses",
    "text": "Tests of Hypotheses\nLet’s start with the simpler example of coin flips. Imagine you are a kid (again?)\n\n\nYou and your sibling are responsible for doing the dishes\nInstead of just taking turns, your sibling suggests a gamble: Each night they will flip a coin.\nHEADS, you do the dishes. TAILS, they do the dishes.\nAfter three weeks, you’ve done the dishes 16 times and your sibling only 5 times.\nYou suspect the coin is… RIGGED (!)\nBut how can you be certain?\nCan you be certain?"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#the-null-hypothesis",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#the-null-hypothesis",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "The Null Hypothesis",
    "text": "The Null Hypothesis\nYour hypothesis: THE COIN IS RIGGED!!. Specifically:\n\nHEADS is significantly more likely to come up than TAILS.\n\nHowever, the thing you can actually test is the null hypothesis.\n\n\nThe null hypothesis is the complement of your hypothesis:\n\n\n\n\nH0\n\nThe coin is equally likely to land on HEADS as it was on TAILS.\n\nIn most cases, including this one, the null hypothesis is that any differences you observed between outcomes were the result of chance.\n\nWhy is it that the null hypothesis, but not your actual hypothesis, is what we can test?\n\n\n\nThe null hypothesis provides a specific model for the state-of-the world.\n\nThat any given coin flip has a 50% chance of landing on HEADS and a 50% chance of landing on TAILS.\n\nThe actual hypothesis (that the coin is rigged) is more vague.\n\nIs HEADS 60% likely? 70%? Do you even care?"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#is-the-data-consistent-with-the-null-hypothesis",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#is-the-data-consistent-with-the-null-hypothesis",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Is the Data Consistent with the Null Hypothesis?",
    "text": "Is the Data Consistent with the Null Hypothesis?\nAssess whether the data we observed (16 HEADS out of 21 flips) is consistent with the null hypothesis.\n\nIs this a result that would likely arise by chance alone?\n\nIf not, that would constitute evidence of foul play (and support of our actual hypothesis).\n\nWho thinks this result constitutes evidence of foul play?\n\n16/21 = ~76% of flips\n\n\n\nNot so fast!!\n\n\n\nImagine flipping a (truly fair) coin twice.\nFour possible outcomes, all equally likely:\n\nHEADS-HEADS\nHEADS-TAILS\nTAILS-HEADS\nTAILS-TAILS\n\nWould it be suspicious if I got HEADS-HEADS?"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#is-the-data-consistent-with-the-null-hypothesis-1",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#is-the-data-consistent-with-the-null-hypothesis-1",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Is the Data Consistent with the Null Hypothesis?",
    "text": "Is the Data Consistent with the Null Hypothesis?\nWould it be suspicious if I got HEADS-HEADS?\n\nProbably not, as this happens 1/4 times in expectation\n\n\n\nWe take this logic further with hypothesis testing:\n\nIs it possible to flip a perfectly fair coin 21 times and get 16 HEADS?\n\nYes!\n\nIt’s even possible to flip 21 HEADS.\nSo we can never be certain–from data alone about the truth of our hypothesis.\nInstead, we can quantify how likely any given outcome would be if the null hypothesis is true."
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#quantifying-likelihood-under-the-null-hypothesis",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#quantifying-likelihood-under-the-null-hypothesis",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Quantifying likelihood under the null hypothesis",
    "text": "Quantifying likelihood under the null hypothesis\nTesting the Null Hypothesis with Simulation\nOne approach is to use theoretical math.\n\nIn a way, I did this when I said that the likelihood of flipping two HEADS in two flips is 25%.\n\nAnother approach is to use simulation.\n\nSimulate a lot of outcomes given the assumption of the null hypothesis\n\ne.g., each toss is equally likely to be HEADS or TAILS)\n\nWe “flip” 21 fair coins many, many times\n\nEach time counting the number of HEADS out of the 21 flips.\n\nWe call each set of 21 flips a simulated sample\n\nWe’ll generate a lot of simulated samples\n\nThen, we assess the likelihood of any given result by examining how frequently that result occurred over the entirety of the simulated samples"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#quantifying-likelihood-under-the-null-hypothesis-1",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#quantifying-likelihood-under-the-null-hypothesis-1",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Quantifying likelihood under the null hypothesis",
    "text": "Quantifying likelihood under the null hypothesis\n\nWith simulation\nLet’s flip some coins!"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#todays-programming",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#todays-programming",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Today’s pRogramming",
    "text": "Today’s pRogramming\nWe are going to dive into R.\nWe’ll just use it to do stuff.\nThere is extra R help on Canvas:\n\nR Help Module\n\nIntro to R.html & Plotting.html"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#quantifying-likelihood-under-the-null-hypothesis-2",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#quantifying-likelihood-under-the-null-hypothesis-2",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Quantifying likelihood under the null hypothesis",
    "text": "Quantifying likelihood under the null hypothesis\nTesting the Null Hypothesis with Simulation\nCreate a “coin”\n\naCoin &lt;- c(\"HEADS\", \"TAILS\") # create a \"coin\"\n\n\n\nWhen R sees an assignment arrow (&lt;-), it will evaluate the code right of the arrow, and save it as the name left of the arrow\nThe c() bit is a function\n\nAnytime you see parentheses in R, we are calling a function\nThe name of the function is left of the (), the arguments are inside\nHere we are saying “concatenate ‘HEADS’ and ‘TAILS’ into a vector”"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#terminology-alert",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#terminology-alert",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "TERMINOLOGY ALERT",
    "text": "TERMINOLOGY ALERT\n\nFunction:\n\nA bit of R code that will perform some task we want to repeat\nAnytime you see parentheses in R, we are calling a function\nThe name of the function is left of the (), the arguments are inside\nHere we are saying “concatenate ‘HEADS’ and ‘TAILS’ into a vector”\n\nVector:\n\nLike a column in Excel\nA column of numbers/words/characters/etc"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#quantifying-likelihood-under-the-null-hypothesis-3",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#quantifying-likelihood-under-the-null-hypothesis-3",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Quantifying likelihood under the null hypothesis",
    "text": "Quantifying likelihood under the null hypothesis\nUse sample() to simulate flipping the coin.\n\n\nsample( aCoin, \n        size=1, \n        replace=TRUE ) # \"flip\" it once\n\n[1] \"HEADS\"\n\n\n\nRun this a few times in R. It should be generating HEADS or TAILS with equal likelihood."
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#terminology-alert-1",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#terminology-alert-1",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "TERMINOLOGY ALERT",
    "text": "TERMINOLOGY ALERT\n\n\nArguments:\n\nThe things inside of the () in an R function\nThese tell the function what you want\nArguments often have defaults\n\nTry running sample( aCoin, size=1)\nThe default is not to replace\nWhen we simulate things, we replace so that the distribution of any one draw stays consistent\n\n\nComments (#)\n\nThese let you write text to yourself and others in your code\nR stops reading a line when it hits one\nAnything left of a comment runs\nAnything right does not"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#quantifying-likelihood-under-the-null-hypothesis-4",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#quantifying-likelihood-under-the-null-hypothesis-4",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Quantifying likelihood under the null hypothesis",
    "text": "Quantifying likelihood under the null hypothesis\nUse sample() to simulate flipping the coin\nThen modify the code a bit to generate and store 21 flips of the coin.\n\n\naCoin &lt;- c(\"HEADS\", \"TAILS\") # create a \"coin\"\ntwentyOneFlips &lt;- sample( aCoin, size=21, replace=TRUE ) # \"flip\" it 21 times\nprint( twentyOneFlips ) # print the flips\n\n [1] \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\"\n[10] \"HEADS\" \"HEADS\" \"TAILS\" \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\"\n[19] \"HEADS\" \"TAILS\" \"TAILS\""
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#pause",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#pause",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Pause",
    "text": "Pause\n\n\nWe’re seeing a lot of new functions today\nIt might seem like you should memorize them and their arguments\nDon’t\nThere are too many. Instead, focus on the logic/grammar\n\nFirst comes the name\nThen the ( opens it all up\nThen I give arguments with argument = what I want\nThen I close it off with )\n\nIf you get stuck, Google things\n\nI don’t recommend using chatGPT for these classes\nIt’s going to give you some crazy shit, and works when you know how to give a good prompt"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#quantifying-likelihood-under-the-null-hypothesis-5",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#quantifying-likelihood-under-the-null-hypothesis-5",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Quantifying likelihood under the null hypothesis",
    "text": "Quantifying likelihood under the null hypothesis\nCount how many heads you got\n\n\nnHeads &lt;- sum( twentyOneFlips == \"HEADS\" ) # count the number of HEADS out of 21\nprint( nHeads ) # print the number of heads\n\n[1] 12"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#quantifying-likelihood-under-the-null-hypothesis-6",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#quantifying-likelihood-under-the-null-hypothesis-6",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Quantifying likelihood under the null hypothesis",
    "text": "Quantifying likelihood under the null hypothesis\nTesting the Null Hypothesis with Simulation\nNow, do this a whole bunch of times.\n\nUsing a loop\n\nIn each iteration, the loop will create a single simulated sample:\n\n\n\n# setting up the simulation\nnSims &lt;- 10000 # number of simulations\naCoin &lt;- c(\"HEADS\", \"TAILS\") # create a \"coin\"\nnHeads &lt;- rep(NA, times=nSims) # an empty vector to hold simulation results\n# loop to conduct simulation\nfor (i in 1:nSims) {\ntwentyOneFlips &lt;- sample( aCoin, size=21, replace=TRUE ) # flip 21 coins\nnHeads[i] &lt;- sum( twentyOneFlips == \"HEADS\" ) # count and store the number of HEADS\n}\n\nYou may notice there is no “output” for this simulation (i.e., nothing was printed).\n\nThat was deliberate. And good!\nIt would be practically useless to print the results for each sample.\nInstead, we store the result of interest from each sample in the nHeads vector."
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#quantifying-likelihood-under-the-null-hypothesis-7",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#quantifying-likelihood-under-the-null-hypothesis-7",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Quantifying likelihood under the null hypothesis",
    "text": "Quantifying likelihood under the null hypothesis\nTesting the Null Hypothesis with Simulation\nWe can look at that vector in a number of ways.\n\n\nWe can use a table:\n\n\ntable(nHeads)\n\nnHeads\n   2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17 \n   2    7   25   94  249  565  980 1354 1697 1730 1393  975  551  246   95   32 \n  18 \n   5 \n\n\n\nWe can use a histogram\n\n\nhist(nHeads, breaks = 20)\n\n\n\n\n\n\n\n\nWhy would we use a table and histogram?"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#testing-our-coin",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#testing-our-coin",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Testing Our Coin",
    "text": "Testing Our Coin\nOut of 10,000 simulated samples, I flipped exactly 16 HEADS in 95 (or 0.95%) of the samples.\n\n\nIf the null hypothesis was true, it is pretty unlikely\n\nIf the coin was flipping was fair\n\nYou may be tempted to take the percentage of simulated samples that had exactly 16 HEADS and call it a p-value\n\nI have evidence the coin wasn’t fair (p = 0.01)!\n\nIs this correct?\nNo!\n\nA p-value represents the likelihood of observing a certain outcome or an outcome more extreme\n\n\n\n\n\nsum( nHeads &gt;= 16 ) # samples with 16 or more HEADS\n\n[1] 132\n\nsum( nHeads &gt;= 16 )/nSims # ...as a proportion\n\n[1] 0.0132\n\n\nWe could say something like this:\n“The evidence suggests that your sibling was flipping a coin biased to land on HEADS (simulated one-tailed p = 0.013).”"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#testing-our-coin-1",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#testing-our-coin-1",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Testing Our Coin",
    "text": "Testing Our Coin\n“The evidence suggests that your sibling was flipping a coin biased to land on HEADS (simulated one-tailed p = 0.013).”\nWhat did I mean by “one-tailed”?\n\n\nhist(nHeads, breaks = 20)\n\n\n\n\n\n\n\n\n\nIs this reasonable?\n\n\n\nIn this case, a one-sided test is reasonable.\n\nWe have a directional prediction\n\nThe null hypothesis says the coin is fair.\nOur hypothesis is that it is biased to land on HEADS.\n\nThat being said, in many research applications we often start with a directional hypothesis, but use a two-sided test."
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#two-sided-tests",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#two-sided-tests",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Two-Sided Tests",
    "text": "Two-Sided Tests\nTwo-tailed tests consider extreme outcomes on both sides of the distribution.\n\nIt should be equally unlikely under the null hypthesis to observe 16 or more TAILS out of 21 flips.\n\n(5 or fewer heads)\n\n\n\nhist(nHeads, breaks = 20)\n\n\n\n\n# count samples with 16 or more HEADS and 5 or fewer HEADS\nsum( nHeads &gt;= 16 ) + sum( nHeads &lt;= 5)\n\n[1] 260\n\n(sum( nHeads &gt;= 16 ) + sum( nHeads &lt;= 5))/nSims # ...as a proportion\n\n[1] 0.026"
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#your-sibling-probably-rigged-the-coin-flips",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#your-sibling-probably-rigged-the-coin-flips",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "Your Sibling Probably Rigged the Coin Flips",
    "text": "Your Sibling Probably Rigged the Coin Flips\nA fair coin, flipped 21 times, only has a 2%chance of landing on HEADS 16 or more times.\n\nBut you still can’t be 100% certain\n\nThere is a chance you sibling just got lucky.\n\n\nHow could you be more certain?\n\n\nCollect more data\n\nFlip it another 100 times or so and tally the results\n\nThe bigger your sample size, the more power you have to detect systematic differences.\n\n\nYou could also look for other evidence\n\nHas your brother been searching the internet for ways to rig coin flips?\n\nBut, there will always be some uncertainty. That’s just how it is."
  },
  {
    "objectID": "teaching/01 Hypotheses and Statistical Inference.html#rejecting-the-null-hypothesis",
    "href": "teaching/01 Hypotheses and Statistical Inference.html#rejecting-the-null-hypothesis",
    "title": "01 - Hypotheses and Statistical Inference",
    "section": "“Rejecting” the Null Hypothesis",
    "text": "“Rejecting” the Null Hypothesis\nTraditionally (arbitrarily) researchers will often declare a result “significant” if p &lt; .05 (for a two-tailed test)\n\n\nWhen a result is less than p &lt; .05, researchers will often say that they can “reject the null hypothesis”\nThus, there is tendency for people to consider results with p-values less than .05 as “true” and dismiss those with p-values greater than .05 as noise.\nWe should be more nuanced as researchers by recognizing that evidence comes in varying strengths and to treat each piece of evidence accordingly.\nI don’t think the “accept”/“reject” language helps.\nInstead, I prefer language that I think is more accurate:\n\nEvidence is either consistent with or inconsistent with the null hypothesis"
  }
]