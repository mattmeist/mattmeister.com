[
  {
    "objectID": "teaching/R_pcc.html",
    "href": "teaching/R_pcc.html",
    "title": "Intro to R",
    "section": "",
    "text": "In this semester, I am going to ask you to do a fair bit of work before coming to class. This will make our class time shorter, more manageable, and hopefully less boring.\nI am also going to use this as an opportunity for you to directly earn grade points for your effort/labor, rather than “getting things right” on an exam.\nTherefore, I will ask you to work through these worksheets before class. Throughout, I will post Pre-Class Questions for you to work through in R. These will look like this:\n\nIn R, please write code that will read in the .csv from Canvas called sf_listings_2312.csv. Assign this the name bnb.\nYou will then write your answer in a .r script:\n\n\nClick to show code and output\n\n\n\n# Q0\n#bnb &lt;- read.csv(\"sf_listings_2312.csv\")\n\n\n\n\n\nTo earn full points, you need to organize your code correctly. Specifically, you need to:\n\nAnswer questions in order.\n\nIf you answer them out of order, just re-arrange the code after.\n\nPreface each answer with a comment (# Q1/# Q2/# Q3) that indicates exactly which question you are answering.\n\nPlease just write the letter Q and the number in this comment.\n\nMake sure your code runs on its own, on anyone’s computer.\n\nTo do this, I would always include rm(list = ls()) at the top of every .r script. This will clean everything from the environment, allowing you to see if this runs on my computer.\n\n\n\n\n\n\nYou must submit this to Canvas before the start of class.\nYou must submit this code as a .txt file. This is because Canvas cannot present .R files to me in SpeedGrader. To save as .txt:\n\nClick File -&gt; New File -&gt; Text File\nCopy and paste your completed code to that new text file.\nSave the file as firstname_lastname_module.txt\n\nFor example, my file for Module 01 would be matt_meister_01.txt\nMy file for module 05 would be matt_meister_05.txt\n\n\n\n\n\n\n\nI will grade these for completion.\nYou will receive 1 point for every question you give an honest attempt to answer\nYour grade will be the number of questions you answer, divided by the total number of questions.\n\nThis is why it is important that you number each answer with # Q1.\nAny questions that are not numbered this way will be graded incomplete, because I can’t find them.\n\nYou will receive a 25% penalty for submitting these late."
  },
  {
    "objectID": "teaching/R_pcc.html#pre-class-code-assignment-instructions",
    "href": "teaching/R_pcc.html#pre-class-code-assignment-instructions",
    "title": "Intro to R",
    "section": "",
    "text": "In this semester, I am going to ask you to do a fair bit of work before coming to class. This will make our class time shorter, more manageable, and hopefully less boring.\nI am also going to use this as an opportunity for you to directly earn grade points for your effort/labor, rather than “getting things right” on an exam.\nTherefore, I will ask you to work through these worksheets before class. Throughout, I will post Pre-Class Questions for you to work through in R. These will look like this:\n\nIn R, please write code that will read in the .csv from Canvas called sf_listings_2312.csv. Assign this the name bnb.\nYou will then write your answer in a .r script:\n\n\nClick to show code and output\n\n\n\n# Q0\n#bnb &lt;- read.csv(\"sf_listings_2312.csv\")\n\n\n\n\n\nTo earn full points, you need to organize your code correctly. Specifically, you need to:\n\nAnswer questions in order.\n\nIf you answer them out of order, just re-arrange the code after.\n\nPreface each answer with a comment (# Q1/# Q2/# Q3) that indicates exactly which question you are answering.\n\nPlease just write the letter Q and the number in this comment.\n\nMake sure your code runs on its own, on anyone’s computer.\n\nTo do this, I would always include rm(list = ls()) at the top of every .r script. This will clean everything from the environment, allowing you to see if this runs on my computer.\n\n\n\n\n\n\nYou must submit this to Canvas before the start of class.\nYou must submit this code as a .txt file. This is because Canvas cannot present .R files to me in SpeedGrader. To save as .txt:\n\nClick File -&gt; New File -&gt; Text File\nCopy and paste your completed code to that new text file.\nSave the file as firstname_lastname_module.txt\n\nFor example, my file for Module 01 would be matt_meister_01.txt\nMy file for module 05 would be matt_meister_05.txt\n\n\n\n\n\n\n\nI will grade these for completion.\nYou will receive 1 point for every question you give an honest attempt to answer\nYour grade will be the number of questions you answer, divided by the total number of questions.\n\nThis is why it is important that you number each answer with # Q1.\nAny questions that are not numbered this way will be graded incomplete, because I can’t find them.\n\nYou will receive a 25% penalty for submitting these late."
  },
  {
    "objectID": "teaching/R_pcc.html#getting-started",
    "href": "teaching/R_pcc.html#getting-started",
    "title": "Intro to R",
    "section": "Getting Started",
    "text": "Getting Started\n\nR for Data Science\nThis book: https://r4ds.hadley.nz/ is a great tool for this class. I am going to require you to work through Chapters 9-11 by next class, but I think you should work through it all throughout this semester. If you get stuck on R in class, look back to this book!\nI will not ask specific test questions from this book, but it helped me immensely to learn R.\n\n\nMake sure you have R and RStudio downloaded\nIf you do not, go to this link https://www.r-project.org for R. Download whatever version matches your computer. Then, once this is downloaded, go to https://posit.co/download/rstudio-desktop/ and download RStudio. RStudio is an IDE (integrated development environment), which will help you to organize, run, and edit your code a lot.\n\n\nMy Notes/Code/Slides this semester\nI write my notes/slides/code in .qmd (called “Quarto”) documents. I will upload these as .html documents. .html files are webpages. You open them in your browser, but you do not need an internet connection to open them.\nI write my code in Quarto for four reasons:\n\nI want to be able to share my code with you\nI kind of like the organization of it all\nThis way, I don’t have to write both R scripts AND powerpoint (or these documents)\nThis gives me an excuse to have boring slides\n\nThere are some differences between .R and .qmd, which you only need to know if you download my .qmd files and are confused.\nIn an R script (.R), you can just write code in lines (i.e., nothing special to do). To run that code, you can either: (a) Hit cmd+enter (or cntrl+enter) to run the line of code you are on, or (b) Highlight a section of code and hit cmd/enter (or cntrl/enter). To “comment” (i.e., write text that is not code), you have to place a # at the start of each comment.\n\n#This comment will not run as code\n\nIn a Quarto document (.qmd), you write comments in line. This means that if you want to write code, you do so in chunks. You can create a chunk by: (a) Typing ```{r}, (b) Pressing option + cmd (cntrl) + i, or (c) Clicking Code &gt; Insert chunk. Within a chunk, you behave just like within an R script.\nFor yourselves right now, open a new .R script in RStudio. Do this by opening RStudio, then clicking File &gt; New File &gt; R Script. Then:\n\nMake a new folder on your computer called MSMI_603\n\nI’d prefer you put this in your Documents folder or on your Desktop\n\nFind cereals.csv on Canvas\n\nDownload it, and move it into the MSMI_603 folder\n\nSave this .R file into the MSMI_603 folder"
  },
  {
    "objectID": "teaching/R_pcc.html#starting",
    "href": "teaching/R_pcc.html#starting",
    "title": "Intro to R",
    "section": "StaRting",
    "text": "StaRting\nWe’re going to start by reading data into our workspace.\nFor this introduction to R, we will take the role of a Junior Market Researcher at Safeway Grocery Stores. You have two jobs:\n\nDescribe the relationship between customers’ rating and cereals’ calories\nDescribe how Safeway’s shelves are ordered\n\nTo read in data, we will first have to set our working directory. The working directory is a folder on our computers. Setting it tells R what folder we are: (a) Reading data from, (b) Saving data to, and (c) Saving code to. I repeat: Your working directory is a folder. There are three ways to set it in RStudio:\n\nOption A: Click Session -&gt; Set Working Directory -&gt; To Source File Location\n\nThis only works if your .R script is saved to the right folder\n\nOption B: Click Session -&gt; Set Working Directory -&gt; Choose Directory\n\nThen find the folder you want, and Open the folder\n\nOption C: Write setwd(~/Documents/MSMI_603) in the console (the bottom left panel in RStudio), and then hit enter\n\nIf you have saved this folder to your desktop, you’d write setwd(~/Desktop/MSMI_603)\n\n\n\nRead in cereals.csv\nWe are going to read our first data frame into R. Very exciting! To do so, please and run write the following code:\n\nread.csv(\"cereals.csv\")\n\nHere’s an explanation:\n\nread.csv()\n\nIs a function. We can tell because it is to the left of ().\nInside of the (), we write arguments, which tell the function specifically what to do.\nIt reads a .csv file into R.\nA .csv file is a spreadsheet.\n\n\"cereals.csv\"\n\nIs the file we want to read in.\nNote that you need the \"\", and you need the .csv\n\n\nIf you ran that code, you should have seen a bunch of numbers and letters pop up in your console. However, you won’t see that data anywhere else. So… What’s going on?\nread.csv() will read in a .csv, but it won’t do anything else unless we specifically tell R we want it to keep the data. This probably seems silly (why would we read it in if we didn’t want to use it?), but R will almost never assume things for us. We will usually have to tell R what we want explicitly every time, and in a lot of detail. This is going to be annoying at first, but it is what makes everything work.\nSo, we need to tell R we want to keep these data. We can do that by assigning the data to some name (similar to how naming a stray cat may make your parents more likely to let you keep it). We can do this with &lt;- or =.\n\nPre-Class Q1\n\nIn R, please read in the .csv from Canvas called cereals.csv. Assign this the name cereals.\n\n\nClick to show code and output\n\n\n\n# Q1\ncereals &lt;- read.csv(\"cereals.csv\")\n\n\n\nNote that when you assign something, it won’t print out into the console. For example, this will print out:\n\n1 + 1\n\n[1] 2\n\n\nBut this will not:\n\ntwo &lt;- 1 + 1 # add one and one\n\nWhen you assign a name to something, you are telling R that you want to use it later. So R doesn’t print it out. Instead, you will see the name of this object appear in the Environment pane in the top right of your RStudio. (Look there now! You should see cereals, as “75 obs. of 17 variables” because it has 75 rows and 17 columns).\n\n\nComments\nI mentioned comments above, and you may have noticed this #:\n\ntwo &lt;- 1 + 1 # add one and one\n\nThis is how we comment in R. Anything to the right of the # will be ignored by R. Anything to the left will run like code. This can be a very useful way to remind yourself (or team members) what some code does. Best practice is to comment a lot.\n\nPre-Class Q2\n\nIn R, please read in the .csv from Canvas called cereals.csv. Assign this the name cereals. This time, write a note to yourself that explains what this code does. Do not use the same comment as I do.\n\n\nClick to show code and output\n\n\n\n# Q2\ncereals &lt;- read.csv(\"cereals.csv\") # read in cereals.csv\n\n\n\nWhile we are here, note that you can also write code on multiple lines, as long as the lines above are an incomplete command:\n\ncereals &lt;- read.csv( # Use read.csv\n  \"cereals.csv\") # to bring this file in\n# Save it as \"cereals\"\n\n\nPre-Class Q3\n\nThis example does not work because the first line is complete. The second line is then incomplete, which throws an error. I would like you to change what I have to get something that will work.\n\n# Q3\n(1+1)\n/2\n\n\n\nBack to cereals\nIt is usually good to have a unique identifier (ID) for each unit of analysis in our data. In this case, the unit of analysis is a given cereal. You may notice that we do not have a unique ID yet. We have name, but there may be duplicates. We don’t know yet. So, let’s add a new column called id.\nI want id to be a unique number for each row in cereals. Here’s the worst way I could do this, which you should not do:\n\ncereals$id &lt;- c(1,2,3,4,5,6,7,8,9,\n        10,11,12,13,14,15,16,17,18,19,\n        20,21,22,23,24,25,26,27,28,29,\n        30,31,32,33,34,35,36,37,38,39,\n        40,41,42,43,44,45,46,47,48,49,\n        50,51,52,53,54,55,56,57,58,59,\n        60,61,62,63,64,65,66,67,68,69,\n        70,71,72,73,74,75)\n\nThis is terrible because it is so manual. There are a lot of things about R that are difficult, but we push through them so that we can avoid doing hard things manually. So we will figure out a better way. But first, here is an explanation of what that just did.\nIn R, data are often contained in vectors. Vectors are effectively like columns in an Excel file, or columns in a table. All of the columns in our data frame cereals are individual vectors. Therefore, to add a column, we need to first create a vector, and then attach it to cereals.\nWe can create vectors with a function, called c(). The c refers to “concatenate”. For example. to concatenate the numbers 1, 2, and 3 into a vector, you could write this:\n\nc(1, 2, 3)\n\n[1] 1 2 3\n\n\nTherefore, the code cereals$id &lt;- c(...) says:\n\nMake a vector that goes from 1 to 75\nThen assign that vector to the cereals data frame\nIn the place where column id is\n\nIf a column doesn’t exist, we create it\nIf it does exist, we overwrite it\n\n\nWhen we see a (, we know that the thing to the left is the name of a function, and the thing(s) inside of the () are arguments.\nWhen we see a $, we should know that the thing to the left is the name of a data.frame, and the thing to the right is the name of a column.1\nThis $ also works for accessing columns.\nc() works well when we have small vectors. But there are better ways to make longer vectors:\n\nseq() function\n\nseq refers to “sequence”\n\nrep() function\n\nrep refers to “repeat”\n\n\n\nPre-Class Q4\n\nCreate a sequence that goes from 1 to 3, and does so by 1:\n\nHint: Run ?seq() in your console to pull up help\n\n\n\nClick to show code and output\n\n\n\n#Q4\nseq(from=1, to=3, by=1)\n\n[1] 1 2 3\n\n\n\n\n\nPre-Class Q5\n\nRepeat the number 1 75 times:\n\nHint: Run ?rep() in your console to pull up help\n\n\n\nClick to show code and output\n\n\n\n#Q5\nrep(x = 1, times = 75)\n\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[39] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\n\n\nNow, use this to create the id column\n\ncereals$id &lt;- seq(from=1, to=75, by=1)\n\nIn this case, you could do this simpler, with:\n\ncereals$id &lt;- 1:75\n\nWhich will work when you want sequential numbers that increase by 1.\n\n\nNesting functions and the order of operations\nA better solution is to use a function inside of seq() so that you don’t have to even know how many rows there are in cereals (again, we are trying to minimize manual effort). The function nrow() will take a data.frame and tell you how many rows it has.\n\ncereals$id &lt;- seq(from=1, to=nrow(cereals), by=1)\n\nWhat was that??\n\nR works by:\n\nStarting immediately right of the assignment arrow\nReading left to right\nBut following the order of operations\n\n“Create a sequence” (seq()\n\n“Start at 1” (from = 1)\n“End at nrow(cereals)” (nrow(cereals))\n\nWhat is that?? ?nrow()\n75!\n\n“Go by 1 at a time” (by = 1)\n“End the function” ())\n\n\nThe way I think of it is that R reads inside to out, left to right.\n\n\nFunctions\nFunctions are super useful. They also can be very easy to learn when you understand the rhythm/grammar of them.\nEach function in R takes some set of arguments. These tell the function what to do. We separate arguments with commas.\nFor example, seq() can take the arguments from, to, by, length.out, and along.with.\nWe’ll try some examples of using these arguments. Take note of what the different arguments do.\n\nseq( #Create a sequence\n  from=0, #starting at 0\n  to=10, #ending at 10\n  by=2) #by 2 (at a time)\n\n[1]  0  2  4  6  8 10\n\n\n\nseq( #Create a sequence\n  from=0, #starting at 0\n  by=2, #by 2 (at a time)\n  length.out=6) # Make it 6 items long\n\n[1]  0  2  4  6  8 10\n\n\n\nseq(#Create a sequence\n  from=0, #starting at 0\n  to=10, #ending at 10\n  length.out=6) # Make it 6 items long\n\n[1]  0  2  4  6  8 10\n\n\n\nseq( #Create a sequence\n  to=100, #To 10\n  by=2, #By 2s\n  length.out=6 ) # Make it 6 items long\n\n[1]  90  92  94  96  98 100\n\n\n\nseq( #Create a sequence\n  0, 10, 2 ) #???\n\n[1]  0  2  4  6  8 10\n\n\nYou might be curious why that last one worked. To find out, look at the help by running ?seq().\nIf you input the arguments in the “default” order, you don’t need to explicitly name them.\nIn my code, I write everything out. It’s much better if you ever share your code (including with your future self). Sometimes that’s super unnecessary. Sometimes it’s obvious what each argument is. Ultimately, this choice is up to you.\n\n\nWhy did I recommend using ?seq()?\nI also recommend you go to a function’s documentation online (Google it) if you have problems. For specific questions about specific functions, I do not recommend going to chatGPT first.\n\nCouple reasons\nMain: functions are updated constantly\n\nLLMs are not amazing at picking this up\nThis can give extremely confusing errors\n\nSecond:\n\nEven if a function has not been updated, a LLM does not know how it “works”\nIt knows how people have talked about it (and functions like it) on stackOverflow\nIt makes predictions of how the function works, based on some information it has seen\nThis also can cause some strange errors\n\n\nInstead, it is best to start with ? in R. I normally scroll to the bottom and look at examples. If this does not help, go to the function’s documentation (Google it). If this is hard to read, go to an LLM.\nAdditionally, if you have problems with a function you already know a bit, go to an LLM! They can do the last mile pretty well. But be ware of “wishful” coding, which is when GPT gives you code that looks correct, but is not.\n\n\nWhat I do recommend chatGPT for\nLLMs work by analysing a ton of data. When you ask an LLM a question, it predicts what words should go together in an answer, based on what words have appeared together in similar answers. This means that it doesn’t know anything–it predicts what a good answer would be.\nFor coding, this means that you need to ask narrow, direct questions (i.e., “I have this very specific problem, how can I solve it in R?”, or “I have this code that should work, but need help with this one bit”). You also need to give as much information as possible (i.e., “I am trying to plot group means using ggplot2 in R”). If your questions are too broad or don’t contain enough info, you are giving an LLM too much room. And at this point, you are not comfortable enough in R to know if it is wrong.\n\nPre-Class Q6\n\nPrint out the column sodium from cereals.\n\n\nClick to show code and output\n\n\n\n# Q6\ncereals$sodium\n\n [1] 130  15 140 200 180 125 210 200 210 220 290 210 140 180 280 290  90 180 140\n[20]  80 220 140 190 125 200   0 160 240 135  45 280 140 170  75 220 250 180 170\n[39] 170 260 150 180   0  95 150 150 220 190 220 170 170 200   0   0 135   0 210\n[58] 140   0 240 290   0   0   0  70 230  15 200 190 200 250 140 230 200 200\n\n\n\n\n\nPre-Class Q7\n\nUse the function mean to find the mean of the column calories.\n\n\nClick to show code and output\n\n\n\n# Q7\nmean(cereals$calories)\n\n[1] 107.4667\n\n\n\n\n\n\nPipes\nLet’s say I want to split these cereals into five groups, so I write:\n\nrep( #Repeat\n  seq( #A sequence\n    from=1, #Starting at 1\n    to=5, # Ending at 5\n    by=1), # By 1\n  times= nrow(cereals)/5 ) # As many times as we can\n\nThis may be a little confusing, because nested parentheses can get messy. Instead, we can use a pipe (|&gt;) to clean it up. A pipe PIPES the result of one function into another. You can read them as if they say “and then…”. Here’s what our code would look like with a pipe:\n\nseq( from=1, to=5, by=1) |&gt;\n  rep(times = nrow(cereals)/5 ) #Repeat as many times as we can\n\nEach line needs to be complete before the pipe, and the pipe has to end the line.\nLet’s assign that group as a column:\n\ncereals$group &lt;- seq( from=1, to=5, by=1) |&gt;\n  rep(times = nrow(cereals)/5 ) #Repeat as many times as we can\n\n\n\nNaming Objects\nThose names also didn’t mean anything. They can be almost anything.\nOver the next few weeks, see what works, what doesn’t.\nIn general:\n\nNames should be useful, readable (for someone else), and follow some common convention\n\nVariable names should be nouns and function names should be verbs.\nAvoid re-using names of common functions and variables.\nSeparate words with _, ., or capital letters\n\n\n\n\nData Types\nR has many different types of data. We’ll focus on:\n\nNumbers\nCharacters (strings)\nLogical\n\nYou can check what type something is with str().\n\nstr(cereals)\n\n'data.frame':   75 obs. of  19 variables:\n $ name        : chr  \"100% Bran\" \"100% Natural Bran\" \"All-Bran with Extra Fiber\" \"Almond Delight\" ...\n $ mfr         : chr  \"N\" \"Q\" \"K\" \"R\" ...\n $ type        : chr  \"C\" \"C\" \"C\" \"C\" ...\n $ calories    : int  70 120 50 110 110 110 130 90 90 120 ...\n $ protein     : int  4 3 4 2 2 2 3 2 3 1 ...\n $ fat         : int  1 5 0 2 2 0 2 1 0 2 ...\n $ sodium      : int  130 15 140 200 180 125 210 200 210 220 ...\n $ fiber       : num  10 2 14 1 1.5 1 2 4 5 0 ...\n $ carbo       : num  5 8 8 14 10.5 11 18 15 13 12 ...\n $ sugars      : int  6 8 0 8 10 14 8 6 5 12 ...\n $ potass      : int  280 135 330 -1 70 30 100 125 190 35 ...\n $ vitamins    : int  25 0 25 25 25 25 25 25 25 25 ...\n $ shelf       : int  3 3 3 3 1 2 3 1 3 2 ...\n $ weight      : num  1 1 1 1 1 1 1.33 1 1 1 ...\n $ cups        : num  0.33 1 0.5 0.75 0.75 1 0.75 0.67 0.67 0.75 ...\n $ rating      : num  68.4 34 93.7 34.4 29.5 ...\n $ sodiumPerCal: num  1.857 0.125 2.8 1.818 1.636 ...\n $ id          : num  1 2 3 4 5 6 7 8 9 10 ...\n $ group       : num  1 2 3 4 5 1 2 3 4 5 ...\n\n\n\n\nNumbers:\n\nstr(cereals$calories)\n\n int [1:75] 70 120 50 110 110 110 130 90 90 120 ...\n\n\n\n\nStrings (characters/text)\n\nstr(cereals$name)\n\n chr [1:75] \"100% Bran\" \"100% Natural Bran\" \"All-Bran with Extra Fiber\" ...\n\n\nQuotes (” or ’) make strings\n\naString &lt;- \"5\"\nstr(aString)\n\n chr \"5\"\n\n\n\n\nLogical\n\nTRUE or T for true\nFALSE or F for false.\n\nIf you don’t use all caps, you’ll get an error.\n\nlogical &lt;- T\nstr(logical)\n\n logi TRUE\n\n\nLogical data matters because it is the result of a logical test, like this:\n\ncereals$fat == 0\n\n [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE\n[13] FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE FALSE\n[25]  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE\n[37]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[49] FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE\n[61]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n[73] FALSE FALSE FALSE\n\n\nThis code tests for each observation in cereals$fat, whether that observation is 0. This results in a logical vector that is TRUE if fat is 0. So, if I asked you to make a vector that indicated whether a cereal was fat free, you could just write:\n\ncereals$fat_free &lt;- cereals$fat == 0\n\nTo finish the pre-class code, make two new columns in cereals. First,\n\nPre-Class Q8\n\nlow_cal that is TRUE if calories is less than 100\n\n\nClick to show code and output\n\n\n\n# Q8\ncereals$low_cal &lt;- cereals$calories &lt; 100\n\n\n\n\nPre-Class Q9\n\nnot_shelf3 that is TRUE if shelf is NOT 3\n\n\nClick to show code and output\n\n\n\n# Q9\ncereals$not_shelf3 &lt;- cereals$shelf != 3"
  },
  {
    "objectID": "teaching/R_pcc.html#footnotes",
    "href": "teaching/R_pcc.html#footnotes",
    "title": "Intro to R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLater in the semester, this will not be exactly true. Sometimes, the thing to the left will be the name of a list, and the thing to the right an item in that list. We will make this more specific later.↩︎"
  },
  {
    "objectID": "teaching/06 effect sizes.html#thus-far",
    "href": "teaching/06 effect sizes.html#thus-far",
    "title": "Effect Sizes",
    "section": "Thus far",
    "text": "Thus far\nSo far, we have learned about:\n\nMeans\nVariance\nStatistical significance\n\n(among other things)"
  },
  {
    "objectID": "teaching/06 effect sizes.html#thus-far-1",
    "href": "teaching/06 effect sizes.html#thus-far-1",
    "title": "Effect Sizes",
    "section": "Thus far",
    "text": "Thus far\nWe’ve learned to say things like:\n\n\nThe difference in clicking between group A and B is 2%\n\nAnd this is significant because p &lt; .001\n\nWith every $10,000 increase in income, customers spend $25 more in our stores\n\nAnd this slope is significantly different from 0 because p = .02\n\nCustomers who are 25-34 are more interested in our product than those who are 35-44\n\n\\(M_{25-34}\\) = 4.85/6\n\\(M_{25-34}\\) = 4.32/6\nThis might be due to chance, as p = .09"
  },
  {
    "objectID": "teaching/06 effect sizes.html#thus-far-2",
    "href": "teaching/06 effect sizes.html#thus-far-2",
    "title": "Effect Sizes",
    "section": "Thus far",
    "text": "Thus far\nHave we learned to say things like:\n\n\nThe difference in clicking between group A and B is 2%\n\nThis is a big difference?\n\nWith every $10,000 increase in income, customers spend $25 more in our stores\n\nThis is a big difference?\n\nCustomers who are 25-34 are more interested in our product than those who are 35-44\n\n\\(M_{25-34}\\) = 4.85/6\n\\(M_{25-34}\\) = 4.32/6\nThis is a big difference?"
  },
  {
    "objectID": "teaching/06 effect sizes.html#thus-far-3",
    "href": "teaching/06 effect sizes.html#thus-far-3",
    "title": "Effect Sizes",
    "section": "Thus far",
    "text": "Thus far\nNo!\nFor the clearest example, let’s focus on the third:\n\nThe one that uses a 0-6 scale\n\n\n\nWhat is a difference of .53 on a 0-6 scale?\n\nIs that big?\nDoes it matter in this context?\nTo answer this, we are going to learn about effect sizes"
  },
  {
    "objectID": "teaching/06 effect sizes.html#effect-sizes",
    "href": "teaching/06 effect sizes.html#effect-sizes",
    "title": "Effect Sizes",
    "section": "Effect sizes",
    "text": "Effect sizes\nEffect sizes put our results into a standard format.\n\n\nThey do not tell us if our result is statistically significant or not.\n\nWe use them after that\n\nThey tell us about how big our results are\n\nAgain, in a standardized format"
  },
  {
    "objectID": "teaching/06 effect sizes.html#effect-sizes-1",
    "href": "teaching/06 effect sizes.html#effect-sizes-1",
    "title": "Effect Sizes",
    "section": "Effect sizes",
    "text": "Effect sizes\nEffect sizes put our results into a standard format.\nThere are two kinds of effect sizes, broadly:\n\n\nStandardized differences\n\nThese give us a standardized way to say whether the difference between groups is big\n\nVariance explained\n\nThese tell us whether some variable explains a lot or a little of our DV"
  },
  {
    "objectID": "teaching/06 effect sizes.html#effect-sizes-2",
    "href": "teaching/06 effect sizes.html#effect-sizes-2",
    "title": "Effect Sizes",
    "section": "Effect sizes",
    "text": "Effect sizes\nEffect sizes put our results into a standard format.\nWe will learn two today\n\n\nStandardized differences\n\nCohen’s d\n\n\\(\\frac{(M_A - M_B)}{SD_{AB}}\\)\n\n\nVariance explained\n\n\\(R^2\\)\n\n\\(1 - \\frac{SSR}{n - p - 1} \\div \\frac{SST}{n - 1}\\)\n\nThese tell us whether some variable explains a lot or a little of our DV"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d",
    "href": "teaching/06 effect sizes.html#cohens-d",
    "title": "Effect Sizes",
    "section": "Cohen’s d",
    "text": "Cohen’s d\n\\(\\frac{(M_A - M_B)}{SD_{AB}}\\)\n\n\\(M_A\\): Mean of group A\n\\(M_B\\): Mean of group B\n\\(SD_{AB}\\): Pooled standard deviation\n\nAveraging the standard deviation is fine\n\n\nThis tells us how large the difference between groups is in terms of total variance in the data."
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples",
    "href": "teaching/06 effect sizes.html#cohens-d---examples",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nHeights of men and women in the US:\nAre men and women different heights on average?\n\n\n\\(M_{Male}\\) = 69 inches\n\\(M_{Female}\\) = 64 inches\n\\(SD_{Height}\\) = 2.75 inches\nCohen’s d?\n\n1.81"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples-1",
    "href": "teaching/06 effect sizes.html#cohens-d---examples-1",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nHeights of men and women in the US:\n\nCohen’s d = 1.81"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples-2",
    "href": "teaching/06 effect sizes.html#cohens-d---examples-2",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nHeights of men and women in the US:\n\nCohen’s d = 1.81"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples-3",
    "href": "teaching/06 effect sizes.html#cohens-d---examples-3",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nAre people are more aggressive toward individuals who have provoked them?\n\n\n\\(M_{Provoked}\\) = 8.232/10\n\\(M_{Unprovoked}\\) = 4.4/10\n\\(SD_{Aggression}\\) = 3.22\nCohen’s d?\n\n1.19"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples-4",
    "href": "teaching/06 effect sizes.html#cohens-d---examples-4",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nAre people are more aggressive toward individuals who have provoked them?\n\nCohen’s d = 1.19"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples-5",
    "href": "teaching/06 effect sizes.html#cohens-d---examples-5",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nAre people are more aggressive toward individuals who have provoked them?\n\nCohen’s d = 1.19"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples-6",
    "href": "teaching/06 effect sizes.html#cohens-d---examples-6",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nAre people who are seen as more credible are also more persuasive?\n\n\n\\(M_{Credible}\\) = 5.42/10\n\\(M_{Not}\\) = 4.76/10\n\\(SD_{Persuasion}\\) = 3.29\nCohen’s d?\n\n.20"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples-7",
    "href": "teaching/06 effect sizes.html#cohens-d---examples-7",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nAre people who are seen as more credible are also more persuasive?\n\nCohen’s d = .20"
  },
  {
    "objectID": "teaching/06 effect sizes.html#cohens-d---examples-8",
    "href": "teaching/06 effect sizes.html#cohens-d---examples-8",
    "title": "Effect Sizes",
    "section": "Cohen’s d - Examples",
    "text": "Cohen’s d - Examples\nAre people who are seen as more credible are also more persuasive?\n\nCohen’s d = .20"
  },
  {
    "objectID": "teaching/06 effect sizes.html#contextualize-your-effect-sizes",
    "href": "teaching/06 effect sizes.html#contextualize-your-effect-sizes",
    "title": "Effect Sizes",
    "section": "Contextualize your effect sizes",
    "text": "Contextualize your effect sizes\nSometimes you can look to other research\n\nOr benchmarks like the things above\n\nSometimes you cannot\n\nOne good comparison is covariates"
  },
  {
    "objectID": "teaching/06 effect sizes.html#contextualizing-with-covariates",
    "href": "teaching/06 effect sizes.html#contextualizing-with-covariates",
    "title": "Effect Sizes",
    "section": "Contextualizing with covariates",
    "text": "Contextualizing with covariates\nHypothesis: Mode of ordering (smartphone vs. desktop) will influence people’s portion choices\n\\(Portion Size = \\beta_{Device}xDevice + \\beta_{Hunger}Hunger + \\beta_{Dieting}Dieting\\)"
  },
  {
    "objectID": "teaching/06 effect sizes.html#contextualizing-with-covariates-1",
    "href": "teaching/06 effect sizes.html#contextualizing-with-covariates-1",
    "title": "Effect Sizes",
    "section": "Contextualizing with covariates",
    "text": "Contextualizing with covariates\nHypothesis: Mode of ordering (smartphone vs. desktop) will influence people’s portion choices\n\\(Portion Size = \\beta_{Device}xDevice + \\beta_{Hunger}Hunger + \\beta_{Dieting}Dieting\\)\n\n\n…And use common sense…"
  },
  {
    "objectID": "teaching/06 effect sizes.html#r-squared",
    "href": "teaching/06 effect sizes.html#r-squared",
    "title": "Effect Sizes",
    "section": "R-Squared",
    "text": "R-Squared\n\\(1 - \\frac{SSR}{n - p - 1} \\div \\frac{SST}{n - 1}\\)\nThis tells you:\n\n\nFor an entire model, how much of all of the variance you are explaining\n\nWe get this result from lm()\n\nFor each individual effect, how much of all of the variance it explains\n\nWe can get this result from anova()"
  },
  {
    "objectID": "teaching/06 effect sizes.html#r-squared-1",
    "href": "teaching/06 effect sizes.html#r-squared-1",
    "title": "Effect Sizes",
    "section": "R-Squared",
    "text": "R-Squared\nFrom anova()\n\ncustomerData &lt;- read.csv('customerData.csv')\n\nm_1 &lt;- lm( data = customerData, sat.service ~ 1) # Just the mean\nm_2 &lt;- lm( data = customerData, sat.service ~ email) # Effect of email\nm_3 &lt;- lm( data = customerData, sat.service ~ email + income) # Effect of email and income\n\nanova(m_1, m_2, m_3)\n\nAnalysis of Variance Table\n\nModel 1: sat.service ~ 1\nModel 2: sat.service ~ email\nModel 3: sat.service ~ email + income\n  Res.Df     RSS Df Sum of Sq        F  Pr(&gt;F)    \n1    590 1187.70                                  \n2    589 1179.40  1      8.30   5.9544 0.01497 *  \n3    588  819.51  1    359.89 258.2261 &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "teaching/06 effect sizes.html#r-squared-2",
    "href": "teaching/06 effect sizes.html#r-squared-2",
    "title": "Effect Sizes",
    "section": "R-Squared",
    "text": "R-Squared\nFrom anova()\n\n\nAnalysis of Variance Table\n\nModel 1: sat.service ~ 1\nModel 2: sat.service ~ email\nModel 3: sat.service ~ email + income\n  Res.Df     RSS Df Sum of Sq        F  Pr(&gt;F)    \n1    590 1187.70                                  \n2    589 1179.40  1      8.30   5.9544 0.01497 *  \n3    588  819.51  1    359.89 258.2261 &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\\(R^2_{email}\\)?\n\n\\(1 - \\frac{1592}{658 - 1 - 1} \\div \\frac{1606}{658 - 1}\\)\n.009\n\n\\(R^2_{income}\\)?\n\n\\(1 - \\frac{945}{658 - 2 - 1} \\div \\frac{1606}{658 - 1}\\)\n.407"
  },
  {
    "objectID": "teaching/06 effect sizes.html#r-squared-3",
    "href": "teaching/06 effect sizes.html#r-squared-3",
    "title": "Effect Sizes",
    "section": "R-Squared",
    "text": "R-Squared\nFrom lm()\n\nsummary(m_2)\n\n\nCall:\nlm(formula = sat.service ~ email, data = customerData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9813 -0.7347  0.0187  1.0187  4.0187 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.98131    0.09673  41.159   &lt;2e-16 ***\nemailyes    -0.24656    0.12111  -2.036   0.0422 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.415 on 589 degrees of freedom\n  (409 observations deleted due to missingness)\nMultiple R-squared:  0.006987,  Adjusted R-squared:  0.005301 \nF-statistic: 4.144 on 1 and 589 DF,  p-value: 0.04222"
  },
  {
    "objectID": "teaching/06 effect sizes.html#effect-size-conclusion",
    "href": "teaching/06 effect sizes.html#effect-size-conclusion",
    "title": "Effect Sizes",
    "section": "Effect Size Conclusion",
    "text": "Effect Size Conclusion\n\n\nThere are lots of effect size measures out there\nThey are useful, in that it’s nice to contextualize our effects\nThey come in two forms:\n\nStandardized differences\n\nThese give us a standardized way to say whether the difference between groups is big\n\nVariance explained\n\nThese tell us whether some variable explains a lot or a little of our DV"
  },
  {
    "objectID": "teaching/04 Treatment Effects.html",
    "href": "teaching/04 Treatment Effects.html",
    "title": "05 Treatment Effects",
    "section": "",
    "text": "In this semester, I am going to ask you to do a fair bit of work before coming to class. This will make our class time shorter, more manageable, and hopefully less boring.\nI am also going to use this as an opportunity for you to directly earn grade points for your effort/labor, rather than “getting things right” on an exam.\nTherefore, I will ask you to work through the posted slides on Canvas before class. Throughout the slides, I will post Pre-Class Questions for you to work through in R. These will look like this:\n\nIn R, please write code that will read in the .csv from Canvas called sf_listings_2312.csv. Assign this the name bnb.\nYou will then write your answer in a .r script:\n\n\nClick to show code and output\n\n\n\n# Q1\n#bnb &lt;- read.csv(\"sf_listings_2312.csv\")\n\n\n\n\n\nTo earn full points, you need to organize your code correctly. Specifically, you need to:\n\nAnswer questions in order.\n\nIf you answer them out of order, just re-arrange the code after.\n\nPreface each answer with a comment (# Q1/# Q2/# Q3) that indicates exactly which question you are answering.\n\nPlease just write the letter Q and the number in this comment.\n\nMake sure your code runs on its own, on anyone’s computer.\n\nTo do this, I would always include rm(list = ls()) at the top of every .r script. This will clean everything from the environment, allowing you to see if this runs on my computer.\n\n\n\n\n\n\nYou must submit this to Canvas before 9:00am on the day of class. Even if class starts at 10:00am that day, these are always due at 9:00.\nYou must submit this code as a .txt file. This is because Canvas cannot present .R files to me in SpeedGrader. To save as .txt:\n\nClick File -&gt; New File -&gt; Text File\nCopy and paste your completed code to that new text file.\nSave the file as firstname_lastname_module.txt\n\nFor example, my file for Module 01 would be matt_meister_01.txt\nMy file for module 05 would be matt_meister_05.txt\n\n\n\n\n\n\n\nI will grade these for completion.\nYou will receive 1 point for every question you give an honest attempt to answer\nYour grade will be the number of questions you answer, divided by the total number of questions.\n\nThis is why it is important that you number each answer with # Q1.\nAny questions that are not numbered this way will be graded incomplete, because I can’t find them.\n\nYou will receive a 25% penalty for submitting these late.\nI will post my solutions after class."
  },
  {
    "objectID": "teaching/04 Treatment Effects.html#pre-class-code-assignment-instructions",
    "href": "teaching/04 Treatment Effects.html#pre-class-code-assignment-instructions",
    "title": "05 Treatment Effects",
    "section": "",
    "text": "In this semester, I am going to ask you to do a fair bit of work before coming to class. This will make our class time shorter, more manageable, and hopefully less boring.\nI am also going to use this as an opportunity for you to directly earn grade points for your effort/labor, rather than “getting things right” on an exam.\nTherefore, I will ask you to work through the posted slides on Canvas before class. Throughout the slides, I will post Pre-Class Questions for you to work through in R. These will look like this:\n\nIn R, please write code that will read in the .csv from Canvas called sf_listings_2312.csv. Assign this the name bnb.\nYou will then write your answer in a .r script:\n\n\nClick to show code and output\n\n\n\n# Q1\n#bnb &lt;- read.csv(\"sf_listings_2312.csv\")\n\n\n\n\n\nTo earn full points, you need to organize your code correctly. Specifically, you need to:\n\nAnswer questions in order.\n\nIf you answer them out of order, just re-arrange the code after.\n\nPreface each answer with a comment (# Q1/# Q2/# Q3) that indicates exactly which question you are answering.\n\nPlease just write the letter Q and the number in this comment.\n\nMake sure your code runs on its own, on anyone’s computer.\n\nTo do this, I would always include rm(list = ls()) at the top of every .r script. This will clean everything from the environment, allowing you to see if this runs on my computer.\n\n\n\n\n\n\nYou must submit this to Canvas before 9:00am on the day of class. Even if class starts at 10:00am that day, these are always due at 9:00.\nYou must submit this code as a .txt file. This is because Canvas cannot present .R files to me in SpeedGrader. To save as .txt:\n\nClick File -&gt; New File -&gt; Text File\nCopy and paste your completed code to that new text file.\nSave the file as firstname_lastname_module.txt\n\nFor example, my file for Module 01 would be matt_meister_01.txt\nMy file for module 05 would be matt_meister_05.txt\n\n\n\n\n\n\n\nI will grade these for completion.\nYou will receive 1 point for every question you give an honest attempt to answer\nYour grade will be the number of questions you answer, divided by the total number of questions.\n\nThis is why it is important that you number each answer with # Q1.\nAny questions that are not numbered this way will be graded incomplete, because I can’t find them.\n\nYou will receive a 25% penalty for submitting these late.\nI will post my solutions after class."
  },
  {
    "objectID": "teaching/04 Treatment Effects.html#setup",
    "href": "teaching/04 Treatment Effects.html#setup",
    "title": "05 Treatment Effects",
    "section": "Setup",
    "text": "Setup\nLoad in these packages. If you do not have them, you will need to install them.\n\ne.g., install.packages(\"fixest\")\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(fixest)\n\nRead in cola_data.csv from Canvas, and assign it the name DF:\n\n\nClick to show code and output\n\n\n\nDF &lt;- read.csv(\"cola_data.csv\")"
  },
  {
    "objectID": "teaching/04 Treatment Effects.html#causality",
    "href": "teaching/04 Treatment Effects.html#causality",
    "title": "05 Treatment Effects",
    "section": "Causality",
    "text": "Causality\nWe are often attempting to test some causal relationship with our data. Specifically, we want to know if the factor we care about is causing a change in the outcome that matters to us. We can visualize this like this:\n\nThe Potential Outcomes Model\n\n\n\n\n\n\n\nExample 1: Causal effect of getting a college degree on earnings"
  },
  {
    "objectID": "teaching/04 Treatment Effects.html#example-2-causal-effect-of-receiving-a-target-coupon-on-purchasing",
    "href": "teaching/04 Treatment Effects.html#example-2-causal-effect-of-receiving-a-target-coupon-on-purchasing",
    "title": "05 Treatment Effects",
    "section": "Example 2: Causal effect of receiving a Target coupon on purchasing",
    "text": "Example 2: Causal effect of receiving a Target coupon on purchasing\n\n\n\n\n\nThis causal effect (also called a treatment effect) of the treatment Wi on individual i can be written as:\n\\(Y_i(W_i = 1) - Y_i(W_i = 0)\\)\nAll this estimates is the difference in earnings (\\(Y\\)) if individual (\\(i\\)) went to college (\\(W = 1\\)) vs. did not go to college (\\(W = 0\\)). Or the difference in purchase rate (\\(Y\\)) if individual (\\(i\\)) received a coupon (\\(W = 1\\)) vs. did not (\\(W = 0\\))\nIn the absolute best case, we would have a model of parallel worlds, where the treatment is the only (!) difference across the two worlds. Unfortunately, we only have access to a single world… for now.\n\nPotential outcomes vs. actually observed data\nIn any data we only observe the realized outcome for each individual:\n\\(Y_i(W_i = 1)\\) OR \\(Y_i(W_i = 0)\\)\nThis means we cannot directly estimate the individual causal effect (treatment effect) for each individual person. This is because each person only has a single outcome at a single time. Because… again… one universe for now.\nInstead of estimating individual treatment effects, we must estimate the average treatment effect in the population:\n\\(ATE = Avg [Y_i(W_i = 1) - Y_i(W_i = 0)]\\)\nThis is the average difference in outcomes between groups. The average difference in earnings for individuals who went to college vs. those who did not go to college, and the average difference in purchase rate when receiving vs. not receiving coupon. Unfortunately, this introduces the potential for “confounds”.\n\n\nConfounds\nIf you have ever heard the saying that “correlation does not imply causation”, then you have heard about confounds. These are “other explanations” that may cause the relationships we observe. In the examples above:\n\nIndividuals who go to college may have better academic skills and a higher earnings potential than those who do not go to college\nSuppose coupon is targeted to individuals who buy the product at the current purchase occasion.\nIndividuals with a coupon will generally be more likely to buy in future than others.\n\nIn general, this means that we cannot just subtract one group from the other. Treated individuals may be selected in systematic ways, and we only observe one outcome per person. So we can’t just say that treatment causes an outcome… generally.\nBut all hope is not lost, and we do have some solutions. We will cover two this week:\n\nRandomization + experiments\n\n“lab” conditions — fully randomized, controlled experiments\n“field” conditions — partially randomized, controlled experiments\n\nModel based predictions\n\nIdea: Use regression models to predict outcome of treated unit in control condition\nRequires explicit assumptions on omitted factors\nUsed with observational data, field experiments\n\n\nIn class we will cover randomization and experiments. But first, we will talk about model-based predictions below."
  },
  {
    "objectID": "teaching/04 Treatment Effects.html#to-begin",
    "href": "teaching/04 Treatment Effects.html#to-begin",
    "title": "05 Treatment Effects",
    "section": "To begin",
    "text": "To begin\nThese data are observations of cola sales at a series of stores over a series of months. Sometimes, this cola was on promotion. We are going to use this data to analyze the effect of price and promotion on sales. Our goal is to understand these effects so that we can design some pricing policy for our stores.\nFirst, check how many stores and months we have here.\n\n\nClick to show code and output\n\n\n\nlength(unique(DF$store))\n\n[1] 100\n\nlength(unique(DF$month))\n\n[1] 12\n\n\n\n\n\nPre-Class Q1\n\nGenerate a scatterplot of sales vs. price. Then, in a comment discuss whether you think that there is a relationship between price and sales.\n\n\nClick to show code and output\n\n\n\nggplot(data = DF, aes(x = price, y = sales)) +\n  geom_point(alpha = .8, size = 2)\n\n\n\n\n\n\n\n#Possible relationship, but weak if at all\n\n\n\n\nPre-Class Q2\n\nNow, estimate a linear regression of sales as a function of price and promotion. In a comment, interpret the coefficients.\n\n\nClick to show code and output\n\n\n\nsummary(lm(sales ~ price + promo, data=DF))\n\n\nCall:\nlm(formula = sales ~ price + promo, data = DF)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-63.739 -14.311  -0.635  14.518  76.703 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 112.5135     2.7918  40.301  &lt; 2e-16 ***\nprice         2.8881     0.6163   4.686 3.10e-06 ***\npromo        10.9676     1.6460   6.663 4.07e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.26 on 1197 degrees of freedom\nMultiple R-squared:  0.05993,   Adjusted R-squared:  0.05836 \nF-statistic: 38.16 on 2 and 1197 DF,  p-value: &lt; 2.2e-16\n\n#   1 dollar price increase leads to +2.88 units sold\n#   Promotion month leads to +10.97 units sold\n\n\n\nThe estimated price coefficient is positive. This means that an increase in price will lead to more cola sold. That does not make much basic economic sense, since people shouldn’t buy things more if price increases. Let’s investigate this a bit.\nWhat we are worried about here is called Omitted Variable Bias. Omitted variable bias happens when some variable we do not include in our model is biasing our results. This is worrisome because we care about making causal claims—i.e., price increases cause people to buy more/less.\nOne way we have tried to reconcile omitted variable bias in the past is by controlling for things in our models. When we have these other variables, we can remove their impact directly. However, these data do not have many control variables. For example, if we worry that demand for other items at a store might increase demand for cola, we cannot directly control for that here–we don’t have “demand.”\nLuckily, the data we have here is well-suited to addressing potential omitted variable bias through a second path. That path is by looking at Within-person/Within-unit variation in sales. We can do this whenever we have panel data, which is data that has repeated observations of the same units (i.e., stores) over time. This is also commonly called “cross-sectional” data, because we have a cross-section of stores, observed over many time periods.\nThe fact that we have repeated observations on both month and store gives us strong controls for omitted variables. We will do this through store fixed effects and time trends/fixed effects.\n\nOmitted Variable Bias\nHow does adding fixed effects/trends help with omitted variables? The variation absorbed by these parameters (controls) no longer enters the error in our regressions. This means that far fewer things are left to be potentially correlated with price. The “stronger” the set of controls (more parameters), the less concern for bias. We will build up to this step-by-step.\nFirst, let’s estimate a linear regression of sales as a function of price, promotion and month. This would remove omitted variable bias if our omitted variable was something that correlated with both sales and month, and did so linearly. For example, inflation goes up every month, meaning that price has a slightly different meaning each month.\n\n\nClick to show code and output\n\n\n\nsummary(lm(sales~price + promo + month, data=DF))\n\n\nCall:\nlm(formula = sales ~ price + promo + month, data = DF)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-71.152 -13.982   0.184  14.515  67.791 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 103.2991     2.8654  36.050  &lt; 2e-16 ***\nprice         2.5903     0.5956   4.349 1.49e-05 ***\npromo        10.9283     1.5885   6.880 9.64e-12 ***\nmonth         1.6230     0.1718   9.446  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.52 on 1196 degrees of freedom\nMultiple R-squared:  0.1252,    Adjusted R-squared:  0.123 \nF-statistic: 57.06 on 3 and 1196 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nFrom this, we see that each month, sales are increasing by 1.62 units from the prior month. This is linear growth—because it comes from a linear regression—and hence called a linear time trend.\n\nPre-Class Q3\n\nIn a comment, answer whether there is statistical evidence of a linear time trend in sales.\n\n\nClick to show code and output\n\n\n\n#Yes, the coefficient on month is highly significant (and positive).\n\n\n\n\nPre-Class Q4\n\nIn a comment, answer:\nHas the inclusion of a linear time trend “fixed” the sign of the price coefficient? What does this mean?\n\n\nClick to show code and output\n\n\n\n#No, price still has a positive effect on demand.\n#This *suggests* we need further controls for omitted variables (e.g., time fixed effects).\n\n\n\nWhat if the omitted variable is correlated with both sales and month, but not linearly? The previous regression handles omitted variable bias if the omitted variable follows the calendar (e.g., 1 &gt; 2 &gt; 3…). But it may not. For example, if our stores were in vacation destinations, we should see higher sales in the summer and winter, and lower in spring/fall.\nWith a fixed-effect for month, we can control for this non-linear relationship. This effectively removes variation across each individual month, rather than calculating a trend. We will use feols() from the fixest package to calculate this.\n\n\nClick to show code and output\n\n\n\nsummary(feols(sales ~ price + promo | month, data = DF))\n\nOLS estimation, Dep. Var.: sales\nObservations: 1,200\nFixed-effects: month: 12\nStandard-errors: Clustered (month) \n      Estimate Std. Error  t value   Pr(&gt;|t|)    \nprice  2.59026   0.497769  5.20373 2.9278e-04 ***\npromo 10.98585   1.083242 10.14164 6.4225e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 20.4     Adj. R2: 0.122741\n             Within R2: 0.060409\n\n\n\n\n\nPre-Class Q5\n\nIn a comment, answer:\nWhat do you notice about the coefficients for price and promotion in this model, compared to the others?\n\n\nClick to show code and output\n\n\n\n#Price still has a positive effect on demand.\n#This *suggests* we need further controls for omitted variables (e.g., store-specific fixed effects).\n\n\n\n\nPre-Class Q6\n\nNow, let’s try to remove between-store variation with a store fixed effect.\n\n\nClick to show code and output\n\n\n\nsummary(feols(sales ~ price + promo | month + store, data = DF))\n\nOLS estimation, Dep. Var.: sales\nObservations: 1,200\nFixed-effects: month: 12,  store: 100\nStandard-errors: Clustered (month) \n      Estimate Std. Error  t value   Pr(&gt;|t|)    \nprice -28.1558    4.90899 -5.73556 1.3113e-04 ***\npromo  11.5085    1.21824  9.44684 1.3015e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 19.0     Adj. R2: 0.173069\n             Within R2: 0.081193\n\n\n\n\n\nPre-Class Q7\n\nIn a comment, answer whether and why this changed our result.\n\n\nClick to show code and output\n\n\n\n#This changes our result a lot. This must be because different stores lead to different sales and prices."
  },
  {
    "objectID": "teaching/04 Treatment Effects.html#summary",
    "href": "teaching/04 Treatment Effects.html#summary",
    "title": "05 Treatment Effects",
    "section": "Summary",
    "text": "Summary\n\nWhen we did not control for store fixed effects, we got a surprising result\nThe simplest model we ran showed what we think is the wrong result\nSo we dug deeper\n\n“Is this result due to something we can’t observe?”\n\nRemoving month variation did not fix the issue"
  },
  {
    "objectID": "teaching/04 Treatment Effects.html#panel-data-how-does-it-work",
    "href": "teaching/04 Treatment Effects.html#panel-data-how-does-it-work",
    "title": "05 Treatment Effects",
    "section": "Panel Data… How does it work?",
    "text": "Panel Data… How does it work?\n\nWithin-Unit Transformation\nfeols() does not use dummy variables to estimate fixed effects. Instead, it transforms the data to “eliminate” the fixed effect, then runs a regular linear regression.\nRather than sales ~ price + store:\n\nWe subtract the average of sales for each store from each observation.\nAnd we subtract the average of price for each store from each observation.\n\nThis gives us De-meaned y regressed on de-meaned X. This means there is NO intercept in this regression (it is absorbed by fixed effects).\n\nImplications of this transformation:\nAll time-constant factors entering X and Y are removed from the regression. Including all unobserved time-constant omitted variables. This leads to a major reduction in omitted variable bias, as we are only left with time-varying factors. The nice thing is that these fixed effects mean we don’t even need to know what the omitted variables could be, let alone have data for them."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/star_ratings_kilos.html",
    "href": "posts/star_ratings_kilos.html",
    "title": "Star ratings are a bad way to compare products (2)",
    "section": "",
    "text": "This is the second in a series about star ratings/online reviews. I plan to mainly talk about my own research. But who knows.\nThe kilogram is a useful measure of mass. The kilogram is widely used because it is objectively defined. One kilogram is equal to the weight of one liter of liquid water—so a bathroom scale tells users how much liquid water they equate to. Beyond providing a good ice breaker for dinner parties, this also makes the kilogram easy to interpret and compare; the kilogram helps people to compare objects to important benchmarks (e.g., “Is this truck too heavy to cross this bridge?”), and to other objects (e.g., “Which truck should I take to cross this rickety bridge?”). It is a good thing that the kilogram is widely used—the kilogram is a good measure.\nStar ratings are similar to the kilogram in one sense, but very different in another. Ratings are also widely used. Almost anywhere we shop online, we find some form of ratings. However, star ratings are absolutely not objectively defined. There is no equivalent to the liter of water that defines what each star in a rating represents, and not even guidance as to what ratings should measure. As a result, ratings are open to being swayed by context. In my last post, I wrote about one example of mental context—expectations; influenced by Airbnb Superhost certifications—and its effect on ratings. In this post, I’ll write about this in a more general sense, explaining why (I think) context influences our evaluations.1"
  },
  {
    "objectID": "posts/star_ratings_kilos.html#footnotes",
    "href": "posts/star_ratings_kilos.html#footnotes",
    "title": "Star ratings are a bad way to compare products (2)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn a later post, I will move onto physical contexts.↩︎\nPhoto by Muhammad Mahdi Karim FacebookThe making of this document was supported by Wikimedia CH.↩︎\nBy IFCAR, Public Domain, https://commons.wikimedia.org/w/index.php?curid=17491394↩︎\nThe main paradigm is adapted from Abeler, J., Falk, A., Goette, L., & Huffman, D. (2011). “Reference points and effort provision.” American Economic Review, 101(2), 470-492.↩︎\nMeans of 3.73 v. 4.44, F(1, 199) = 22.91, p &lt; .001, d = .68.↩︎\nMeans of $1.16 v. $.93↩︎"
  },
  {
    "objectID": "soap_box.html",
    "href": "soap_box.html",
    "title": "Blog",
    "section": "",
    "text": "Star ratings are a bad way to compare products (2)\n\n\nStars are not kilograms\n\n\n\n\n\nJun 5, 2025\n\n\nMatt Meister\n\n\n\n\n\n\n\n\n\n\n\n\nStar ratings are a bad way to compare products (1)\n\n\nWe don’t know enough about what goes into other people’s ratings\n\n\n\n\n\nJun 3, 2025\n\n\nMatt Meister\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/star_ratings_steves.html",
    "href": "posts/star_ratings_steves.html",
    "title": "Star ratings are a bad way to compare products (1)",
    "section": "",
    "text": "This is the first in a series about star ratings/online reviews. I plan to mainly talk about my own research. But who knows. Maybe we’ll get weird."
  },
  {
    "objectID": "posts/star_ratings_steves.html#footnotes",
    "href": "posts/star_ratings_steves.html#footnotes",
    "title": "Star ratings are a bad way to compare products (1)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNot always, of course; When I have to use the restroom, I don’t compare the time cost to the time I would lose by changing my pants. But many of the decisions we care about result from comparisons.↩︎\nIn America, this translates to “Barnes and Noble”.↩︎\nIf only temporarily in some cases.↩︎\nWhich might be more arid and empty than the moon↩︎\nA decent argument for cynicism↩︎\nBefore continuing, it is worth noting that at least two other papers provide compelling and well-specified evidence for the first condition and its impact. “Can Lower(ed) Expert Opinions Lead to Better Consumer Ratings?: The Case of Michelin Stars” by Xingyi Li and colleagues finds that losing Michelin stars has a positive effect on ratings for restaurants. “The Good, the Bad and the Picky: Consumer Heterogeneity and the Reversal of Product Ratings” by Tommaso Bondi, Michelangelo Rossi, and Ryan Stevens finds similarly that people are more critical of movies after they have been nominated for Oscars.↩︎\nSometimes these certifications are influenced by ratings (Superhost status is), sometimes they are not (Heather seems unswayed by the masses), and sometimes it is not clear (eBay’s Top Rated Sellers are a confusing example of this).↩︎\nThe specific assumption we make here is that nothing that influences ratings changes between platforms alongside Superhost status. Because Superhost status is entirely an Airbnb feature, this should be more defensible.↩︎\nAnd another paper, detailed in the next blog post↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\nMatt Meister\n",
    "section": "",
    "text": "Matt Meister\n\n\n(.com)\n\n\n\n\nI study how people learn about and evaluate their choices—especially through online information. My work combines large-scale scraped data with experiments to learn how and why people make the judgments and choices they do.\n\n\nI teach applied statistics, marketing analytics, and other marketing courses at both undergraduate and graduate levels. My classes emphasize real (messy) data, decision-making, and gaining comfort with difficult topics.\n\n\n\n  Email: mmeister@usfca.edu \n  GitHub \n  LinkedIn \n  CV \n\n\nRecent Blogs\nStar ratings are a bad way to compare products (2)\nStar ratings are a bad way to compare products (1)"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-vs-noise",
    "href": "teaching/bias_vs_noise.html#bias-vs-noise",
    "title": "Bias vs. Noise",
    "section": "Bias vs Noise",
    "text": "Bias vs Noise\nIn statistics, the terms bias and noise refer to very specific things.\n\nBias:\n\nSystematic errors that consistently push measurements or estimates away from the true value\nIt is predictable, and often stems from flaws in the measurement process or the sampling method.\nBias tends to push measurements or estimates in a particular direction, either overestimating or underestimating the true value.\n\n\n\nNoise:\n\nRandomness or fluctuations in data that can’t be attributed to a systematic cause\nUnpredictable. Can result from various sources, including measurement errors, and chance.\nNoise doesn’t consistently push data in one direction but adds random fluctuations around the true value."
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-vs-noise-1",
    "href": "teaching/bias_vs_noise.html#bias-vs-noise-1",
    "title": "Bias vs. Noise",
    "section": "Bias vs Noise",
    "text": "Bias vs Noise\nHow we handle each also differs:\n\n\nBias:\n\nUsually involves identifying and eliminating or reducing sources of systematic error.\nTechniques such as calibration, randomization, and careful study design can help mitigate bias.\n\nNoise:\n\nInherently random and cannot be eliminated entirely.\nTechniques like averaging, statistical tests, and increasing sample sizes reduce the impact of noise."
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-1",
    "href": "teaching/bias_vs_noise.html#noise-1",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\n“Why isn’t having a small sample size a source of bias?”\nRead in the data customerData.csv\n\ncustomerData &lt;- read.csv(\"customerData.csv\")\n\nLet’s treat this full data set of 1000 observations as the population – the entire set of people we are interested in.\nIn the population, what is the mean of income?\n\n\nmean(customerData$income)\n\n[1] 65476.08"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-2",
    "href": "teaching/bias_vs_noise.html#noise-2",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nImagine we did not know this number for the population\n\nWe could only estimate it by surveying a sample of people.\n\n\n\nTake a random subset of 10 observations by running this code:\n\n\nsample.size &lt;- 10\n\ncustomerDataSmall &lt;- customerData[\n  sample(x = 1:nrow(customerData),\n         size = sample.size,\n         replace = F), \n  ]"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-3",
    "href": "teaching/bias_vs_noise.html#noise-3",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nIf you did not know the population average of income, how would you estimate it with your survey sample?\n\n\nYou would see what the mean in the sample is!\n\n\nmean(customerDataSmall$income)\n\n[1] 65858.9\n\n\n\n\nThat sample estimate is going to be noisy\n\nIt’s going to vary from sample to sample around the population average\n\n“true mean”"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-4",
    "href": "teaching/bias_vs_noise.html#noise-4",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nThat sample estimate is going to be noisy\n\nIt’s going to vary from sample to sample around the population average.\nIf everyone in the class had their own sample (which you do), what might your different estimates look like?"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-5",
    "href": "teaching/bias_vs_noise.html#noise-5",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nAre these estimates biased?\n\n\nAre they more likely to be above or below the true mean?\n\n\n\n% above:\n\nsum( # Sum of the logical argument\n  sample.results$mean &gt; mean(customerData$income))/ # Is the sample mean &gt; than the pop?\n  nsims\n\n[1] 0.51\n\n\n\n\n% below:\n\nsum( # Sum of the logical argument\n  sample.results$mean &lt; mean(customerData$income))/ # Is the sample mean &gt; than the pop?\n  nsims\n\n[1] 0.49"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-6",
    "href": "teaching/bias_vs_noise.html#noise-6",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nWith a small sample, our results are not more likely to fall on one side of the true mean than the other.\n\nAs long as our data don’t have crazy outliers!!\n\nWhat is the benefit of larger samples, then?\n\n\nPrecision!\nHow far off were our estimates, on average?\n\n\nmean(abs(sample.results$mean - mean(customerData$income))) |&gt;\n  round(2)\n\n[1] 5357.99"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-7",
    "href": "teaching/bias_vs_noise.html#noise-7",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nLet’s try with samples of 20:\n\nsample.size &lt;- 20\n\ncustomerDataSmall &lt;- customerData[\n  sample(x = 1:nrow(customerData),\n         size = sample.size,\n         replace = F), \n  ]\n\nmean(customerDataSmall$income)\n\n[1] 70132.8"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-8",
    "href": "teaching/bias_vs_noise.html#noise-8",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nLet’s try with samples of 20:"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-9",
    "href": "teaching/bias_vs_noise.html#noise-9",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nLet’s try with samples of 20:\n\nHow far off was our estimate, on average?\n\n\nmean(abs(sample.results[sample.results$sample.size==20,'mean'] - mean(customerData$income))) |&gt;\n  round(2)\n\n[1] 3638.14"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-10",
    "href": "teaching/bias_vs_noise.html#noise-10",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nLet’s try with samples of 100:\n\nsample.size &lt;- 100\n\ncustomerDataSmall &lt;- customerData[\n  sample(x = 1:nrow(customerData),\n         size = sample.size,\n         replace = F), \n  ]\n\nmean(customerDataSmall$income)\n\n[1] 66418.28"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-11",
    "href": "teaching/bias_vs_noise.html#noise-11",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nLet’s try with samples of 100:"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-12",
    "href": "teaching/bias_vs_noise.html#noise-12",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nLet’s try with samples of 100:\n\nHow far off was our estimate, on average?\n\n\nmean(abs(sample.results[sample.results$sample.size==100,'mean'] - mean(customerData$income))) |&gt;\n  round(2)\n\n[1] 1570.13"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#noise-13",
    "href": "teaching/bias_vs_noise.html#noise-13",
    "title": "Bias vs. Noise",
    "section": "Noise",
    "text": "Noise\nConclusion:\n\nNoise is random error\nIt causes our estimates to bounce around the true/population mean\nMakes our estimates imprecise\nBut it doesn’t push them in one direction or another\n\nBias on the other hand…"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-1",
    "href": "teaching/bias_vs_noise.html#bias-1",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\n\n\nSystematic errors that consistently push measurements or estimates away from the true value\nIt is predictable, and often stems from flaws in the measurement process or the sampling method.\nBias tends to push measurements or estimates in a particular direction, either overestimating or underestimating the true value.\nIs not made better by increasing sample sizes"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-2",
    "href": "teaching/bias_vs_noise.html#bias-2",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nLet’s imagine that instead of estimating income with a random sample of 10/20/100 people, we sent out a survey, and got 10 responses.\n\nBut young people were more likely to respond\n\n\nsample.size &lt;- 10\n\nunder30 &lt;- customerData[customerData$age &lt;= 29,]\n\ncustomerDataSmall &lt;- under30[\n  sample(x = 1:nrow(under30),\n         size = sample.size,\n         replace = F), \n  ]"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-3",
    "href": "teaching/bias_vs_noise.html#bias-3",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nWhat is our estimate of income from these samples?"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-4",
    "href": "teaching/bias_vs_noise.html#bias-4",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nAre these estimates biased?\n\n\nAre they more likely to be above or below the true mean?\n\n\n\n% above:\n\nsum( # Sum of the logical argument\n  sample.results$mean &gt; mean(customerData$income))/ # Is the sample mean &gt; than the pop?\n  nrow(sample.results)\n\n[1] 0.071\n\n\n\n\n% below:\n\nsum( # Sum of the logical argument\n  sample.results$mean &lt; mean(customerData$income))/ # Is the sample mean &gt; than the pop?\n  nrow(sample.results)\n\n[1] 0.929"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-5",
    "href": "teaching/bias_vs_noise.html#bias-5",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nLet’s try with samples of 20:\n\nsample.size &lt;- 20\n\ncustomerDataSmall &lt;- under30[\n  sample(x = 1:nrow(under30),\n         size = sample.size,\n         replace = F), \n  ]\n\nmean(customerDataSmall$income)\n\n[1] 57041.2"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-6",
    "href": "teaching/bias_vs_noise.html#bias-6",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nLet’s try with samples of 20:"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-7",
    "href": "teaching/bias_vs_noise.html#bias-7",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nAre these estimates biased?\n\n\nAre they more likely to be above or below the true mean?\n\n\n\n% above:\n\nsum( # Sum of the logical argument\n  sample.results$mean &gt; mean(customerData$income))/ # Is the sample mean &gt; than the pop?\n  nrow(sample.results)\n\n[1] 0.042\n\n\n\n\n% below:\n\nsum( # Sum of the logical argument\n  sample.results$mean &lt; mean(customerData$income))/ # Is the sample mean &gt; than the pop?\n  nrow(sample.results)\n\n[1] 0.958"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-8",
    "href": "teaching/bias_vs_noise.html#bias-8",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nLet’s try with samples of 100:\n\nsample.size &lt;- 100\n\ncustomerDataSmall &lt;- under30[\n  sample(x = 1:nrow(under30),\n         size = sample.size,\n         replace = F), \n  ]\n\nmean(customerDataSmall$income)\n\n[1] 56206.99"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-9",
    "href": "teaching/bias_vs_noise.html#bias-9",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nLet’s try with samples of 100:"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-10",
    "href": "teaching/bias_vs_noise.html#bias-10",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nAre these estimates biased?\n\n\nAre they more likely to be above or below the true mean?\n\n\n\n% above:\n\nsum( # Sum of the logical argument\n  sample.results$mean &gt; mean(customerData$income))/ # Is the sample mean &gt; than the pop?\n  nrow(sample.results)\n\n[1] 0.028\n\n\n\n\n% below:\n\nsum( # Sum of the logical argument\n  sample.results$mean &lt; mean(customerData$income))/ # Is the sample mean &gt; than the pop?\n  nrow(sample.results)\n\n[1] 0.972"
  },
  {
    "objectID": "teaching/bias_vs_noise.html#bias-11",
    "href": "teaching/bias_vs_noise.html#bias-11",
    "title": "Bias vs. Noise",
    "section": "Bias",
    "text": "Bias\nConclusion:\n\nBias is not random error\nIt causes our estimates to be higher or lower than the true/population mean\nMakes our estimates predictably wrong\nIt does not get better with sample size"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#relationships-between-multiple-variables",
    "href": "teaching/03 Correlation and Regression.html#relationships-between-multiple-variables",
    "title": "Correlation and Linear Regression",
    "section": "Relationships between multiple variables",
    "text": "Relationships between multiple variables\nWe’ve just started to talk about relationships between multiple variables.\n\n\nDo different advertisements lead to different interest?\nHow do different features of shoes lead to different ratings?\nDo stores closer to population centers sign up more customers than further stores?\n\n\n\nWhy might we care a lot about relationships between multiple variables?\n\n\n\nOften, we can manipulate factors related to them.\netc."
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#relationships-between-multiple-variables-1",
    "href": "teaching/03 Correlation and Regression.html#relationships-between-multiple-variables-1",
    "title": "Correlation and Linear Regression",
    "section": "Relationships between multiple variables",
    "text": "Relationships between multiple variables\nIdentifying multivariate relationships helps us as marketers.\n\n\ne.g., If stores in population centers do better than further ones, what is an obvious implication?\n\nMove people closer to our stores\n\nNo!\n\nPlace new stores near population centers\n\n\n\n\nIn this module, we focus on:\n\nUnderstanding the relationships between pairs of variables (multivariate data)\nVisualizing these relationships\nComputing statistics that describe their associations\n\nCorrelation coefficients\nRegression coefficients"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#relationships-between-multiple-variables-2",
    "href": "teaching/03 Correlation and Regression.html#relationships-between-multiple-variables-2",
    "title": "Correlation and Linear Regression",
    "section": "Relationships between multiple variables",
    "text": "Relationships between multiple variables\nI’ve taken real data from rei.com and added a few simulated variables for class.\nDownload it from Canvas, and load it into R!\n\nreiData &lt;- read.csv('rei_products.csv')"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#our-setting",
    "href": "teaching/03 Correlation and Regression.html#our-setting",
    "title": "Correlation and Linear Regression",
    "section": "Our Setting",
    "text": "Our Setting\n\n\nFor today’s class, we are going to take the role of a market analyst at REI\nSpecifically, our job is to identify new types of shoes that we should stock\nWe are going to use data from REI stores to make this decision"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#these-data",
    "href": "teaching/03 Correlation and Regression.html#these-data",
    "title": "Correlation and Linear Regression",
    "section": "These Data",
    "text": "These Data\n\n\nHow many shoes?\n1022\nHow many brands?\n\n54\n\nWhat’s a good way to look at these data?\nDescribe these variables:\n\nprice\navg_rating\nsales\nmens\navg_size\navg_running\ncushion\nweight"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#how-can-we-predict-sales",
    "href": "teaching/03 Correlation and Regression.html#how-can-we-predict-sales",
    "title": "Correlation and Linear Regression",
    "section": "How Can We Predict sales?",
    "text": "How Can We Predict sales?\n\n\nWe want to know what variables impact sales\nIf our goal is to increase future sales, these variables are worth knowing\nTwo general types of predictor variables\n\nGroups (discrete)\nContinuous\n\nLuckily, we can test them the same way"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors",
    "text": "Continuous Predictors\nWith continuous predictors (independent variables), we are seeing if some numeric variable predicts outcomes\n\nIs there a meaningful relationship between these two variables?\n\nDoes x cause y?"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#outline-for-today",
    "href": "teaching/03 Correlation and Regression.html#outline-for-today",
    "title": "Correlation and Linear Regression",
    "section": "Outline for Today",
    "text": "Outline for Today\n\nData = Model + Error\nWhy is the mean our best guess (model) when we know nothing else?\nUsing lm() to find better (linear) models\nTesting whether that model is better"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#data-model-error",
    "href": "teaching/03 Correlation and Regression.html#data-model-error",
    "title": "Correlation and Linear Regression",
    "section": "Data = Model + Error",
    "text": "Data = Model + Error\n\nThe whole point of this is to find better models to predict our data\n“model” can be a single guess, a guess at a relationship, or something else\nRegardless, we are trying to minimize Error\n\nWhich is the sum of squared error"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors-1",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors-1",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors",
    "text": "Continuous Predictors\nIn our REI data, our outcome is sales"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors-2",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors-2",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors",
    "text": "Continuous Predictors\nIn our REI data, our outcome is sales\n\nWithout knowing anything else, what is our best guess at the sales a new shoe will bring?\n\n\n\nThe mean of sales (10.05)"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors-3",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors-3",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors",
    "text": "Continuous Predictors\nIn our REI data, our outcome is sales\n\nWithout knowing anything else, what is our best guess at the sales a new shoe will bring?\n\nThe mean of sales (10.05)\n\nHow wrong will we be normally?\n\n\n\nThe standard deviation of sales (0.92)"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors-4",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors-4",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors",
    "text": "Continuous Predictors\nIn our REI data, our outcome is sales\n\nWithout knowing anything else, what is our best guess at the sales a new shoe will bring?\n\nThe mean of sales (10.05)\n\nHow wrong will we be normally?\n\nThe standard deviation of sales (0.92)\n\nHow could we get better guesses?\n\n\n\nUse some other variable!\n\nLike weight"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors-5",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors-5",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors",
    "text": "Continuous Predictors\nIn our REI data, our outcome is sales\n\n\nWe can use weight to predict sales better\n\nWhy would we care whether weight predicts sales?\nBecause we can make new shoes with different weights!\n\nWhat is one way we have looked at continuous predictors so far (in R)?"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors-scatterplots",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors-scatterplots",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors: Scatterplots",
    "text": "Continuous Predictors: Scatterplots\n\n\nggplot(data = reiData,\n       aes(x = weight, y = sales)) +\n  geom_point(alpha = .5) +\n  geom_smooth(method = 'lm', se = F, size = 1) +\n  theme_bw()"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors-6",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors-6",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors",
    "text": "Continuous Predictors\nIs this scatterplot showing a strong relationship?\n\n\nScatterplots provide a lot of visual information\nBut they’re very ~vibesy~\n\nNot precise\n\nAnd when there are more than a few variables, they’re a mess\nTherefore, it’s nice to also have a number"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#continuous-predictors-7",
    "href": "teaching/03 Correlation and Regression.html#continuous-predictors-7",
    "title": "Correlation and Linear Regression",
    "section": "Continuous Predictors",
    "text": "Continuous Predictors\nAre these scatterplots showing strong relationships?\nTwo ways to know with numbers:\n\n\nRegression\n\nUsing lm() in R\nBueno\n\nCorrelations:\n\nUsing cor() in R"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression",
    "href": "teaching/03 Correlation and Regression.html#linear-regression",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nRegression does the following:\n\nIt helps us to understand how two (or more) variables vary together\nWe can make clear predictions from regression coefficients\nWe can test regression coefficients against a null hypothesis"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-1",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-1",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nSo what is linear regression?\n\nWhy did I say that the mean of a variable is our best guess at any given value of that variable?\nLet’s try some guesses of reiData$sales\n\n\nTo understand this, let’s look the first 10 values of reiData\n\nreiDataSmall &lt;- head(reiData, 20)\n\nAnd I’m going to only keep the columns I think are useful for this example\n\nreiDataSmall &lt;- reiDataSmall[, c('weight', 'sales')]\nreiDataSmall\n\n     weight     sales\n1  3.160536 10.069211\n2  4.970014 10.131396\n3  6.759033  8.856539\n4  3.068526 10.783086\n5  5.254373 11.149486\n6  4.769999 10.239619\n7  3.831546 10.078726\n8  3.805244 10.128758\n9  3.172416 10.696915\n10 2.901195  9.825507\n11 2.461316 10.569462\n12 2.737338 10.440997\n13 2.982250  9.584194\n14 5.910042 11.216148\n15 5.154571  9.845226\n16 6.090791  9.356569\n17 5.153109 10.137007\n18 4.059393  9.513671\n19 5.565751  9.480360\n20 2.473582 11.151987"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nIf we are interested in the sales column, let’s see what it looks like:\n\nggplot(data = reiDataSmall, # Scatterplot\n       aes(x = 0, \n           # ^ This is just a hacky way to get everything on the same spot on the x-axis\n           y = sales)) +\n  geom_point() +\n  theme_bw()"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-1",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-1",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\n\n\nWe are going to try to make the BEST guess we can at each value of sales\nThe best guess will be the one with the lowest possible squared error (residuals) between:\n\nOur guess and the real data\n\nWhy squared?\n\nThis makes every error positive\n\nPenalizing us for guessing too high and too low\n\nThis penalizes us for very wrong guesses more than slightly wrong ones"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-2",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-2",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nI’ll start by guessing the mean, or average\n\nguess &lt;- mean(reiDataSmall$sales)\nreiDataSmall$guess &lt;- guess\nreiDataSmall\n\n     weight     sales    guess\n1  3.160536 10.069211 10.16274\n2  4.970014 10.131396 10.16274\n3  6.759033  8.856539 10.16274\n4  3.068526 10.783086 10.16274\n5  5.254373 11.149486 10.16274\n6  4.769999 10.239619 10.16274\n7  3.831546 10.078726 10.16274\n8  3.805244 10.128758 10.16274\n9  3.172416 10.696915 10.16274\n10 2.901195  9.825507 10.16274\n11 2.461316 10.569462 10.16274\n12 2.737338 10.440997 10.16274\n13 2.982250  9.584194 10.16274\n14 5.910042 11.216148 10.16274\n15 5.154571  9.845226 10.16274\n16 6.090791  9.356569 10.16274\n17 5.153109 10.137007 10.16274\n18 4.059393  9.513671 10.16274\n19 5.565751  9.480360 10.16274\n20 2.473582 11.151987 10.16274"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-3",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-3",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nHow wrong was I?\n\nSubtract the real data from the guess\n\n\nreiDataSmall$how_wrong &lt;- reiDataSmall$guess - reiDataSmall$sales\n\n\n\nSquare them to make them all positive\n\n\nreiDataSmall$how_wrong2 &lt;- (reiDataSmall$guess - reiDataSmall$sales) ^ 2\nreiDataSmall\n\n     weight     sales    guess   how_wrong   how_wrong2\n1  3.160536 10.069211 10.16274  0.09353188 0.0087482124\n2  4.970014 10.131396 10.16274  0.03134738 0.0009826582\n3  6.759033  8.856539 10.16274  1.30620416 1.7061693156\n4  3.068526 10.783086 10.16274 -0.62034322 0.3848257155\n5  5.254373 11.149486 10.16274 -0.98674322 0.9736621774\n6  4.769999 10.239619 10.16274 -0.07687579 0.0059098874\n7  3.831546 10.078726 10.16274  0.08401714 0.0070588794\n8  3.805244 10.128758 10.16274  0.03398506 0.0011549846\n9  3.172416 10.696915 10.16274 -0.53417160 0.2853392931\n10 2.901195  9.825507 10.16274  0.33723672 0.1137286034\n11 2.461316 10.569462 10.16274 -0.40671836 0.1654198253\n12 2.737338 10.440997 10.16274 -0.27825397 0.0774252726\n13 2.982250  9.584194 10.16274  0.57854943 0.3347194427\n14 5.910042 11.216148 10.16274 -1.05340448 1.1096609896\n15 5.154571  9.845226 10.16274  0.31751697 0.1008170242\n16 6.090791  9.356569 10.16274  0.80617426 0.6499169379\n17 5.153109 10.137007 10.16274  0.02573597 0.0006623402\n18 4.059393  9.513671 10.16274  0.64907221 0.4212947353\n19 5.565751  9.480360 10.16274  0.68238311 0.4656467078\n20 2.473582 11.151987 10.16274 -0.98924365 0.9786030031"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-4",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-4",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nThe column how_wrong2 is equal to guess - sales, squared - …i.e., the guess, minus the real data, squared\n\nHow wrong was I in total?\n\nsum(reiDataSmall$how_wrong2)\n\n[1] 7.791746"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-5",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-5",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nTo see if that’s the best guess we could have made:\n\n\nLet’s try some others!"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-6",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-6",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nI’ll guess something else\n\nguess &lt;- 11\nreiDataSmall$guess &lt;- guess\nreiDataSmall\n\n     weight     sales guess   how_wrong   how_wrong2\n1  3.160536 10.069211    11  0.09353188 0.0087482124\n2  4.970014 10.131396    11  0.03134738 0.0009826582\n3  6.759033  8.856539    11  1.30620416 1.7061693156\n4  3.068526 10.783086    11 -0.62034322 0.3848257155\n5  5.254373 11.149486    11 -0.98674322 0.9736621774\n6  4.769999 10.239619    11 -0.07687579 0.0059098874\n7  3.831546 10.078726    11  0.08401714 0.0070588794\n8  3.805244 10.128758    11  0.03398506 0.0011549846\n9  3.172416 10.696915    11 -0.53417160 0.2853392931\n10 2.901195  9.825507    11  0.33723672 0.1137286034\n11 2.461316 10.569462    11 -0.40671836 0.1654198253\n12 2.737338 10.440997    11 -0.27825397 0.0774252726\n13 2.982250  9.584194    11  0.57854943 0.3347194427\n14 5.910042 11.216148    11 -1.05340448 1.1096609896\n15 5.154571  9.845226    11  0.31751697 0.1008170242\n16 6.090791  9.356569    11  0.80617426 0.6499169379\n17 5.153109 10.137007    11  0.02573597 0.0006623402\n18 4.059393  9.513671    11  0.64907221 0.4212947353\n19 5.565751  9.480360    11  0.68238311 0.4656467078\n20 2.473582 11.151987    11 -0.98924365 0.9786030031"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-7",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-7",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nHow wrong was I?\n\nSubtract the real data from the guess\n\n\nreiDataSmall$how_wrong &lt;- reiDataSmall$guess - reiDataSmall$sales\n\n\n\nSquare them to make them all positive\n\n\nreiDataSmall$how_wrong2 &lt;- (reiDataSmall$guess - reiDataSmall$sales) ^ 2\nreiDataSmall\n\n     weight     sales guess  how_wrong how_wrong2\n1  3.160536 10.069211    11  0.9307886 0.86636747\n2  4.970014 10.131396    11  0.8686041 0.75447313\n3  6.759033  8.856539    11  2.1434609 4.59442468\n4  3.068526 10.783086    11  0.2169135 0.04705148\n5  5.254373 11.149486    11 -0.1494865 0.02234620\n6  4.769999 10.239619    11  0.7603810 0.57817920\n7  3.831546 10.078726    11  0.9212739 0.84874557\n8  3.805244 10.128758    11  0.8712418 0.75906230\n9  3.172416 10.696915    11  0.3030852 0.09186061\n10 2.901195  9.825507    11  1.1744935 1.37943490\n11 2.461316 10.569462    11  0.4305384 0.18536330\n12 2.737338 10.440997    11  0.5590028 0.31248410\n13 2.982250  9.584194    11  1.4158062 2.00450713\n14 5.910042 11.216148    11 -0.2161477 0.04671984\n15 5.154571  9.845226    11  1.1547737 1.33350233\n16 6.090791  9.356569    11  1.6434310 2.70086548\n17 5.153109 10.137007    11  0.8629927 0.74475644\n18 4.059393  9.513671    11  1.4863290 2.20917378\n19 5.565751  9.480360    11  1.5196399 2.30930530\n20 2.473582 11.151987    11 -0.1519869 0.02310002"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-8",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-8",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nThe column how_wrong2 is equal to guess - sales, squared - …i.e., the guess, minus the real data, squared\n\nHow wrong was I in total?\n\nsum(reiDataSmall$how_wrong2)\n\n[1] 21.81172"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-9",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-9",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nI’ll guess something else\n\nguess &lt;- 9\nreiDataSmall$guess &lt;- guess\nreiDataSmall\n\n     weight     sales guess  how_wrong how_wrong2\n1  3.160536 10.069211     9  0.9307886 0.86636747\n2  4.970014 10.131396     9  0.8686041 0.75447313\n3  6.759033  8.856539     9  2.1434609 4.59442468\n4  3.068526 10.783086     9  0.2169135 0.04705148\n5  5.254373 11.149486     9 -0.1494865 0.02234620\n6  4.769999 10.239619     9  0.7603810 0.57817920\n7  3.831546 10.078726     9  0.9212739 0.84874557\n8  3.805244 10.128758     9  0.8712418 0.75906230\n9  3.172416 10.696915     9  0.3030852 0.09186061\n10 2.901195  9.825507     9  1.1744935 1.37943490\n11 2.461316 10.569462     9  0.4305384 0.18536330\n12 2.737338 10.440997     9  0.5590028 0.31248410\n13 2.982250  9.584194     9  1.4158062 2.00450713\n14 5.910042 11.216148     9 -0.2161477 0.04671984\n15 5.154571  9.845226     9  1.1547737 1.33350233\n16 6.090791  9.356569     9  1.6434310 2.70086548\n17 5.153109 10.137007     9  0.8629927 0.74475644\n18 4.059393  9.513671     9  1.4863290 2.20917378\n19 5.565751  9.480360     9  1.5196399 2.30930530\n20 2.473582 11.151987     9 -0.1519869 0.02310002"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-10",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-10",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nHow wrong was I?\n\nSubtract the real data from the guess\n\n\nreiDataSmall$how_wrong &lt;- reiDataSmall$guess - reiDataSmall$sales\n\n\n\nSquare them to make them all positive\n\n\nreiDataSmall$how_wrong2 &lt;- (reiDataSmall$guess - reiDataSmall$sales) ^ 2\nreiDataSmall\n\n     weight     sales guess  how_wrong how_wrong2\n1  3.160536 10.069211     9 -1.0692114 1.14321296\n2  4.970014 10.131396     9 -1.1313959 1.28005662\n3  6.759033  8.856539     9  0.1434609 0.02058103\n4  3.068526 10.783086     9 -1.7830865 3.17939738\n5  5.254373 11.149486     9 -2.1494865 4.62029208\n6  4.769999 10.239619     9 -1.2396190 1.53665537\n7  3.831546 10.078726     9 -1.0787261 1.16365003\n8  3.805244 10.128758     9 -1.1287582 1.27409505\n9  3.172416 10.696915     9 -1.6969148 2.87952000\n10 2.901195  9.825507     9 -0.8255065 0.68146104\n11 2.461316 10.569462     9 -1.5694616 2.46320975\n12 2.737338 10.440997     9 -1.4409972 2.07647300\n13 2.982250  9.584194     9 -0.5841938 0.34128242\n14 5.910042 11.216148     9 -2.2161477 4.91131075\n15 5.154571  9.845226     9 -0.8452263 0.71440747\n16 6.090791  9.356569     9 -0.3565690 0.12714145\n17 5.153109 10.137007     9 -1.1370073 1.29278555\n18 4.059393  9.513671     9 -0.5136710 0.26385794\n19 5.565751  9.480360     9 -0.4803601 0.23074587\n20 2.473582 11.151987     9 -2.1519869 4.63104763"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-11",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-11",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nThe column how_wrong2 is equal to guess - sales, squared - …i.e., the guess, minus the real data, squared\n\nHow wrong was I in total?\n\nsum(reiDataSmall$how_wrong2)\n\n[1] 34.83118"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-12",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-12",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\nTo see if that’s the best guess we could have made:\n\n\nLet’s try some others!\nLet’s simulate a thousand other guesses:\n\nFrom the minimum sales to the maximum\n\n\n\n\n\n# Set up simulation with a data frame to hold results\nsims &lt;- 1000\nsimulation.results &lt;- data.frame(\n  guess = c(mean(reiDataSmall$sales),\n            seq(length.out = sims-1, \n              from = min(reiDataSmall$sales),\n              to = max(reiDataSmall$sales))),\n  how_wrong2 = rep(NA, times = sims)\n)"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-13",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-13",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\n\nRun simulation with for() loop\n\n\nfor( i in 1:sims){\n  simulation.results$how_wrong2[i] &lt;- sum(\n    (simulation.results$guess[i] - reiDataSmall$sales) ^2 )\n}"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-14",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-14",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\n\nWhat does it look like if I plot how wrong I was by what my guess was?\n\n\nggplot(data = simulation.results,\n       aes(x = guess,\n           y = how_wrong2)) +\n  geom_point()"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-15",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-15",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\n\nWhat is the minimum amount I was wrong?\n\n\nmin(simulation.results$how_wrong2)\n\n[1] 7.791746\n\n\n\nWhat was my guess at that point?\n\n\nbestguess &lt;- min(simulation.results$how_wrong2)\nsimulation.results[simulation.results$how_wrong2 == bestguess,]\n\n     guess how_wrong2\n1 10.16274   7.791746\n\n\n\nAnd what was the mean again?\n\n\nmean(reiDataSmall$sales)\n\n[1] 10.16274"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-16",
    "href": "teaching/03 Correlation and Regression.html#why-is-the-mean-the-best-guess-16",
    "title": "Correlation and Linear Regression",
    "section": "Why is the mean the best guess?",
    "text": "Why is the mean the best guess?\n\nggplot(data = simulation.results,\n       aes(x = guess,\n           y = how_wrong2)) +\n  geom_point() + \n  geom_point(data = simulation.results[simulation.results$guess == mean(reiDataSmall$sales),],\n             aes(x = guess,\n                 y = how_wrong2),\n             color = 'red',\n             size = 4)"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#fun-fact",
    "href": "teaching/03 Correlation and Regression.html#fun-fact",
    "title": "Correlation and Linear Regression",
    "section": "Fun fact:",
    "text": "Fun fact:\n\nThe median will minimize the sum of absolute error:\n\n\nsimulation.results.median &lt;- data.frame(\n  guess = c(median(reiDataSmall$sales),\n            seq(length.out = sims-1, \n              from = min(reiDataSmall$sales),\n              to = max(reiDataSmall$sales))),\n  how_wrong = rep(NA, times = sims)\n)\n\nfor( i in 1:sims){\n  simulation.results.median$how_wrong[i] &lt;- sum(abs(simulation.results.median$guess[i] - reiDataSmall$sales))\n}"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#fun-fact-1",
    "href": "teaching/03 Correlation and Regression.html#fun-fact-1",
    "title": "Correlation and Linear Regression",
    "section": "Fun fact:",
    "text": "Fun fact:\n\nThe median will minimize the sum of absolute error:\n\n\nggplot(data = simulation.results.median,\n       aes(x = guess,\n           y = how_wrong)) +\n  geom_point()"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#fun-fact-2",
    "href": "teaching/03 Correlation and Regression.html#fun-fact-2",
    "title": "Correlation and Linear Regression",
    "section": "Fun fact:",
    "text": "Fun fact:\nThe median will minimize the sum of absolute error:\n\nWhat is the minimum amount I was wrong?\n\n\nmin(simulation.results.median$how_wrong)\n\n[1] 9.777342\n\n\n\nWhat was my guess at that point?\n\n\nbestguess.median &lt;- min(simulation.results.median$how_wrong)\nsimulation.results.median[simulation.results.median$how_wrong == bestguess.median,]\n\n       guess how_wrong\n1   10.13008  9.777342\n541 10.13092  9.777342"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-2",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-2",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nIf we did not know a shoe’s weight, what would be our best guess of sales?\n\nThe mean sales!\n\nmean(reiDataSmall$sales)\n\n[1] 10.16274"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-3",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-3",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nGraphically, that “guess” looks like this:\n\nggplot(reiDataSmall,\n       aes(x = 1, \n           y = sales)) +\n  geom_point() +\n  geom_hline(yintercept = mean(reiDataSmall$sales))"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-4",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-4",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nBut what if we have information about their weight?\n\n\nggplot(reiDataSmall,\n       aes(x = weight, \n           y = sales)) +\n  geom_point()"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-5",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-5",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nHow does our old guess look?\n\n\n\nggplot(reiDataSmall,\n       aes(x = weight, \n           y = sales)) +\n  geom_point() +\n  geom_hline(yintercept = mean(reiDataSmall$sales))"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-6",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-6",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nThat flat line is no longer our best guess, is it?\n\n\n\nIn this case, the mean is no longer the best possible guess\n\nThis is because there are large residuals between the line we guess and many points\n\nEspecially on the edges"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-7",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-7",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nInstead, what is our new best guess?\n\n\nIt’s the one that minimizes the sum of squared residuals, given that we now know weight\nWe can find it the same way we found the best guess without weight\nBut rather than guessing one number, we’ll guess that the slope of this line is:"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-8",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-8",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat is our new best guess?\n\nI’m going to test a bunch of different intercepts and slopes\n\n\nsims &lt;- 10000\nsimulation.results &lt;- data.frame(\n   guess.intercept = runif(n = sims,\n               min = 400,\n               max = 500),\n   guess.slope = runif(n = sims,\n               min = 0,\n               max = 15),\n   how_wrong2 = rep(NA, times = sims)\n)\n \nfor( i in 1:sims){\n   for( j in 1:nrow(reiDataSmall)){\n     reiDataSmall$how_wrong2[j] &lt;- ((simulation.results$guess.intercept[i] + simulation.results$guess.slope[i] * reiDataSmall$weight[j]) - reiDataSmall$sales[j])^2\n   }\n   simulation.results$how_wrong2[i] &lt;- sum(reiDataSmall$how_wrong2)\n }"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-9",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-9",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat is our new best guess?\n\nI’m going to test a bunch of different intercepts and slopes\nWhat gets us the lowest squared error?\n\n\nsimulation.results[simulation.results$how_wrong2 == min(simulation.results$how_wrong2),]\n\n     guess.intercept guess.slope how_wrong2\n1298        401.3774  0.02219569    3062451\n\n\nTo get the exact numbers\n\nUse lm() to find the coefficient for weight\n\n\nlm(data = reiDataSmall,\n   formula = sales ~ weight)\n\n\nCall:\nlm(formula = sales ~ weight, data = reiDataSmall)\n\nCoefficients:\n(Intercept)       weight  \n    10.9061      -0.1764"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-10",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-10",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nThat’s how we end up with this line:"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-11",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-11",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nThese best guesses are now different at each level of weight:\n\nweight.lm &lt;- lm(data = reiDataSmall,\n   formula = sales ~ weight)\nreiDataSmall$bestguess.lm &lt;- weight.lm$coef[\"(Intercept)\"] + \n  weight.lm$coef[\"weight\"]  * reiDataSmall$weight\nreiDataSmall$how_wrong2.lm &lt;- ( reiDataSmall$bestguess.lm - reiDataSmall$sales)^2\n\nreiDataSmall[, c('weight', 'bestguess.lm')]\n\n     weight bestguess.lm\n1  3.160536    10.348593\n2  4.970014    10.029385\n3  6.759033     9.713786\n4  3.068526    10.364824\n5  5.254373     9.979221\n6  4.769999    10.064669\n7  3.831546    10.230221\n8  3.805244    10.234861\n9  3.172416    10.346497\n10 2.901195    10.394343\n11 2.461316    10.471942\n12 2.737338    10.423249\n13 2.982250    10.380044\n14 5.910042     9.863555\n15 5.154571     9.996827\n16 6.090791     9.831670\n17 5.153109     9.997085\n18 4.059393    10.190026\n19 5.565751     9.924291\n20 2.473582    10.469778"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-12",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-12",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nAnd what are the residuals if we take weight into account?\n\nggplot(reiDataSmall,\n       aes(x = weight, \n           y = sales)) +\n  geom_point() +\n  geom_smooth(formula = 'y~x',\n              method = 'lm', \n              se = F, color = 'black') +\n  geom_segment(aes(xend = weight, yend = reiDataSmall$bestguess.lm), \n               color = \"red\",\n               linetype = 'dashed',\n               size = 1) +\n  geom_point()"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-13",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-13",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nAnd what are the residuals if we take weight into account?\n\n\nreiDataSmall[, c('weight', 'bestguess.lm', 'how_wrong2.lm')]\n\n     weight bestguess.lm how_wrong2.lm\n1  3.160536    10.348593  0.0780539703\n2  4.970014    10.029385  0.0104062769\n3  6.759033     9.713786  0.7348715017\n4  3.068526    10.364824  0.1749433569\n5  5.254373     9.979221  1.3695209324\n6  4.769999    10.064669  0.0306075101\n7  3.831546    10.230221  0.0229505609\n8  3.805244    10.234861  0.0112577147\n9  3.172416    10.346497  0.1227925882\n10 2.901195    10.394343  0.3235748427\n11 2.461316    10.471942  0.0095101693\n12 2.737338    10.423249  0.0003150111\n13 2.982250    10.380044  0.6333775757\n14 5.910042     9.863555  1.8295062522\n15 5.154571     9.996827  0.0229828344\n16 6.090791     9.831670  0.2257205210\n17 5.153109     9.997085  0.0195782530\n18 4.059393    10.190026  0.4574564464\n19 5.565751     9.924291  0.1970749260\n20 2.473582    10.469778  0.4654095167\n\n\n\nsum(reiDataSmall$how_wrong2.lm)\n\n[1] 6.739911"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-14",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-14",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nIs that smaller than before?\n\nSSE from mean:\n\nsum(reiDataSmall$how_wrong2)\n\n[1] 5245806\n\n\n\n\nSSE including weight:\n\nsum(reiDataSmall$how_wrong2.lm)\n\n[1] 6.739911"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#but-is-it-worth-drawing-that-line",
    "href": "teaching/03 Correlation and Regression.html#but-is-it-worth-drawing-that-line",
    "title": "Correlation and Linear Regression",
    "section": "But is it worth drawing that line?",
    "text": "But is it worth drawing that line?\n\n\nIs the reduction in squared error worth it?\nWe’re always going to reduce error when we add more things into a model\nBut we only want to add things that are really worth it\n\nOverfitting old data makes us likely to make worse predictions in the future\nComplexity makes our results hard to explain"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#but-is-it-worth-drawing-that-line-1",
    "href": "teaching/03 Correlation and Regression.html#but-is-it-worth-drawing-that-line-1",
    "title": "Correlation and Linear Regression",
    "section": "But is it worth drawing that line?",
    "text": "But is it worth drawing that line?\n\n\n\n\nggplot(reiDataSmall,\n       aes(x = weight, \n           y = sales)) +\n  geom_point() +\n  geom_hline(yintercept = mean(reiDataSmall$sales))\n\n\n\n\n\n\n\n\n\n\nggplot(reiDataSmall,\n       aes(x = weight, \n           y = sales)) +\n  geom_point() +\n  geom_smooth(formula = 'y~x',\n              method = 'lm', \n              se = F)"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#testing-linear-regression",
    "href": "teaching/03 Correlation and Regression.html#testing-linear-regression",
    "title": "Correlation and Linear Regression",
    "section": "Testing Linear Regression",
    "text": "Testing Linear Regression\nUse summary() around lm() to find the coefficient and p-value weight\n\nsummary(lm(data = reiDataSmall,\n   formula = sales ~ weight))\n\n\nCall:\nlm(formula = sales ~ weight, data = reiDataSmall)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.85725 -0.45172 -0.04418  0.21882  1.35259 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  10.9061     0.4642  23.496 5.87e-15 ***\nweight       -0.1764     0.1053  -1.676    0.111    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6119 on 18 degrees of freedom\nMultiple R-squared:  0.135, Adjusted R-squared:  0.08694 \nF-statistic: 2.809 on 1 and 18 DF,  p-value: 0.111\n\n\n\n\nWhat is the meaning of this p-value?\nIt’s the same as any p-value\n\n“What is the probability of seeing a slope this far (or more) from zero?”\n\nIf there was no true relationship between weight and sales"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-15",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-15",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhat (I think) you should take away from this lesson if nothing else.\n\n\nAll linear regression is is answering the question:\n\n“If I know something about variable x, what is my best guess about the value of y?”\nSpecifically, what is the best line I can draw through my data\n\nAnd for the most part, “fancier” statistics essentially boil down to linear regression."
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nWe can use regression as just an extension of a t-test\n\n\nDoes cushion impact sales?\nLet’s simplify this by removing the “Medium” cushion group\n\n\nreiData &lt;- reiData[reiData$cushion != \"Medium\",]\n\nFirst, let’s plot this"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors-1",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors-1",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nDoes cushion impact sales?\nPlot!\n\n\nreiData |&gt;\n  group_by(cushion) |&gt;\n  summarise(sales = mean(sales, na.rm = T)) |&gt; # Not sure if we have missing data\n  ggplot(\n    aes(x = cushion,\n        y = sales)) +\n  geom_bar(stat = \"identity\")"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors-2",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors-2",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nWhat we are basically doing is comparing these plots:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd answering the question:\nIs it worth knowing that we can separate the data into these two groups?"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#group-predictors",
    "href": "teaching/03 Correlation and Regression.html#group-predictors",
    "title": "Correlation and Linear Regression",
    "section": "Group Predictors",
    "text": "Group Predictors\nSo let’s see!\nUsing the lm() function in R:\n\nlm( \n  data = reiData, \n  formula = sales ~ cushion ) |&gt; # The dv is sales, IV is \"cushion\"\n  summary() # Summarise it\n\n\nCall:\nlm(formula = sales ~ cushion, data = reiData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3956 -0.4987  0.0878  0.6200  3.1414 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 10.32820    0.04648 222.193   &lt;2e-16 ***\ncushionLow  -0.69344    0.07466  -9.288   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9583 on 692 degrees of freedom\nMultiple R-squared:  0.1108,    Adjusted R-squared:  0.1096 \nF-statistic: 86.26 on 1 and 692 DF,  p-value: &lt; 2.2e-16\n\n\n\nWhat do you notice about this result, compared to the one from the t-test?\n\n\n\nt.test( \n  reiData[reiData$cushion == 'Low', 'sales'], \n  reiData[reiData$cushion == 'High', 'sales']) \n\n\n    Welch Two Sample t-test\n\ndata:  reiData[reiData$cushion == \"Low\", \"sales\"] and reiData[reiData$cushion == \"High\", \"sales\"]\nt = -8.9923, df = 509.88, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.8449394 -0.5419355\nsample estimates:\nmean of x mean of y \n 9.634764 10.328201 \n\n\nIt’s the same thing!"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors-3",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors-3",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nWhy is the result the same thing?\n\nBecause a regression and a t-test are mostly the same thing\n\nTake some data\nCompare the difference in two groups to the difference that would happen from chance alone"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors-4",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors-4",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nLet’s extend this.\nRead the entire data back in, which has the third group\n\nreiData &lt;- read.csv('rei_products.csv')"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors-5",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors-5",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nI want to know:\n\nIf cushion is worth knowing for sales:\n\nIf the three levels differ from eachother"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#answer",
    "href": "teaching/03 Correlation and Regression.html#answer",
    "title": "Correlation and Linear Regression",
    "section": "Answer",
    "text": "Answer\n\nNull hypothesis: That there is no difference in sales between cushion levels.\n\nlm(\n  data = reiData, \n  sales ~ cushion) |&gt;\n  summary()\n\n\nCall:\nlm(formula = sales ~ cushion, data = reiData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3956 -0.4759  0.0694  0.5737  3.1414 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   10.32820    0.04280 241.313  &lt; 2e-16 ***\ncushionLow    -0.69344    0.06875 -10.087  &lt; 2e-16 ***\ncushionMedium -0.28409    0.06485  -4.381 1.31e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8823 on 1019 degrees of freedom\nMultiple R-squared:  0.09084,   Adjusted R-squared:  0.08905 \nF-statistic: 50.91 on 2 and 1019 DF,  p-value: &lt; 2.2e-16\n\nreiData |&gt;\n  group_by(cushion) |&gt;\n  summarise(sales = mean(sales, na.rm = T)) |&gt;\n  ggplot(\n    aes(x = cushion,\n        y = sales)) +\n  geom_bar(stat = 'identity', fill = c('seagreen', 'goldenrod', 'seagreen3')) +\n  theme_bw()\n\n\n\n\n\n\n\n\nConclusion: There is likely a difference between conditions, as the likelihood that a difference this large arises by chance alone is &lt; .001%."
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#answer-1",
    "href": "teaching/03 Correlation and Regression.html#answer-1",
    "title": "Correlation and Linear Regression",
    "section": "Answer",
    "text": "Answer\nTo make the plot less gross\n\nreiData$cushion &lt;- factor(reiData$cushion, levels = c(\"Low\", \"Medium\", \"High\"))\nlm(\n  data = reiData, \n  sales ~ cushion) |&gt;\n  summary()\n\n\nCall:\nlm(formula = sales ~ cushion, data = reiData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3956 -0.4759  0.0694  0.5737  3.1414 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    9.63476    0.05380  179.09  &lt; 2e-16 ***\ncushionMedium  0.40935    0.07258    5.64  2.2e-08 ***\ncushionHigh    0.69344    0.06875   10.09  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8823 on 1019 degrees of freedom\nMultiple R-squared:  0.09084,   Adjusted R-squared:  0.08905 \nF-statistic: 50.91 on 2 and 1019 DF,  p-value: &lt; 2.2e-16\n\nreiData |&gt;\n  group_by(cushion) |&gt;\n  summarise(sales = mean(sales, na.rm = T)) |&gt;\n  ggplot(\n    aes(x = cushion,\n        y = sales)) +\n  geom_bar(stat = 'identity', fill = c('seagreen', 'goldenrod', 'seagreen3')) +\n  theme_bw()"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors-6",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors-6",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nBut what if I want to know:\n\nIf it is worth knowing if cushion is “High” specifically\n\nVs. Low and Medium\nYou’ll have to construct a “dummy code”\nGoogle this\n\nTest with lm()\nPlot the condition means"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors-7",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors-7",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nAnswer\n\nNull hypothesis: That there is no difference in sales between cushion high and the combination of low and medium.\n\nreiData$cushionHigh &lt;- ifelse(reiData$cushion == \"High\", 1, 0)\n\nlm(\n  data = reiData, \n  sales ~ cushionHigh) |&gt;\n  summary()\n\n\nCall:\nlm(formula = sales ~ cushionHigh, data = reiData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6205 -0.4556  0.0827  0.5721  3.1414 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  9.85967    0.03665 268.998  &lt; 2e-16 ***\ncushionHigh  0.46853    0.05684   8.243 5.12e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8956 on 1020 degrees of freedom\nMultiple R-squared:  0.06246,   Adjusted R-squared:  0.06154 \nF-statistic: 67.95 on 1 and 1020 DF,  p-value: 5.116e-16\n\nreiData |&gt;\n  group_by(cushionHigh) |&gt;\n  summarise(sales = mean(sales, na.rm = T)) |&gt;\n  ggplot(\n    aes(x = cushionHigh,\n        y = sales)) +\n  geom_bar(stat = 'identity', fill = c('seagreen', 'goldenrod')) +\n  theme_bw()\n\n\n\n\n\n\n\n\nConclusion: There is likely a difference."
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#discretegroup-predictors-8",
    "href": "teaching/03 Correlation and Regression.html#discretegroup-predictors-8",
    "title": "Correlation and Linear Regression",
    "section": "Discrete/Group Predictors",
    "text": "Discrete/Group Predictors\nBut what if I want to know whether the relationship between the three conditions is linear?\n\ni.e., Low is worst, then medium, then high?\n\n\n\nWe can construct a contrast code\n\n\nreiData$cushionLinear &lt;- ifelse( reiData$cushion == \"Low\", -1,\n                                 ifelse(reiData$cushion == \"Medium\", 0, 1))\n\nsummary(lm(data = reiData, sales ~ cushionLinear))\n\n\nCall:\nlm(formula = sales ~ cushionLinear, data = reiData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4211 -0.4745  0.0694  0.5697  3.1253 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   10.00230    0.02809  356.11   &lt;2e-16 ***\ncushionLinear  0.34204    0.03408   10.04   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8824 on 1020 degrees of freedom\nMultiple R-squared:  0.08985,   Adjusted R-squared:  0.08896 \nF-statistic: 100.7 on 1 and 1020 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-controlling-for-variables",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-controlling-for-variables",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression: Controlling for variables",
    "text": "Linear Regression: Controlling for variables\nControlling for variables\nSometimes we think that some… other variable predicts our outcome, and we want to take its effect away.\n\ne.g., If we thought shoes sold more when they were high cushion, but we know that cushion effects weight\nTo “control” for this weight, we can add (+) it to our model:\n\n\nlm(\n  data = reiData, \n  formula = sales ~ cushion + weight) |&gt;\n  summary()\n\n\nCall:\nlm(formula = sales ~ cushion + weight, data = reiData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1702 -0.4305  0.0788  0.5436  3.3486 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    8.63808    0.11750  73.516   &lt;2e-16 ***\ncushionMedium  0.06406    0.07865   0.815    0.416    \ncushionHigh   -0.13318    0.10961  -1.215    0.225    \nweight         0.41007    0.04343   9.442   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8465 on 1018 degrees of freedom\nMultiple R-squared:  0.164, Adjusted R-squared:  0.1616 \nF-statistic: 66.59 on 3 and 1018 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#controlling-for-variables",
    "href": "teaching/03 Correlation and Regression.html#controlling-for-variables",
    "title": "Correlation and Linear Regression",
    "section": "Controlling for variables",
    "text": "Controlling for variables\nRules for controlling for variables:\nFirst:\n\nTest the effect of the control on the DV alone\n\n\nlm(\n  data = reiData, \n  formula = sales ~ weight) |&gt;\n  summary()\n\n\nCall:\nlm(formula = sales ~ weight, data = reiData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1616 -0.4220  0.0828  0.5490  3.2713 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.78927    0.09499   92.53   &lt;2e-16 ***\nweight       0.35751    0.02577   13.87   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8484 on 1020 degrees of freedom\nMultiple R-squared:  0.1587,    Adjusted R-squared:  0.1579 \nF-statistic: 192.4 on 1 and 1020 DF,  p-value: &lt; 2.2e-16\n\n\n\nIf it is meaningful, it’s worth controlling for\nIf the control does not predict the DV, it can’t affect our result"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#controlling-for-variables-1",
    "href": "teaching/03 Correlation and Regression.html#controlling-for-variables-1",
    "title": "Correlation and Linear Regression",
    "section": "Controlling for variables",
    "text": "Controlling for variables\nRules for controlling for variables:\nSecond:\n\nTest the effect of the IV on the control\n\n\nlm(\n  data = reiData, \n  weight ~ cushion) |&gt;\n  summary()\n\n\nCall:\nlm(formula = weight ~ cushion, data = reiData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0509 -0.3196 -0.0154  0.2716  3.0308 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    2.43052    0.03723   65.29   &lt;2e-16 ***\ncushionMedium  0.84201    0.05023   16.77   &lt;2e-16 ***\ncushionHigh    2.01580    0.04757   42.37   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6106 on 1019 degrees of freedom\nMultiple R-squared:  0.6494,    Adjusted R-squared:  0.6487 \nF-statistic: 943.7 on 2 and 1019 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#controlling-for-variables-2",
    "href": "teaching/03 Correlation and Regression.html#controlling-for-variables-2",
    "title": "Correlation and Linear Regression",
    "section": "Controlling for variables",
    "text": "Controlling for variables\nIf the control predicts the DV AND IV predicts control:\n\nWe say this “IV is confounded with the control variable”\n\nIt is harder to disentangle:\n\n“IV leads to DV” from\n“control leads to DV, and that is why it looks like IV leads to DV”\n\n\n\n\nIf the control predicts the DV BUT IV DOES NOT predict control:\n\nWe include the control, and it makes our IV appear more reliable\n\nIt reduces the amount of variance in the DV that our IV has to explain"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#controlling-for-variables-3",
    "href": "teaching/03 Correlation and Regression.html#controlling-for-variables-3",
    "title": "Correlation and Linear Regression",
    "section": "Controlling for variables",
    "text": "Controlling for variables\nTo know from this:\n\nWhat a “control variable” is\nWhat two steps we perform to decide whether to include a control\nWhat if the control predicts our DV?\nWhat if the IV also predicts our control?"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#linear-regression-wrap-up",
    "href": "teaching/03 Correlation and Regression.html#linear-regression-wrap-up",
    "title": "Correlation and Linear Regression",
    "section": "Linear Regression Wrap Up",
    "text": "Linear Regression Wrap Up\n\n\nWe’re basically always drawing lines through points in statistics:\n\nLogistic regression:\n\nLines, but they have to be between 0 and 1\n\nLasso Regression (machine learning):\n\nLines, but a little more conservative\n\nRandom Forests (machine learning):\n\nLines, but lots of lines, and just to a single end point"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#correlation-1",
    "href": "teaching/03 Correlation and Regression.html#correlation-1",
    "title": "Correlation and Linear Regression",
    "section": "Correlation",
    "text": "Correlation\nAre these scatterplots showing strong relationships?\nTwo ways to know with numbers:\n\n\nRegression\n\nUsing lm() in R\nBueno\n\nCorrelations:\n\nUsing cor() in R"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#correlation-coefficients",
    "href": "teaching/03 Correlation and Regression.html#correlation-coefficients",
    "title": "Correlation and Linear Regression",
    "section": "Correlation Coefficients",
    "text": "Correlation Coefficients\nSpecifically, this is a Pearson correlation coefficient\n\nOften abbreviated with r.\nThis is a continuous metric between -1 and 1.\n\nPerfectly positive (x goes up, y goes up the same): +1\nPerfectly negative: -1\n\n\n\n\n\n\nplot(c(1:50), \n     c(51:100))\n\n\n\n\n\n\n\n\n\n\nplot(c(50:1), \n     c(51:100))"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#correlation-matrices",
    "href": "teaching/03 Correlation and Regression.html#correlation-matrices",
    "title": "Correlation and Linear Regression",
    "section": "Correlation Matrices",
    "text": "Correlation Matrices\nFor more than two variables, you can compute the correlations between all pairs x, y at once as a correlation matrix.\n\nLet’s look at one:\n\n\ncor(reiData[, c(\"sales\", \"price\", \"weight\")], use = 'pairwise.complete.obs') |&gt;\n  round(3)\n\n        sales  price weight\nsales   1.000 -0.132  0.398\nprice  -0.132  1.000  0.747\nweight  0.398  0.747  1.000"
  },
  {
    "objectID": "teaching/03 Correlation and Regression.html#correlation-coefficients-1",
    "href": "teaching/03 Correlation and Regression.html#correlation-coefficients-1",
    "title": "Correlation and Linear Regression",
    "section": "Correlation Coefficients",
    "text": "Correlation Coefficients\nProblem with correlation:\n\nWe can’t make predictions with correlations\n\nWe can say “y increases as x increases” generally\n\nBut not “y increases this much as x increases this much”"
  },
  {
    "objectID": "teaching/03 Logistic Regression.html",
    "href": "teaching/03 Logistic Regression.html",
    "title": "03 - Logistic Regression",
    "section": "",
    "text": "In this semester, I am going to ask you to do a fair bit of work before coming to class. This will make our class time shorter, more manageable, and hopefully less boring.\nI am also going to use this as an opportunity for you to directly earn grade points for your effort/labor, rather than “getting things right” on an exam.\nTherefore, I will ask you to work through the posted slides on Canvas before class. Throughout the slides, I will post Pre-Class Questions for you to work through in R. These will look like this:\n\nIn R, please write code that will read in the .csv from Canvas called sf_listings_2312.csv. Assign this the name bnb.\nYou will then write your answer in a .r script:\n\n\nClick to show code and output\n\n\n\n# Q1\n#bnb &lt;- read.csv(\"sf_listings_2312.csv\")\n\n\n\n\n\nTo earn full points, you need to organize your code correctly. Specifically, you need to:\n\nAnswer questions in order.\n\nIf you answer them out of order, just re-arrange the code after.\n\nPreface each answer with a comment (# Q1/# Q2/# Q3) that indicates exactly which question you are answering.\n\nPlease just write the letter Q and the number in this comment.\n\nMake sure your code runs on its own, on anyone’s computer.\n\nTo do this, I would always include rm(list = ls()) at the top of every .r script. This will clean everything from the environment, allowing you to see if this runs on my computer.\n\n\n\n\n\n\nYou must submit this to Canvas before 9:00am on the day of class. Even if class starts at 10:00am that day, these are always due at 9:00.\nYou must submit this code as a .txt file. This is because Canvas cannot present .R files to me in SpeedGrader. To save as .txt:\n\nClick File -&gt; New File -&gt; Text File\nCopy and paste your completed code to that new text file.\nSave the file as firstname_lastname_module.txt\n\nFor example, my file for Module 01 would be matt_meister_01.txt\nMy file for module 05 would be matt_meister_05.txt\n\n\n\n\n\n\n\nI will grade these for completion.\nYou will receive 1 point for every question you give an honest attempt to answer\nYour grade will be the number of questions you answer, divided by the total number of questions.\n\nThis is why it is important that you number each answer with # Q1.\nAny questions that are not numbered this way will be graded incomplete, because I can’t find them.\n\nYou will receive a 25% penalty for submitting these late.\nI will post my solutions after class."
  },
  {
    "objectID": "teaching/03 Logistic Regression.html#pre-class-code-assignment-instructions",
    "href": "teaching/03 Logistic Regression.html#pre-class-code-assignment-instructions",
    "title": "03 - Logistic Regression",
    "section": "",
    "text": "In this semester, I am going to ask you to do a fair bit of work before coming to class. This will make our class time shorter, more manageable, and hopefully less boring.\nI am also going to use this as an opportunity for you to directly earn grade points for your effort/labor, rather than “getting things right” on an exam.\nTherefore, I will ask you to work through the posted slides on Canvas before class. Throughout the slides, I will post Pre-Class Questions for you to work through in R. These will look like this:\n\nIn R, please write code that will read in the .csv from Canvas called sf_listings_2312.csv. Assign this the name bnb.\nYou will then write your answer in a .r script:\n\n\nClick to show code and output\n\n\n\n# Q1\n#bnb &lt;- read.csv(\"sf_listings_2312.csv\")\n\n\n\n\n\nTo earn full points, you need to organize your code correctly. Specifically, you need to:\n\nAnswer questions in order.\n\nIf you answer them out of order, just re-arrange the code after.\n\nPreface each answer with a comment (# Q1/# Q2/# Q3) that indicates exactly which question you are answering.\n\nPlease just write the letter Q and the number in this comment.\n\nMake sure your code runs on its own, on anyone’s computer.\n\nTo do this, I would always include rm(list = ls()) at the top of every .r script. This will clean everything from the environment, allowing you to see if this runs on my computer.\n\n\n\n\n\n\nYou must submit this to Canvas before 9:00am on the day of class. Even if class starts at 10:00am that day, these are always due at 9:00.\nYou must submit this code as a .txt file. This is because Canvas cannot present .R files to me in SpeedGrader. To save as .txt:\n\nClick File -&gt; New File -&gt; Text File\nCopy and paste your completed code to that new text file.\nSave the file as firstname_lastname_module.txt\n\nFor example, my file for Module 01 would be matt_meister_01.txt\nMy file for module 05 would be matt_meister_05.txt\n\n\n\n\n\n\n\nI will grade these for completion.\nYou will receive 1 point for every question you give an honest attempt to answer\nYour grade will be the number of questions you answer, divided by the total number of questions.\n\nThis is why it is important that you number each answer with # Q1.\nAny questions that are not numbered this way will be graded incomplete, because I can’t find them.\n\nYou will receive a 25% penalty for submitting these late.\nI will post my solutions after class."
  },
  {
    "objectID": "teaching/03 Logistic Regression.html#setup",
    "href": "teaching/03 Logistic Regression.html#setup",
    "title": "03 - Logistic Regression",
    "section": "Setup",
    "text": "Setup\nLoad in these packages. If you do not have them, you will need to install them.\n\ne.g., install.packages(\"dplyr\")\n\n\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\n\nRead in airline_satisfaction.csv from Canvas, and assign it the name data:\n\n\nClick to show code and output\n\n\n\ndata &lt;- read.csv('airline_satisfaction.csv')\n\n\n\nAnd read in airbnb_logistic.csv from Canvas, and assign it the name airbnb.data:\n\n\nClick to show code and output\n\n\n\nairbnb.data &lt;- read.csv('airbnb_logistic.csv')"
  },
  {
    "objectID": "teaching/03 Logistic Regression.html#binary-outcomes",
    "href": "teaching/03 Logistic Regression.html#binary-outcomes",
    "title": "03 - Logistic Regression",
    "section": "Binary Outcomes",
    "text": "Binary Outcomes\nThus far, we’ve spent a lot of time talking about “linear” regression. Linear regression is called linear because it calculates the best equation of a line to fit our data. Effectively–what is the best line we can draw through our data points?\nNow, we will introduce logistic regression. Logistic regression is similar to linear regression, in that we are drawing a line through points in our data, which we do to predict some outcome. However, logistic regression is used in situations where our outcome is either a 0 or a 1 (or a yes/no, or a TRUE/FALSE). With a logistic regression, we are predicting the probability of an outcome. (In linear regression, we are predicting the level of an outcome.)\nFor example, consider last week’s Airbnb data. We might be interested in staying at an Airbnb in North Beach, but terrified that the density of apartments and restaurants–which attracted us in the first place–also means that Airbnbs close to the center of North Beach have a rat problem. Being well-informed statisticians (and newly skilled at text analysis), we could analyze whether reviews are more likely to mention rats if they are closer to the center of North Beach.1\n\nPre-Class Q1\n\nIn the data set airbnb_logistic.csv, you have observations from reviews of the distance of an Airbnb from the center of North Beach (in miles), and whether the review mentions rats. Remember that we can only use a logistic regression on outcomes that are binary. So to start, create a histogram for each variable (distance and rats), to check that rats is binary, and that distance is not crazy.\n\n\nClick to show code and output\n\n\n\nhist(airbnb.data$rats)\n\n\n\n\n\n\n\nhist(airbnb.data$distance)\n\n\n\n\n\n\n\n\n\n\n\nPre-Class Q2\n\nNow, using a linear regression, test whether distance is related to rats. Then, plot this with ggplot(), using geom_smooth(method = \"lm\").\n\n\nClick to show code and output\n\n\n\nsummary(lm(data = airbnb.data, rats ~ distance))\n\n\nCall:\nlm(formula = rats ~ distance, data = airbnb.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.49174 -0.21301 -0.02665  0.15364  0.73748 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.56044    0.05812   9.643 7.21e-16 ***\ndistance    -0.10907    0.01374  -7.940 3.40e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2889 on 98 degrees of freedom\nMultiple R-squared:  0.3915,    Adjusted R-squared:  0.3853 \nF-statistic: 63.05 on 1 and 98 DF,  p-value: 3.399e-12\n\nggplot(data = airbnb.data, \n       aes( x = distance, \n            y = rats)) +\n  geom_smooth(method = 'lm')\n\n\n\n\n\n\n\n\n\n\nYou should see that mention of rats decreases with distance, and that this is statistically significant:\n\n\n\n\n\n\n\n\n\nBecause our DV is either 0 or 1, the coefficient can be interpreted as a change in probability of the outcome with a one-unit change in the IV. And each point on the line shows us the predicted probability of rats at a given distance. In cases like this (i.e., with binary dependent variables), linear regressions are also referred to as “linear probability models”. This is good enough to tell us that rats are more prevalent as we get closer to North Beach. However, there are two oddities you should notice with this relationship:\n\nFirst, linearity:\nA linear regression always predicts straight lines. As a result, it predicts that the relationship between rats and distance is perfectly linear. This regression is saying that the likelihood of encountering rats changes by -10.9%2 with each mile further from North Beach.\nSometimes linearity is correct. Sometimes, people’s behavior does change according to some constant relationship with a predictor. But, often, relationships are not linear.\nOne common example of this in marketing is the relationship between advertising spending and sales. When we increase ad spending, sales also increase. However, this is not linear. When we go from spending $0 to $100, we should see a big bump in sales. But if we go from spending $100,000 to $100,100, it is very unlikely we see any difference at all. This specific case is referred to as “diminishing marginal returns”, and applies to a lot of contexts. You can read more at this link.\nSo in our example, we might be concerned that the relationship between distance and rats is also not linear. We’ll talk about when this is a problem (vs when linearity is “fine enough”) later.\n\n\nSecond, impossible predictions:\nLook back up at the plot you made with your linear prediction. Remember, rats is binary. It can be 0 or 1. Therefore, our predictions are probabilities. They should be between 0 and 1–0 corresponding to “no chance”, and 1 corresponding to “certainty”. Let’s test if we satisfy that here.\n\nPre-Class Q3\n\nUse the predict() function to predict the probability of rats at distances of 0, 1, and 6 miles, using your linear regression. Remember that predict() takes three arguments: (1) An object (which is our model, in this case a linear regression), (2) newdata (which is a data.frame with the exact same variable names as in our regression), and (3) type (which we will always set to response in this class).\n\n\nClick to show code and output\n\n\n\nlinear_model &lt;- lm(data = airbnb.data, rats ~ distance)\nnew_data &lt;- data.frame(distance = c(0, 1, 6))\npredict(object = linear_model,\n        newdata = new_data,\n        type = \"response\")\n\n          1           2           3 \n 0.56044316  0.45137074 -0.09399136 \n\n\n\n\nYou might have expected this after looking at the plot above. When distance is relatively close, your predictions are fine–they are between 0 and 1. But, when distance is 6, your prediction is negative! This happens because a linear regression has to fit a straight line to our data. So, while it fits the best line it can, that linearity can cause it to make impossible predictions–probabilities above 1 and below 0.\nSo maybe we need something better. In class and in your homework, we will see that, often, linear regression is fine in practice. But before we do that, let’s consider the alternative–logistic regression."
  },
  {
    "objectID": "teaching/03 Logistic Regression.html#logistic-regression",
    "href": "teaching/03 Logistic Regression.html#logistic-regression",
    "title": "03 - Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic regression models the probability of an outcome using an S-shaped curve called the logistic function. This S-shape ensures that probabilities are between 0 and 1. To run a logistic regression in R, we use a function very similar to lm(), with two changes. For example, to test our question above using linear regression, we wrote:\nlm(data = airbnb.data, rats ~ distance)\nBut to test our question using logistic regression, we write:\nglm(data = airbnb.data, rats ~ distance, family = \"binomial\")\nThe two changes are:\n\nUsing glm(), which stands for Generalized Linear Model, and allows for different forms of regression.\nSpecifying family = \"binomial\", which tells R that the dependent variable is binary, so it should use the logistic function.\n\nAdditionally, when we plot this using ggplot(), we change the code in just two ways again. Instead of writing geom_smooth(method = 'lm'), we:\n\nChange method = 'lm' to method = 'glm'\nInside of geom_smooth() (after method = 'glm'), we add the argument method.args = \"binomial\"\nNote that sometimes you will also have to add the argument formula = y ~ x\n\n\nPre-Class Q4\n\nNow, using a logistic regression, test whether distance is related to rats. Then, plot this with ggplot(), using geom_smooth(method = \"glm\", method.args = \"binomial\").\n\n\nClick to show code and output\n\n\n\nsummary(glm(data = airbnb.data, rats ~ distance, family = \"binomial\"))\n\n\nCall:\nglm(formula = rats ~ distance, family = \"binomial\", data = airbnb.data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   2.8001     0.9715   2.882 0.003949 ** \ndistance     -2.5101     0.7139  -3.516 0.000438 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 87.934  on 99  degrees of freedom\nResidual deviance: 30.340  on 98  degrees of freedom\nAIC: 34.34\n\nNumber of Fisher Scoring iterations: 8\n\nggplot(data = airbnb.data, \n       aes( x = distance, \n            y = rats)) +\n  geom_smooth(method = \"glm\", \n              method.args = \"binomial\")\n\n\n\n\n\n\n\n\n\n\nIn this case, the S-shape is backwards, because rats decrease with distance. But you should notice that this model tells a very different story, and makes very different predictions to the linear model. This is super obvious if you look at the two plots side-by-side, including points:\n\n\n\n\n\n\n\n\n\nObviously, the linear regression was not flexible to pick up on the fact that this relationship is actually not linear. The difference in rats from 0-1 mile is much larger than the distance from 3-4, or even 3-6! There are also no rats further than 3 miles away, and very few at 2 miles away. The logistic regression can fit itself to all of this information, whereas the linear regression cannot. Ideally, this makes better predictions.\n\nPre-Class Q5\n\nUse the predict() function to predict the probability of rats at distances of 0, 1, and 6 miles, using your logistic regression. Remember that predict() takes three arguments: (1) An object (which is our model, in this case a logistic regression), (2) newdata (which is a data.frame with the exact same variable names as in our regression), and (3) type (which we will always set to response in this class).\n\n\nClick to show code and output\n\n\n\nlogistic_model &lt;- glm(data = airbnb.data, rats ~ distance, family = \"binomial\")\nnew_data &lt;- data.frame(distance = c(0, 1, 6))\npredict(object = logistic_model,\n        newdata = new_data,\n        type = \"response\") |&gt;\n  round(3)\n\n    1     2     3 \n0.943 0.572 0.000 \n\n\n\n\nConveniently, we don’t get any impossible predictions!"
  },
  {
    "objectID": "teaching/03 Logistic Regression.html#coefficients-in-a-logistic-regression",
    "href": "teaching/03 Logistic Regression.html#coefficients-in-a-logistic-regression",
    "title": "03 - Logistic Regression",
    "section": "Coefficients in a logistic regression",
    "text": "Coefficients in a logistic regression\nSo far, logistic regression has been all good news. Now is when the bad begins. Look at the coefficients we got for that logistic regression:\n\n\nLogistic Regression Coefficients\n\n\n\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\n(Intercept)\n2.80\n0.972\n2.882\n0.004\n\n\ndistance\n-2.51\n0.714\n-3.516\n0.000\n\n\n\n\n\n\nHow is the intercept above 1 if the whole curve is between 0 and 1? And how is the coefficient for distance -2.51 if a one-unit change in distance clearly does not lead to a 2.51 unit decrease in rats?\nUnfortunately, explaining this will take some math.\nThe key to understanding these results is to be aware that both the intercept and “slope” in a logistic regression are expressed in log-odds, not probabilities.\n\nOdds are the ratio of the probability of success (\\(p\\)) to failure (\\(1-p\\)).\n\n\\(\\text{odds} = \\frac{\\text{p}}{1-\\text{p}}\\)\nThe log-odds is the natural logarithm of these odds (\\(\\text{log-odds} = \\log(\\frac{\\text{p}}{1-\\text{p}})\\)).\n\nWe’ve seen the natural logarithm before, when we used log() in R.\n\n\nMeanwhile, probability is equal to \\(\\text{p} = \\frac{\\text{odds}}{1 + \\text{odds}}\\)\n\nStarting with:\n\n\\(\\text{odds} = \\frac{p}{1-p}\\)\n\nMultiply both sides by \\(1-p\\) to eliminate the denominator:\n\n\\(\\text{odds} \\times (1-p) = p\\)\n\nDistribute \\(\\text{odds}\\):\n\n\\(\\text{odds} - \\text{odds} \\times p = p\\)\n\nRearrange to isolate \\(p\\):\n\n\\(\\text{odds} = p + \\text{odds} \\times p\\)\n\nFactor \\(p\\) out of the right-hand side:\n\n\\(\\text{odds} = p \\times (1 + \\text{odds})\\)\n\nDivide through by \\(1 + \\text{odds}\\):\n\n\\(p = \\frac{\\text{odds}}{1 + \\text{odds}}\\)\n\n\n\n\nWhy is the intercept above 1?\nConveniently, the intercept is still the value of something when the independent variable is 0. But this is not the predicted value of the dependent variable (as it is with linear regression). With logistic regression, everything is in log-odds, including the intercept.\nThe intercept is the log-odds of the outcome (\\(\\text{rats} = 1\\)) when the independent variable ($ $) is 0.\n\nIn our example\nThe intercept is 2.8. Remember also that probability equals \\(p = \\frac{\\text{odds}}{1 + \\text{odds}}\\). And, that log-odds are \\(\\text{log}(\\text{odds})\\). That means that when distance is zero:\n\\(\\text{log}(\\text{odds}) = 2.8\\)\nYou can convert log-odds to odds by exponentiation. In R, this is the function exp():\n\\(\\text{odds} = e^{2.8}\\) \\(\\text{odds} = 16.44\\)\nWe can sub this into the probability equation:\n\\(p = \\frac{\\text{16.44}}{1+\\text{16.44}}\\)\nAnd then just regular ole’ math:\n\\(p = \\frac{\\text{16.44}}{\\text{17.44}}\\) \\(p \\approx 0.943\\)\nWhen distance is zero, the predicted probability of a review mentioning a rat is \\(\\approx\\) 94.3%.\n\n\nNote:\nThis math is not a lot of fun. Here is a function that will take log-odds, and give you probability. Save this function somewhere and source it when you need to use log-odds.\n\nprob_from_log_odds &lt;- function(log_odds){\n  odds &lt;- exp(log_odds) # Exponentiate first\n  prob &lt;- odds / (1 + odds)\n  return(prob) \n}\n\nHere is how you can use it:\n\n# Provide regression coefficient directly\nprob_from_log_odds(logistic_model$coefficients[[1]])\n\n[1] 0.942682\n\n# Input a number\nprob_from_log_odds(16.44)\n\n[1] 0.9999999\n\n\n\n\n\nHow can the coefficient for distance be -2.51 if a one-unit change doesn’t lead to a 2.51 unit decrease in rats?\nThis happens because, again, the coefficient is expressed in log-odds, not the probability scale. A coefficient of -2.51 means that for a one-unit increase in distance, the log-odds of rats decreases by 2.51.\nIn terms of odds, the change (expressed by \\(\\Delta\\)) is multiplicative:\n\\(\\Delta \\text{log}(\\text{odds}) = -2.51\\) \\(\\Delta \\text{odds} = e^{-2.51}\\) \\(\\Delta \\text{odds} \\approx 0.081\\)\nThis means the odds of rats are reduced by 8.1% of the prior odds for each one-unit increase in distance. This is what allows the logistic regression to curve.\n\n\nMy suggestion\nLogistic regression coefficients are quite confusing if you’re not well-versed in this math. This interpretability is their biggest flaw. Instead of trying to understand them in detail, I would use the predict() function in R, to see how your predicted probabilities change at different levels of your independent variable.\nWhat you need to take away from this is not the specifics of the math, but the ideas that:\n\nLogistic regression provides coefficients in log-odds, not probabilities.\nThis allows the logistic regression function to not hit 0 or 1.\nThis also allows the logistic regression function to curve, identifying non-linear relationships between our dependent variable and independent variable."
  },
  {
    "objectID": "teaching/03 Logistic Regression.html#airline-satisfaction",
    "href": "teaching/03 Logistic Regression.html#airline-satisfaction",
    "title": "03 - Logistic Regression",
    "section": "Airline Satisfaction",
    "text": "Airline Satisfaction\nThe data set airline_satisfaction.csv on Canvas contains the results of a survey of airline customers. These responses were collected immediately after getting off a plane trip. The airline collected some personal and travel information from each customer, and then asked about their satisfaction with a number of elements of their trip.\nIn class, we will practice testing logistic regressions, comparing them to linear regressions, and show how we can make predictions of new data with these functions. In the pre-class workshop, I would like you to orient yourself with the data.\n\nPre-Class Q6\n\nOur dependent variable is satisfaction. Summarise this variable, and see what (if anything) we have to do in order to use it in a regression.\n\n\nClick to show code and output\n\n\n\nsummary(data$satisfaction)\n\n   Length     Class      Mode \n    25976 character character \n\nunique(data$satisfaction)\n\n[1] \"satisfied\"               \"neutral or dissatisfied\"\n\n#This variable is a character, so we are going to have to turn it into 0/1\ndata$satisfied &lt;- ifelse(data$satisfaction == \"satisfied\", 1, 0)\n\n\n\n\nPre-Class Q7\n\nWhat percentage of customers are satisfied?\n\n\nClick to show code and output\n\n\n\nmean(data$satisfied)\n\n[1] 0.4389821\n\n\n\n\n\nPre-Class Q8\n\nNow, let’s get some ideas for a potential hypothesis. You can look at the other variables we have with str() and summary().\n\n\nClick to show code and output\n\n\n\nstr(data)\n\n'data.frame':   25976 obs. of  26 variables:\n $ X                                : int  0 1 2 3 4 5 6 7 8 9 ...\n $ id                               : int  19556 90035 12360 77959 36875 39177 79433 97286 27508 62482 ...\n $ Gender                           : chr  \"Female\" \"Female\" \"Male\" \"Male\" ...\n $ Customer.Type                    : chr  \"Loyal Customer\" \"Loyal Customer\" \"disloyal Customer\" \"Loyal Customer\" ...\n $ Age                              : int  52 36 20 44 49 16 77 43 47 46 ...\n $ Type.of.Travel                   : chr  \"Business travel\" \"Business travel\" \"Business travel\" \"Business travel\" ...\n $ Class                            : chr  \"Eco\" \"Business\" \"Eco\" \"Business\" ...\n $ Flight.Distance                  : int  160 2863 192 3377 1182 311 3987 2556 556 1744 ...\n $ Inflight.wifi.service            : int  5 1 2 0 2 3 5 2 5 2 ...\n $ Departure.Arrival.time.convenient: int  4 1 0 0 3 3 5 2 2 2 ...\n $ Ease.of.Online.booking           : int  3 3 2 0 4 3 5 2 2 2 ...\n $ Gate.location                    : int  4 1 4 2 3 3 5 2 2 2 ...\n $ Food.and.drink                   : int  3 5 2 3 4 5 3 4 5 3 ...\n $ Online.boarding                  : int  4 4 2 4 1 5 5 4 5 4 ...\n $ Seat.comfort                     : int  3 5 2 4 2 3 5 5 5 4 ...\n $ Inflight.entertainment           : int  5 4 2 1 2 5 5 4 5 4 ...\n $ On.board.service                 : int  5 4 4 1 2 4 5 4 2 4 ...\n $ Leg.room.service                 : int  5 4 1 1 2 3 5 4 2 4 ...\n $ Baggage.handling                 : int  5 4 3 1 2 1 5 4 5 4 ...\n $ Checkin.service                  : int  2 3 2 3 4 1 4 5 3 5 ...\n $ Inflight.service                 : int  5 4 2 1 2 2 5 4 3 4 ...\n $ Cleanliness                      : int  5 5 2 4 4 5 3 3 5 4 ...\n $ Departure.Delay.in.Minutes       : int  50 0 0 0 0 0 0 77 1 28 ...\n $ Arrival.Delay.in.Minutes         : num  44 0 0 6 20 0 0 65 0 14 ...\n $ satisfaction                     : chr  \"satisfied\" \"satisfied\" \"neutral or dissatisfied\" \"satisfied\" ...\n $ satisfied                        : num  1 1 0 1 1 1 1 1 1 1 ...\n\nsummary(data)\n\n       X               id            Gender          Customer.Type     \n Min.   :    0   Min.   :    17   Length:25976       Length:25976      \n 1st Qu.: 6494   1st Qu.: 32170   Class :character   Class :character  \n Median :12988   Median : 65320   Mode  :character   Mode  :character  \n Mean   :12988   Mean   : 65006                                        \n 3rd Qu.:19481   3rd Qu.: 97584                                        \n Max.   :25975   Max.   :129877                                        \n                                                                       \n      Age        Type.of.Travel        Class           Flight.Distance\n Min.   : 7.00   Length:25976       Length:25976       Min.   :  31   \n 1st Qu.:27.00   Class :character   Class :character   1st Qu.: 414   \n Median :40.00   Mode  :character   Mode  :character   Median : 849   \n Mean   :39.62                                         Mean   :1194   \n 3rd Qu.:51.00                                         3rd Qu.:1744   \n Max.   :85.00                                         Max.   :4983   \n                                                                      \n Inflight.wifi.service Departure.Arrival.time.convenient Ease.of.Online.booking\n Min.   :0.000         Min.   :0.000                     Min.   :0.000         \n 1st Qu.:2.000         1st Qu.:2.000                     1st Qu.:2.000         \n Median :3.000         Median :3.000                     Median :3.000         \n Mean   :2.725         Mean   :3.047                     Mean   :2.757         \n 3rd Qu.:4.000         3rd Qu.:4.000                     3rd Qu.:4.000         \n Max.   :5.000         Max.   :5.000                     Max.   :5.000         \n                                                                               \n Gate.location   Food.and.drink  Online.boarding  Seat.comfort  \n Min.   :1.000   Min.   :0.000   Min.   :0.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000   1st Qu.:2.000  \n Median :3.000   Median :3.000   Median :4.000   Median :4.000  \n Mean   :2.977   Mean   :3.215   Mean   :3.262   Mean   :3.449  \n 3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:5.000  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :5.000  \n                                                                \n Inflight.entertainment On.board.service Leg.room.service Baggage.handling\n Min.   :0.000          Min.   :0.000    Min.   :0.00     Min.   :1.000   \n 1st Qu.:2.000          1st Qu.:2.000    1st Qu.:2.00     1st Qu.:3.000   \n Median :4.000          Median :4.000    Median :4.00     Median :4.000   \n Mean   :3.358          Mean   :3.386    Mean   :3.35     Mean   :3.633   \n 3rd Qu.:4.000          3rd Qu.:4.000    3rd Qu.:4.00     3rd Qu.:5.000   \n Max.   :5.000          Max.   :5.000    Max.   :5.00     Max.   :5.000   \n                                                                          \n Checkin.service Inflight.service  Cleanliness    Departure.Delay.in.Minutes\n Min.   :1.000   Min.   :0.000    Min.   :0.000   Min.   :   0.00           \n 1st Qu.:3.000   1st Qu.:3.000    1st Qu.:2.000   1st Qu.:   0.00           \n Median :3.000   Median :4.000    Median :3.000   Median :   0.00           \n Mean   :3.314   Mean   :3.649    Mean   :3.286   Mean   :  14.31           \n 3rd Qu.:4.000   3rd Qu.:5.000    3rd Qu.:4.000   3rd Qu.:  12.00           \n Max.   :5.000   Max.   :5.000    Max.   :5.000   Max.   :1128.00           \n                                                                            \n Arrival.Delay.in.Minutes satisfaction         satisfied    \n Min.   :   0.00          Length:25976       Min.   :0.000  \n 1st Qu.:   0.00          Class :character   1st Qu.:0.000  \n Median :   0.00          Mode  :character   Median :0.000  \n Mean   :  14.74                             Mean   :0.439  \n 3rd Qu.:  13.00                             3rd Qu.:1.000  \n Max.   :1115.00                             Max.   :1.000  \n NA's   :83                                                 \n\n\n\n\n\nPre-Class Q9\n\nCreate a scatterplot to visualize the relationship between Flight.Distance and satisfied.\n\n\nClick to show code and output\n\n\n\nggplot(data = data, aes(x = Flight.Distance, y = satisfied)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", \n              method.args = \"binomial\") +\n  coord_cartesian(ylim = c(-.1,1.1))\n\n\n\n\n\n\n\n\n\n\n\nPre-Class Q10\n\nIt looks like Flight.Distance might have some outliers. We can see if this is the case with a histogram.\n\n\nClick to show code and output\n\n\n\nhist(data$Flight.Distance)\n\n\n\n\n\n\n\n\n\n\n\nPre-Class Q11\n\nTo fix Flight.Distance, let’s log-transform it with log(). Assign this to the variable data$log_Flight.Distance.\n\n\nClick to show code and output\n\n\n\ndata$log_Flight.Distance &lt;- log(data$Flight.Distance)\nhist(data$log_Flight.Distance)"
  },
  {
    "objectID": "teaching/03 Logistic Regression.html#if-you-stop-here",
    "href": "teaching/03 Logistic Regression.html#if-you-stop-here",
    "title": "03 - Logistic Regression",
    "section": "If you stop here",
    "text": "If you stop here\nYou are ready for class. If you want extra practice and full points, try these questions\n\nPre-Class Q12\n\nYou might have noticed that the logistic regression fit our airbnb.data nicely, but that the coefficients it gave us were… gross. In class, I will talk about how in practice, a linear regression is often good enough, because our relationships are often close enough to linear. True, strong S-shapes are rare in nature, because they suggest that our outcome suddenly flips/turns on at some level.\nHowever, even when our data do follow an S-shape, we can often still manipulate our variables to make a linear regression make sense. For extra practice and a challenge, I would like you to try manipulating the variable distance so that a linear regression will make sense and fit the data. Try doing this without looking at my answer first, and submit something of your own creation!\n\n\nOption 1: Click to show code and output\n\n\n&lt;div&gt; I am eyeballing distance = 1 as a point where the likelihood of rats changes sharply. I'll use this point to split our data. &lt;/div&gt;\n\nairbnb.data$distance_bin &lt;- ifelse(airbnb.data$distance &gt; 1, 1, 0)\nlinear_model_bin &lt;- lm(data = airbnb.data, rats ~ distance_bin)\nsummary(linear_model_bin)\n\n\nCall:\nlm(formula = rats ~ distance_bin, data = airbnb.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.86667 -0.03529 -0.03529 -0.03529  0.96471 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.86667    0.05611   15.45   &lt;2e-16 ***\ndistance_bin -0.83137    0.06086  -13.66   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2173 on 98 degrees of freedom\nMultiple R-squared:  0.6557,    Adjusted R-squared:  0.6522 \nF-statistic: 186.6 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\nggplot(data = airbnb.data, \n       aes( x = distance_bin, \n            y = rats)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  coord_cartesian(ylim = c(-.1,1.1))\n\n\n\n\n\n\n\nnew_data &lt;- data.frame(distance = c(0, 1, 6))\nnew_data$distance_bin &lt;- ifelse(new_data$distance &gt; 1, 1, 0)\npredict(object = linear_model_bin,\n        newdata = new_data,\n        type = \"response\") |&gt;\n  round(3)\n\n    1     2     3 \n0.867 0.867 0.035 \n\n\n\n\n\n\nOption 2: Click to show code and output\n\n\n&lt;div&gt; If we think there is a linear relationship in some subset of distance, but no relationship in another, we could create a variable that just cuts off distance when we think the relationship stops. &lt;/div&gt;\n\nairbnb.data$distance_cut &lt;- ifelse(airbnb.data$distance &gt; 1, 1, \n                                   airbnb.data$distance) # Here, we are keeping distances inside of 1\nlinear_model_cut &lt;- lm(data = airbnb.data, rats ~ distance_cut)\nsummary(linear_model_cut)\n\n\nCall:\nlm(formula = rats ~ distance_cut, data = airbnb.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.47527 -0.06613 -0.06613 -0.06613  0.93387 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.17150    0.09542   12.28   &lt;2e-16 ***\ndistance_cut -1.10536    0.10069  -10.98   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.248 on 98 degrees of freedom\nMultiple R-squared:  0.5515,    Adjusted R-squared:  0.5469 \nF-statistic: 120.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\nggplot(data = airbnb.data, \n       aes( x = distance_cut, \n            y = rats)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  coord_cartesian(ylim = c(-.1,1.1))\n\n\n\n\n\n\n\nnew_data &lt;- data.frame(distance = c(0, 1, 6))\nnew_data$distance_cut &lt;- ifelse(new_data$distance &gt; 1, 1, \n                                   new_data$distance)\npredict(object = linear_model_cut,\n        newdata = new_data,\n        type = \"response\") |&gt;\n  round(3)\n\n    1     2     3 \n1.171 0.066 0.066 \n\n\n\n\n\nPre-Class Q13\n\nThe whole point of the classes you have with me is learning how to make better predictions. That is especially true for logistic regression, where we are taking something we knew well (linear regression), and adding a lot of complexity.\nFor that complexity to be worth it, we have to be making better predictions. To see if we are, we can:\n\nSplit our data into training and testing sets\nTrain a regression on the training set\nMake predictions on the testing set\nSee how close those are to our observations\n\nIn this question, I want you to split your airbnb.data into a training set of 80 rows, and a testing set of 20. Then, train a logistic regression on the training set, and see how well it predicts the test data.\n\n\nClick to show code and output\n\n\n\nset.seed(123)\n#Shuffle airbnb.data\nairbnb.data &lt;- airbnb.data[sample(1:nrow(airbnb.data), size = nrow(airbnb.data)),]\ntrain_df &lt;- airbnb.data[1:80, ]\ntest_df &lt;- airbnb.data[81:100, ]\n\nlogistic_model &lt;- glm(data = train_df, rats ~ distance, family = \"binomial\")\nsummary(logistic_model)\n\n\nCall:\nglm(formula = rats ~ distance, family = \"binomial\", data = train_df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   3.0339     1.1768   2.578  0.00993 **\ndistance     -2.5553     0.8314  -3.074  0.00211 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 71.007  on 79  degrees of freedom\nResidual deviance: 26.341  on 78  degrees of freedom\nAIC: 30.341\n\nNumber of Fisher Scoring iterations: 8\n\ntest_df$prediction &lt;- predict(logistic_model,\n                              newdata = test_df,\n                              type = \"response\")\n\n# MSE\nmean((test_df$prediction - test_df$rats)^2)\n\n[1] 0.03454432\n\n# Make a \"confusion matrix\" by binarizing the DV\ntest_df$predicted_rats &lt;- ifelse(test_df$prediction &gt;= .5, \"Predict Rats\", \"Predict No Rats\")\ntable(test_df$predicted_rats, test_df$rats)\n\n                 \n                   0  1\n  Predict No Rats 16  0\n  Predict Rats     1  3\n\n\n\n\n\nPre-Class Q14\n\nNow, train a linear regression on the training set, and see how well it predicts the test data.\n\n\nClick to show code and output\n\n\n\nlinear_model &lt;- lm(data = train_df, rats ~ distance)\nsummary(linear_model)\n\n\nCall:\nlm(formula = rats ~ distance, data = train_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.45942 -0.22120 -0.03261  0.15540  0.73726 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.56308    0.06713   8.388 1.66e-12 ***\ndistance    -0.10996    0.01605  -6.853 1.50e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2952 on 78 degrees of freedom\nMultiple R-squared:  0.3758,    Adjusted R-squared:  0.3678 \nF-statistic: 46.96 on 1 and 78 DF,  p-value: 1.501e-09\n\ntest_df$prediction &lt;- predict(linear_model,\n                              newdata = test_df,\n                              type = \"response\")\n\n# MSE\nmean((test_df$prediction - test_df$rats)^2)\n\n[1] 0.06913223\n\n# Make a \"confusion matrix\" by binarizing the DV\ntest_df$predicted_rats &lt;- ifelse(test_df$prediction &gt;= .5, \"Predict Rats\", \"Predict No Rats\")\ntable(test_df$predicted_rats, test_df$rats)\n\n                 \n                   0  1\n  Predict No Rats 17  0\n  Predict Rats     0  3"
  },
  {
    "objectID": "teaching/03 Logistic Regression.html#footnotes",
    "href": "teaching/03 Logistic Regression.html#footnotes",
    "title": "03 - Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn this example, our hypothesis is that being further from North Beach leads to a decrease in the mention of “rats” in reviews. The null hypothesis is that distance from North Beach has no effect on the mention of rats.↩︎\nBecause our DV is either 0 or 1, the coefficient can be interpreted as a change in probability of the outcome with a one-unit change in the IV.↩︎"
  }
]