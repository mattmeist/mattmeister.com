---
title: "Segmentation"
author: "Matt Meister"
subtitle: "University of San Francisco"
include-in-header:
  - text: |
      <style>
      #title-slide .title {
        font-size: 3em;
        color: black;
      }
      </style>
format: 
  revealjs:
    self-contained: true
    scrollable: true
    theme: simple
    fontsize: 20pt
code-block-bg: true
code-block-border-left: "#31BAE9"
editor: source
execute: 
  error: true
  echo: true
  include: true
  warning: false
---

```{r, include=FALSE}
package.load <- function(pkg) {
  if (!require(pkg, character.only = TRUE)) {
    suppressMessages(install.packages(pkg, dependencies = TRUE))
    suppressMessages(library(pkg, character.only = TRUE))
  } else {
    suppressMessages(library(pkg, character.only = TRUE))
  }
}

pkgs <- c("dplyr", "ggplot2", "tidyr")

for (pkg in pkgs) {
  package.load(pkg)
}

```

## Segmentation in Marketing

-   Objectives
-   Steps
    -   Data selection
    -   Distance measures
    -   Clustering algorithms
    -   Selecting the number of clusters
    -   Profiling of clusters


## Segmentation in Marketing

::: incremental

- Market segmentation is one of the most fundamental marketing concepts:
  - Grouping people (with the willingness and ability to buy) according to their similarity on dimensions related to product(s) under consideration
- Better segments chosen -> better success
  - Customize products and distribution strategies for target segments
    - Higher customer satisfaction, retention
  - Customize promotions for target segments
    - Higher customer acquisition, retention, up-selling
  - Customize prices for target segments
    - Extract as much \$ from targeted customers as possible

:::

## Segmentation: Marketer’s dilemma

We're making a trade-off as marketers between:

::: incremental

- Market segmentation *(Heterogeneous market)*
- Market aggregation *(Homogeneous market)*

:::

## Segmentation: Marketer’s dilemma

- Market segmentation *(Heterogeneous market)*
  - Higher revenue
  - Individually-customized products
    - High production/admin costs
    - Appeals to many
    - Priced near consumer WTP
- Market aggregation *(Homogeneous market)*
  - Lower costs
  - Single product
    - Low production/admin costs
    - Does not appeal to some
    - Priced too low for some
- Forming a **few** market segments strikes balance between extremes


## Segmentation 

Criteria

::: incremental

- Substantial
  - Measurable market size
  - Large enough to warrant serving
- Actionable
  - Segment characteristics can be translated into targeted marketing policies
    - *e.g.,* age/income differences suggest different promotional vehicles
  - Targeted policies must be consistent with firm abilities
- Differential
  - Differences between segments should be clearly defined
  - Segment-specific marketing policies can be implemented without overlap

:::

## Segmentation

Approaches

::: incremental

- Manual
  - Managerial experience, industry norms used to determine segments
  - Bad
- Automatic
  - Form segments using data-driven techniques
- Hybrid
  - Combination of manual & automatic (often best)
- Cluster analysis is the most commonly used data-driven technique for segmenting customer data

:::

# Cluster analysis

## Cluster analysis

::: incremental

- What?
  - Grouping of objects (e.g. customers) by similarity of attributes (variables)
  - Objects in a group similar, distinct from objects in other groups
  - Natural relationship to market segmentation
- How?
  - Calculates pairwise similarity measures
    - Some measure of "distance" between objects (customers)
    - Many possible distance metrics (Euclidian, Gower, etc.)
  - Searches for "best" groupings using a clustering method (algorithm)
    - Many possible clustering methods (k-means, hierarchical, etc.)

:::

## Cluster analysis

Steps

::: incremental

1. Select variables to use for clustering
2. Define distance measure between individuals (Euclidean, Gower, etc.)
3. Select clustering procedure (k-means, hierarchical, etc.)
4. Select number of clusters
5. Interpret and profile the clusters

:::

## Step 1: Select variables 

Data sources

::: incremental

- Past behavior & derived metrics (firm’s CRM/transactions database)
  - Past expenditure levels
  - Recency & frequency of purchase
  - Lifetime value (CLV)
- Preference measures (CRM, survey/experimental research)
  - Conduct individual demand analyses, recover individual-specific parameters
  - Use individual-specific parameters as input to cluster analysis
- Demographic variables (CRM database, Census records)
  - Directly observed — often limited, but sometimes directly observe gender, age, etc.
  - Imputed from geography — e.g., use knowledge of home ZIP to match Census demographics for region

:::

## Step 1: Select variables 

```{r}
DF <- read.csv('apparel_customer_data.csv')
```

::: fragment

```{r}
summary(DF)
```

:::


## Step 1: Select variables

Data pre-processing

::: incremental

- Clustering algorithms typically perform better on normally distributed data

:::

## Step 1: Select variables

Data pre-processing

::: incremental

- Clustering algorithms typically perform better on normally distributed data
- Often, we "normalize" data, using two techniques:
  - Log-transform of highly skewed variables — to reduce skew & outlier influence
  - Standardization of variables — de-mean and rescale variance to 1
- Best practice is to generate histograms of potential cluster variables
  - Infer from distribution plots which variables to log-transform

:::

## Step 1: Select variables

Data pre-processing

:::: {.columns}

::: {.column width="50%"}

::: fragment

```{r, echo = F}
hist(DF$spend_online, 50)
```

:::

:::

::: {.column width="50%"}

::: fragment

```{r}
DF$log_spend_online <- log(1+DF$spend_online)
```

```{r, echo = F}
hist(DF$log_spend_online, 50)
```

:::

:::

::::

## Step 1: Select variables

Data pre-processing

:::: {.columns}

::: {.column width="50%"}

::: fragment

```{r, echo = F}
hist(DF$spend_retail, 50)
```

:::

:::

::: {.column width="50%"}

::: fragment

```{r}
DF$log_spend_retail <- log(1+DF$spend_retail)
```

```{r, echo = F}
hist(DF$log_spend_retail, 50)
```

:::

:::

::::


## Step 2: Define distance between individuals

::: incremental

- We measure similarity between two customers by calculating the "distance" between them
  - Distance can be measured in different ways, depending on data type
- Two most general-purpose distance metrics
  - **Euclidean** — if all data is continuous, and not multi-modal
  - **Gower** — can be used with mixed (discrete data/dummy variables and continuous) data; also can work better with multi-modal data
  
:::

## Step 2: Define distance between individuals

::: incremental

- Euclidean distance (continuous data)
  - Usually applied to standardized data to give the same weight to all the variables
  - Essentially the standard deviation between customers $i$ and $j$ (over $K$ variables):
    - Calculate difference between $i$ and $j$ on each individual variable
    - Square all of those *(to make them positive)*
    - Add those up
    - Take square root of that
  
:::

## Step 2: Define distance between individuals

Is `199` more similar to `1163` or `9594`?

::: fragment

What is the Euclidean distance between customers `199` and `1163` on `spend_online`, `hh_inc`?

**Standardize first**

```{r}
DF$spend_online_standardized <- scale(DF$spend_online)[,1]
DF$hh_inc_standardized <- scale(DF$hh_inc)[,1]
```

:::

::: fragment

```{r}
spend_online_dif <- ( DF[ DF$iid == 199, ]$spend_online_standardized - DF[ DF$iid == 1163, ]$spend_online_standardized) ^ 2
hh_inc_dif <- ( DF[ DF$iid == 199, ]$hh_inc_standardized - DF[ DF$iid == 1163, ]$hh_inc_standardized) ^ 2

ed_199_1163 <- sqrt( spend_online_dif + hh_inc_dif)
ed_199_1163
```

:::

## Step 2: Define distance between individuals

What is the Euclidean distance between customers `199` and `9594` on `spend_online`, `hh_inc`?

::: fragment

```{r}
spend_online_dif <- ( DF[ DF$iid == 199, ]$spend_online_standardized - DF[ DF$iid == 9594, ]$spend_online_standardized) ^ 2
hh_inc_dif <- ( DF[ DF$iid == 199, ]$hh_inc_standardized - DF[ DF$iid == 9594, ]$hh_inc_standardized) ^ 2

ed_199_9594 <- sqrt( spend_online_dif + hh_inc_dif)
ed_199_9594
```

:::

## Step 2: Define distance between individuals

::: incremental

- Gower distance (mixed continuous & dummy variables)
  - Also sometimes useful for continuous data with more than 1 mode
  - Standardization is effectively "built-in" to the distance formula

:::

## Step 2: Define distance measure between individuals

R implementation of Euclidean & Gower distance

`daisy()` function in `cluster` package

::: incremental

- Euclidean:
  - `daisy(DF, metric = "euclidean", warnType = FALSE, stand = TRUE)`
    - `DF`: dataframe with all continuous variables
    - `warnType=FALSE` to silence warnings
    - `stand=TRUE` to standardize variables
- Gower distance:
  - `daisy(DF, metric = "gower", warnType=FALSE)`
    - `DF`: dataframe with continuous/binary variables
    - `warnType=FALSE` to silence warnings

:::

## Step 2: Define distance measure between individuals

R implementation of Euclidean & Gower distance

`daisy()` function in `cluster` package

```{r}
DF <- read.csv('apparel_customer_data.csv')

library(cluster)

DF_euclidean <- daisy(DF, metric = "euclidean", warnType = FALSE, stand = TRUE)
```


## Step 2: Define distance measure between individuals

R implementation of Euclidean & Gower distance

`daisy()` function in `cluster` package

```{r}
DF_gower <- daisy(DF, metric = "gower", warnType = FALSE)
```

## Step 3: Select clustering procedure 

Two main branches of clustering algorithms

::: incremental

- Hierarchical
  - Iteratively build up clusters from nearest (distance-wise) pairs
  - Cluster "tree" cut at depth = # of desired clusters
- Nonhierarchical
  - Number of segments is specified by the analyst
  - K-means maximizes ratio of between-cluster variance to within-cluster variance
  - Model based approaches use classifiers such as logistic regression
- Most often, with "right" distance measure, get similar results
  - We focus on k-means

:::

## Step 3: Select clustering procedure 

K-means clustering algorithm

::: incremental

- Each cluster is associated with a centroid (center point)
- Each point is assigned to the cluster with the closest centroid
- Number of clusters, $K$, must be specified
- The basic algorithm is very simple:
  - Randomly select $K$ points as the initial centroids
  - Assign all other points to the cluster with the nearest centroid
  - Re-compute centroid as the average in that cluster
  - Reassign each point to the cluster with the nearest centroid
  - Re-compute centroid as the average in that cluster
  - Repeat until points don't change

:::

## Step 3: Select clustering procedure 

K-means clustering algorithm

```{r, echo = F}
# Compute k-means with k = 3
set.seed(123)
df <- data.frame(
  x = rnorm(100),
  y = rnorm(100)
)

ggplot( data = df, 
        aes( x = x, y = y)) +
  geom_point(size = 2, alpha = .7) +
  theme_minimal()
```

## Step 3: Select clustering procedure 

K-means clustering algorithm

- Select 3 points as the start:

```{r}
centroids <- sample( 1:nrow(df), 3)
```

```{r, echo = F}
ggplot( data = df, 
        aes( x = x, y = y)) +
  geom_point(size = 2, alpha = .7) +
  geom_point(data = df[centroids[1], ], size = 4, color = "red", alpha = 1) +
  geom_point(data = df[centroids[2], ], size = 4, color = "blue", alpha = 1) +
  geom_point(data = df[centroids[3], ], size = 4, color = "green", alpha = 1) +
  theme_minimal()
```

## Step 3: Select clustering procedure 

K-means clustering algorithm

- Assign points to closest cluster:

```{r}
df$distance_1 <- sqrt( (df$x - df[centroids[1],]$x) ^ 2 + (df$y - df[centroids[1],]$y) ^ 2)
df$distance_2 <- sqrt( (df$x - df[centroids[2],]$x) ^ 2 + (df$y - df[centroids[2],]$y) ^ 2)
df$distance_3 <- sqrt( (df$x - df[centroids[3],]$x) ^ 2 + (df$y - df[centroids[3],]$y) ^ 2)

df$cluster <- ifelse(df$distance_1 < df$distance_2 & df$distance_1 < df$distance_3, 1,
                     ifelse( df$distance_2 < df$distance_1 & df$distance_2 < df$distance_3, 2,
                             ifelse( df$distance_3 < df$distance_1 & df$distance_3 < df$distance_2, 3, NA_real_)))
```

```{r, echo = F}
df$cluster <- as.factor(df$cluster)
ggplot( data = df, 
        aes( x = x, y = y, color = cluster)) +
  geom_point(size = 2, alpha = .7) +
  geom_point(data = df[centroids[1], ], size = 4, color = "red", alpha = 1) +
  geom_point(data = df[centroids[2], ], size = 4, color = "green", alpha = 1) +
  geom_point(data = df[centroids[3], ], size = 4, color = "blue", alpha = 1) +
  theme_minimal()
```

## Step 3: Select clustering procedure 

K-means clustering algorithm

- Calculate new centroids:

```{r}
centroids <- data.frame(
  cluster = as.factor(c(1, 2, 3)),
  x = c(mean(df[ df$cluster == 1, ]$x), mean(df[ df$cluster == 2, ]$x), mean(df[ df$cluster == 3, ]$x)),
  y = c(mean(df[ df$cluster == 1, ]$y), mean(df[ df$cluster == 2, ]$y), mean(df[ df$cluster == 3, ]$y))
)
```

```{r, echo = F}
ggplot( data = df, 
        aes( x = x, y = y, color = cluster)) +
  geom_point(size = 2, alpha = .7) +
  geom_point(data = centroids[1,], size = 4, color = "red", alpha = 1) +
  geom_point(data = centroids[2, ], size = 4, color = "green", alpha = 1) +
  geom_point(data = centroids[3, ], size = 4, color = "blue", alpha = 1) +
  theme_minimal()

```

## Step 3: Select clustering procedure 

K-means clustering algorithm

- Re-assign points:

```{r}
df$distance_1 <- sqrt( (df$x - centroids[1,]$x) ^ 2 + (df$y - centroids[1,]$y) ^ 2)
df$distance_2 <- sqrt( (df$x - centroids[2,]$x) ^ 2 + (df$y - centroids[2,]$y) ^ 2)
df$distance_3 <- sqrt( (df$x - centroids[3,]$x) ^ 2 + (df$y - centroids[3,]$y) ^ 2)

df$cluster_2 <- ifelse(df$distance_1 < df$distance_2 & df$distance_1 < df$distance_3, 1,
                     ifelse( df$distance_2 < df$distance_1 & df$distance_2 < df$distance_3, 2,
                             ifelse( df$distance_3 < df$distance_1 & df$distance_3 < df$distance_2, 3, NA_real_)))

sum(df$cluster != df$cluster_2)
```

```{r, echo = F}
df$cluster_2 <- as.factor(df$cluster_2)
ggplot( data = df, 
        aes( x = x, y = y, color = cluster_2)) +
  geom_point(size = 2, alpha = .7) +
  geom_point(data = centroids[1,], size = 4, color = "red", alpha = 1) +
  geom_point(data = centroids[2, ], size = 4, color = "green", alpha = 1) +
  geom_point(data = centroids[3, ], size = 4, color = "blue", alpha = 1) +
  geom_point(data = df[df$cluster != df$cluster_2, ], size = 4, alpha = 1, shape = 5) +
  theme_minimal()
```

## Step 3:

- Calculate new centroids:

```{r}
centroids <- data.frame(
  cluster = as.factor(c(1, 2, 3)),
  x = c(mean(df[ df$cluster_2 == 1, ]$x), mean(df[ df$cluster_2 == 2, ]$x), mean(df[ df$cluster_2 == 3, ]$x)),
  y = c(mean(df[ df$cluster_2 == 1, ]$y), mean(df[ df$cluster_2 == 2, ]$y), mean(df[ df$cluster_2 == 3, ]$y))
)
```

```{r, echo = F}
ggplot( data = df, 
        aes( x = x, y = y, color = cluster)) +
  geom_point(size = 2, alpha = .7) +
  geom_point(data = centroids[1,], size = 4, color = "red", alpha = 1) +
  geom_point(data = centroids[2, ], size = 4, color = "green", alpha = 1) +
  geom_point(data = centroids[3, ], size = 4, color = "blue", alpha = 1) +
  theme_minimal()

```

## Step 3: Select clustering procedure 

K-means clustering algorithm

- Re-assign points:

```{r}
df$distance_1 <- sqrt( (df$x - centroids[1,]$x) ^ 2 + (df$y - centroids[1,]$y) ^ 2)
df$distance_2 <- sqrt( (df$x - centroids[2,]$x) ^ 2 + (df$y - centroids[2,]$y) ^ 2)
df$distance_3 <- sqrt( (df$x - centroids[3,]$x) ^ 2 + (df$y - centroids[3,]$y) ^ 2)

df$cluster_3 <- ifelse(df$distance_1 < df$distance_2 & df$distance_1 < df$distance_3, 1,
                     ifelse( df$distance_2 < df$distance_1 & df$distance_2 < df$distance_3, 2,
                             ifelse( df$distance_3 < df$distance_1 & df$distance_3 < df$distance_2, 3, NA_real_)))

sum(df$cluster_2 != df$cluster_3)
```

```{r, echo = F}
df$cluster_3 <- as.factor(df$cluster_3)
ggplot( data = df, 
        aes( x = x, y = y, color = cluster_3)) +
  geom_point(size = 2, alpha = .7) +
  geom_point(data = centroids[1,], size = 4, color = "red", alpha = 1) +
  geom_point(data = centroids[2, ], size = 4, color = "green", alpha = 1) +
  geom_point(data = centroids[3, ], size = 4, color = "blue", alpha = 1) +
  geom_point(data = df[df$cluster_2 != df$cluster_3, ], size = 4, alpha = 1, shape = 5) +
  theme_minimal()
```

## Step 3:

- Calculate new centroids:

```{r}
centroids <- data.frame(
  cluster = as.factor(c(1, 2, 3)),
  x = c(mean(df[ df$cluster_3 == 1, ]$x), mean(df[ df$cluster_3 == 2, ]$x), mean(df[ df$cluster_3 == 3, ]$x)),
  y = c(mean(df[ df$cluster_3 == 1, ]$y), mean(df[ df$cluster_3 == 2, ]$y), mean(df[ df$cluster_3 == 3, ]$y))
)
```

```{r, echo = F}
ggplot( data = df, 
        aes( x = x, y = y, color = cluster)) +
  geom_point(size = 2, alpha = .7) +
  geom_point(data = centroids[1,], size = 4, color = "red", alpha = 1) +
  geom_point(data = centroids[2, ], size = 4, color = "green", alpha = 1) +
  geom_point(data = centroids[3, ], size = 4, color = "blue", alpha = 1) +
  theme_minimal()

```

## Step 3:

K-means clustering algorithm

- Re-assign points:

```{r}
df$distance_1 <- sqrt( (df$x - centroids[1,]$x) ^ 2 + (df$y - centroids[1,]$y) ^ 2)
df$distance_2 <- sqrt( (df$x - centroids[2,]$x) ^ 2 + (df$y - centroids[2,]$y) ^ 2)
df$distance_3 <- sqrt( (df$x - centroids[3,]$x) ^ 2 + (df$y - centroids[3,]$y) ^ 2)

df$cluster_4 <- ifelse(df$distance_1 < df$distance_2 & df$distance_1 < df$distance_3, 1,
                     ifelse( df$distance_2 < df$distance_1 & df$distance_2 < df$distance_3, 2,
                             ifelse( df$distance_3 < df$distance_1 & df$distance_3 < df$distance_2, 3, NA_real_)))

sum(df$cluster_4 != df$cluster_3)
```

```{r, echo = F}
df$cluster_4 <- as.factor(df$cluster_4)
ggplot( data = df, 
        aes( x = x, y = y, color = cluster_4)) +
  geom_point(size = 2, alpha = .7) +
  geom_point(data = centroids[1,], size = 4, color = "red", alpha = 1) +
  geom_point(data = centroids[2, ], size = 4, color = "green", alpha = 1) +
  geom_point(data = centroids[3, ], size = 4, color = "blue", alpha = 1) +
  geom_point(data = df[df$cluster_4 != df$cluster_3, ], size = 4, alpha = 1, shape = 5) +
  theme_minimal()
```


## K-means in R

Once we have the distance (similarity) matrix from `daisy()`:

::: incremental

- We call `kmeans()` to perform the cluster analysis
  - Applies to both Euclidean/Gower distance matrices
  - Use `nstart` option to multi-start algorithm from multiple random points
    - Recommended range: 10 to 25 (or more), depending on data regularity
- Syntax:
  - 4 segments, using a Gower distance matrix
    - `DF$clu_gower_4 <- kmeans(DF_gower, centers = 4, nstart = 10)`
  - 4 segments, using a Euclidean distance matrix
    - `DF$clu_euclid_4 <- kmeans(DF_euclidean, centers = 4, nstart = 10)`

:::

## K-means in R

In total:

```{r}
# Calculate euclidean distance
DF_euclidean <- daisy(DF, metric = "euclidean", warnType = FALSE, stand = TRUE)

# Create segments
segments <- kmeans(DF_euclidean, centers = 4, nstart = 10)

# Assign
DF$clu_euclid_4 <- segments$cluster

```

You can also do this in one step, but I wouldn't recommend it

- Calculating the distances takes time

```{r}
DF$clu_euclid_4 <- kmeans(
  daisy(DF, metric = "euclidean", warnType = FALSE, stand = TRUE), 
  centers = 4, nstart = 10)$cluster
```

## Step 4: Select number of clusters

::: incremental

- To perform k-means, we must specify the number of clusters
- How to determine "how many" clusters?
  - Statistical guidance — elbow plots
  - Segmentation criteria — substantial, actionable, differentiable
- Elbow plots
  - Graph the total within-cluster variation (the sum of squared distances from points to their cluster centroids) vs. # of clusters
  - Within-cluster variation decreases as $k$ gets larger, because when the number of clusters increases, within-cluster distances get smaller
  - The idea of the elbow method is to choose $k$ at which the within-cluster variation decreases abruptly

:::


## Step 4: Select number of clusters

Elbow plot example from `df`

What was the within-cluster variation from the example I had?

```{r, echo = F}
set.seed(123)
df <- data.frame(
  x = rnorm(100),
  y = rnorm(100)
)

df_euclidean <- daisy(df, metric = "euclidean", warnType = FALSE, stand = TRUE)

segments <- kmeans(df_euclidean, centers = 3, nstart = 10)
df$cluster <- as.factor(segments$cluster)

ggplot( data = df, 
        aes( x = x, y = y, color = cluster)) +
  geom_point(size = 2, alpha = .7) +
  theme_minimal()

```

::: incremental

The `kmeans()` output has a `$withinss` result, similar to how it has a `$cluster` result

```{r}
round( sum(kmeans(df_euclidean, centers = 3, nstart = 10)$withinss), 3)
```

:::

## Step 4: Select number of clusters

Elbow plot example from `df`

```{r}
# Euclidean distance
df_euclidean <- daisy(df, metric = "euclidean", warnType = FALSE, stand = TRUE)

# max number of clusters to test
max_clusters <- 10 

# list to hold within-cluster sum-of-squares
wss <- rep(0, max_clusters) 

# loop over cluster
for (i in 1:max_clusters) { 
  segments <- kmeans(df_euclidean, centers = i, nstart=10)
  wss[i] <- sum(segments$withinss) # within-cluster sum-of-squares, summed over clusters
}

as.data.frame(wss)
```

## Step 4: Select number of clusters

Elbow plot example from `df`

```{r}
elbow_data <- data.frame(k = 1:max_clusters, WCSS = wss)

ggplot(elbow_data, aes(x = k, y = WCSS)) +
  geom_line(color = "blue") +
  geom_point(color = "blue") +
  labs(title = "k-means Elbow Plot",
       x = "Number of Clusters",
       y = "Within groups sum of squares") +
  theme_minimal()

```

- Here it's not that clear from the elbow plot
  - Because it's too perfect
  - We would probably choose based on abilities


## Step 4: Select number of clusters

Elbow plot example from `DF`

::: fragment

```{r}
# Euclidean distance
DF_euclidean <- daisy(DF, metric = "euclidean", warnType = FALSE, stand = TRUE)

# max number of clusters to test
max_clusters <- 10 

# list to hold within-cluster sum-of-squares
wss <- rep(0, max_clusters) 

# loop over cluster
for (i in 1:max_clusters) { 
  segments <- kmeans(DF_euclidean, centers = i, nstart=10)
  wss[i] <- sum(segments$withinss) # within-cluster sum-of-squares, summed over clusters
}

as.data.frame(wss)
```

:::

## Step 4: Select number of clusters

Elbow plot example from `DF`

```{r}
elbow_data <- data.frame(k = 1:max_clusters, WCSS = wss)

ggplot(elbow_data, aes(x = k, y = WCSS)) +
  geom_line(color = "blue") +
  geom_point(color = "blue") +
  labs(title = "k-means Elbow Plot",
       x = "Number of Clusters",
       y = "Within groups sum of squares") +
  theme_minimal()

```

::: fragment

3 or 4 is best

:::

## Step 5: Interpret and profile the clusters

::: incremental

- Main objects of interest
  - Segment sizes — % of sample
  - Cluster centroids = mean values of variables within the cluster
- Assessment of segmentation criteria
  - Substantial — Unless of high value, potentially prune small segments
  - Actionable — Can we translate differences in clusters to targeted policies?
  - Differentiable — Sufficiently different to make/enforce different policies?
  
:::

## Step 5: Interpret and profile the clusters

A simple way (among many) to compute sizes:

```{r}
segments <- kmeans(DF_euclidean, centers = 4, nstart=10)

DF$cluster <- segments$cluster

library(dplyr)
DF |>
  group_by(cluster) |>
  summarise(size = n(),
            proportion = round(n()/nrow(DF), 3))

```

## Step 5: Interpret and profile the clusters

A simple way (among many) to compute means:

```{r}
DF |>
  group_by(cluster) |>
  summarise(across(c(spend_online, spend_retail, age, white, college, male, hh_inc), mean)) |>
  round(3)
```

